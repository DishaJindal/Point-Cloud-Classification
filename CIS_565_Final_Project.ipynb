{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS 565 Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-suS8g2ogK8",
        "colab_type": "code",
        "outputId": "c93b8971-4d9a-4f96-88c7-3d38ea7074c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive\n",
        "DRIVE_MOUNT='/content/gdrive'\n",
        "drive.mount(DRIVE_MOUNT)\n",
        "\n",
        "\n",
        "# create folder to write data to\n",
        "CIS_565_FOLDER=os.path.join(DRIVE_MOUNT, 'My Drive', 'CIS_565_2019')\n",
        "FINAL_PROJECT_FOLDER=os.path.join(CIS_565_FOLDER, 'Final_Project')\n",
        "os.makedirs(FINAL_PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "# bootstrap environment into place\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "import io\n",
        "import os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "def download_file(fn, file_id):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    downloaded = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(downloaded, request)\n",
        "    done = False\n",
        "    while done is False:\n",
        "        # _ is a placeholder for a progress object that we ignore.\n",
        "        # (Our file is small, so we skip reporting progress.)\n",
        "        _, done = downloader.next_chunk()\n",
        "    \n",
        "    downloaded.seek(0)\n",
        "\n",
        "    folder = fn.split('/')\n",
        "    if len(folder) > 1:\n",
        "        os.makedirs(folder[0], exist_ok=True)\n",
        "\n",
        "    with open(fn, 'wb') as f:\n",
        "        f.write(downloaded.read())\n",
        "\n",
        "id_to_fn = {}\n",
        "\n",
        "# download all files into the vm\n",
        "for fid, fn in id_to_fn.items():\n",
        "    download_file(fn, fid)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuoNL7tkqwke",
        "colab_type": "code",
        "outputId": "6cc200b9-e615-4171-9dba-dfa13719c154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!wget -nc -P '/content/gdrive/My Drive/CIS_565_2019/Final_Project/' \"https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip\"\n",
        "!unzip -n '/content/gdrive/My Drive/CIS_565_2019/Final_Project/modelnet40_ply_hdf5_2048.zip' -d \"/content/gdrive/My Drive/CIS_565_2019/Final_Project/data/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘/content/gdrive/My Drive/CIS_565_2019/Final_Project/modelnet40_ply_hdf5_2048.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  /content/gdrive/My Drive/CIS_565_2019/Final_Project/modelnet40_ply_hdf5_2048.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnA4pkZcrsTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r -f ./graph\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import sparse\n",
        "import scipy.sparse.linalg as sp\n",
        "import numpy as np\n",
        "from scipy.spatial import cKDTree\n",
        "import pickle\n",
        "\n",
        "class Parameters():\n",
        "  def __init__(self):\n",
        "    self.neighborNumber = 40\n",
        "    self.outputClassN = 10\n",
        "    self.pointNumber = 1024\n",
        "    self.gcn_1_filter_n = 1000\n",
        "    self.gcn_2_filter_n = 1000\n",
        "    self.gcn_3_filter_n = 1000\n",
        "    self.fc_1_n = 600\n",
        "    self.chebyshev_1_Order = 4\n",
        "    self.chebyshev_2_Order = 3\n",
        "    self.keep_prob_1 = 0.9 #0.9 original\n",
        "    self.keep_prob_2 = 0.55\n",
        "    self.batchSize = 28\n",
        "    self.testBatchSize = 1\n",
        "    self.max_epoch = 260\n",
        "    self.learningRate = 12e-4\n",
        "    self.dataset = 'ModelNet40'\n",
        "    self.weighting_scheme = 'weighted'\n",
        "    self.modelDir = '/raid60/yingxue.zhang2/ICASSP_code/global_pooling/model/'\n",
        "    self.logDir = '/raid60/yingxue.zhang2/ICASSP_code/global_pooling/log/'\n",
        "    self.fileName = '0112_1024_40_cheby_4_3_modelnet40_max_var_first_second_layer'\n",
        "    self.weight_scaler = 40#50\n",
        "\n",
        "def adjacency(dist, idx):\n",
        "    \"\"\"Return the adjacency matrix of a kNN graph.\"\"\"\n",
        "    M, k = dist.shape\n",
        "    assert M, k == idx.shape\n",
        "    assert dist.min() >= 0\n",
        "    # Weights.\n",
        "    sigma2 = np.mean(dist[:, -1]) ** 2\n",
        "    #print sigma2\n",
        "    dist = np.exp(- dist ** 2 / sigma2)\n",
        "\n",
        "    # Weight matrix.\n",
        "    I = np.arange(0, M).repeat(k)\n",
        "    J = idx.reshape(M * k)\n",
        "    V = dist.reshape(M * k)\n",
        "    W = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
        "    # No self-connections.\n",
        "    W.setdiag(0)\n",
        "\n",
        "    # Non-directed graph.\n",
        "    bigger = W.T > W\n",
        "    W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
        "    return W\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    adj = sparse.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sparse.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def normalized_laplacian(adj):\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    norm_laplacian = sparse.eye(adj.shape[0]) - adj_normalized\n",
        "    return norm_laplacian\n",
        "\n",
        "def scaled_laplacian(adj):\n",
        "    adj_normalized = normalize_adj(adj)\n",
        "    laplacian = sparse.eye(adj.shape[0]) - adj_normalized\n",
        "    largest_eigval, _ = sp.eigsh(laplacian, 1, which='LM')\n",
        "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - scipy.sparse.eye(adj.shape[0])\n",
        "    return scaled_laplacian\n",
        "\n",
        "\n",
        "def getDataFiles(list_filename):\n",
        "    return [line.rstrip() for line in open(list_filename)]\n",
        "\n",
        "def load_h5(h5_filename):\n",
        "    f = h5py.File(os.path.join(FINAL_PROJECT_FOLDER,h5_filename))\n",
        "    data = f['data'][:]\n",
        "    label = f['label'][:]\n",
        "    return (data, label)\n",
        "\n",
        "def loadDataFile(filename):\n",
        "    return load_h5(filename)\n",
        "    \n",
        "    \n",
        "def farthestSampling(file_names, NUM_POINT):\n",
        "    file_indexs = np.arange(0, len(file_names))\n",
        "    inputData = dict()\n",
        "    inputLabel = dict()\n",
        "    for index in range (len(file_indexs)):\n",
        "        current_data, current_label = loadDataFile(file_names[file_indexs[index]])\n",
        "        current_data = current_data[:,0:NUM_POINT,:]\n",
        "        current_label = np.squeeze(current_label) \n",
        "        current_label= np.int_(current_label)\n",
        "        inputData.update({index : current_data})\n",
        "        inputLabel.update({index : current_label})\n",
        "    return inputData, inputLabel\n",
        "\n",
        "def load_data(NUM_POINT):\n",
        "    \n",
        "    TRAIN_FILES = getDataFiles(os.path.join(FINAL_PROJECT_FOLDER, 'data/modelnet40_ply_hdf5_2048/train_files.txt'))\n",
        "    TEST_FILES = getDataFiles(os.path.join(FINAL_PROJECT_FOLDER, 'data/modelnet40_ply_hdf5_2048/test_files.txt'))\n",
        "    \n",
        "    \n",
        "    inputTrainFarthest, inputTrainLabel = farthestSampling(TRAIN_FILES, NUM_POINT)\n",
        "    inputTestFathest, inputTestLabel = farthestSampling(TEST_FILES, NUM_POINT)\n",
        "    return inputTrainFarthest, inputTrainLabel, inputTestFathest, inputTestLabel\n",
        "\n",
        "\n",
        "\n",
        "para = Parameters()\n",
        "pointNumber = para.pointNumber\n",
        "neighborNumber = para.neighborNumber\n",
        "\n",
        "#generate graph structure and store in the system\n",
        "def prepareGraph(inputData, neighborNumber, pointNumber, dataType):\n",
        "    scaledLaplacianDict = dict()\n",
        "    baseDir = FINAL_PROJECT_FOLDER\n",
        "    fileDir =  baseDir + '/graph/' + dataType+'_pn_'+str(pointNumber)+'_nn_'+str(neighborNumber)\n",
        "        \n",
        "    if (os.path.isdir(fileDir)):\n",
        "        print (\"calculating the graph data\")\n",
        "        # os.makedirs(fileDir)\n",
        "        for batchIndex in range(len(inputData)):\n",
        "            batchInput = inputData[batchIndex]\n",
        "            print(\"len: \", len(batchInput))\n",
        "            batchFlattenLaplacian = []\n",
        "            for i in range(10):\n",
        "                print (i)\n",
        "                pcCoordinates = batchInput[i]\n",
        "                tree = cKDTree(pcCoordinates)\n",
        "                dd, ii = tree.query(pcCoordinates, k = neighborNumber)\n",
        "                A = adjacency(dd, ii)\n",
        "                scaledLaplacian = scaled_laplacian(A)\n",
        "                scaledLaplacian = scaledLaplacian.toarray()\n",
        "                print(\"Lap: \", scaledLaplacian.shape)\n",
        "                # flattenLaplacian = scaledLaplacian.tolil().reshape((1, pointNumber*pointNumber))\n",
        "                batchFlattenLaplacian.append(scaledLaplacian)\n",
        "                # if i ==0:\n",
        "                #     batchFlattenLaplacian = scaledLaplacian\n",
        "                # else:\n",
        "                #     batchFlattenLaplacian = np.stack((batchFlattenLaplacian, scaledLaplacian), axis=-1)\n",
        "            batchFlattenLaplacian = np.stack(batchFlattenLaplacian, axis=0)\n",
        "            print(\"batch lap: \", batchFlattenLaplacian.shape)\n",
        "            scaledLaplacianDict.update({batchIndex: batchFlattenLaplacian})\n",
        "            with open(fileDir+'/batchGraph_'+str(batchIndex), 'wb') as handle:\n",
        "                pickle.dump(batchFlattenLaplacian, handle)\n",
        "            print (\"Saving the graph data batch\"+str(batchIndex))\n",
        "        \n",
        "    else:\n",
        "        print(\"Loading the graph data from \"+dataType+'Data')\n",
        "        scaledLaplacianDict = loadGraph(inputData, neighborNumber, pointNumber, fileDir)\n",
        "    return scaledLaplacianDict\n",
        "\n",
        "def get_laplacian_for_batch(batchInput):\n",
        "  batchFlattenLaplacian = []\n",
        "  for i in range(len(batchInput)):\n",
        "      pcCoordinates = batchInput[i]\n",
        "      tree = cKDTree(pcCoordinates)\n",
        "      dd, ii = tree.query(pcCoordinates, k = neighborNumber)\n",
        "      A = adjacency(dd, ii)\n",
        "      scaledLaplacian = scaled_laplacian(A)\n",
        "      scaledLaplacian = scaledLaplacian.toarray()\n",
        "      batchFlattenLaplacian.append(scaledLaplacian)\n",
        "  batchFlattenLaplacian = np.stack(batchFlattenLaplacian, axis=0)\n",
        "\n",
        "  return batchFlattenLaplacian\n",
        "\n",
        "def loadGraph(inputData, neighborNumber, pointNumber, fileDir):\n",
        "    scaledLaplacianDict = dict()\n",
        "    for batchIndex in range(len(inputData)):\n",
        "        batchDataDir = fileDir+'/batchGraph_'+str(batchIndex)\n",
        "        with open(batchDataDir, 'rb') as handle:\n",
        "            batchGraph = pickle.load(handle)\n",
        "        scaledLaplacianDict.update({batchIndex: batchGraph })\n",
        "        print (\"Finish loading batch_\"+str(batchIndex))\n",
        "    return scaledLaplacianDict\n",
        "\n",
        "                        \n",
        "def prepareData(inputTrain, inputTest, neighborNumber, pointNumber):\n",
        "    scaledLaplacianTrain = prepareGraph(inputTrain, neighborNumber, pointNumber, 'train',)\n",
        "    scaledLaplacianTest = prepareGraph(inputTest, neighborNumber, pointNumber, 'test')\n",
        "    return scaledLaplacianTrain, scaledLaplacianTest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOXtlB-mcqBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputTrain, trainLabel, inputTest, testLabel = load_data(pointNumber)\n",
        "# scaledLaplacianTrain, scaledLaplacianTest = prepareData(inputTrain, inputTest, neighborNumber, pointNumber)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjKzuYF8vtbR",
        "colab_type": "code",
        "outputId": "a23e4397-a0a8-4213-c452-b33f3146742b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "def convert_data_to_tensor(input, is_graph = False):\n",
        "  all_inputs = None\n",
        "  for i in range(len(input)):\n",
        "    if is_graph:\n",
        "      ip = torch.sparse.FloatTensor(input[i])\n",
        "      ip = torch.reshape(ip, (-1, 1024, 1024))\n",
        "    else:\n",
        "      ip = torch.Tensor(input[i])\n",
        "    if all_inputs is None:\n",
        "      all_inputs = ip\n",
        "    else:\n",
        "      all_inputs = torch.cat((all_inputs, ip))\n",
        "  return all_inputs\n",
        "\n",
        "inputTrainTensor = convert_data_to_tensor(inputTrain)\n",
        "trainLabelTensor = convert_data_to_tensor(trainLabel)\n",
        "\n",
        "mask = (trainLabelTensor == 1) | (trainLabelTensor == 2) | (trainLabelTensor == 8) | (trainLabelTensor == 12) | (trainLabelTensor == 14) | (trainLabelTensor == 22) | (trainLabelTensor == 23) | (trainLabelTensor == 30)| (trainLabelTensor == 33) | (trainLabelTensor == 35)\n",
        "\n",
        "# first10mask = (trainLabelTensor < 10)\n",
        "\n",
        "number_of_samples = 2097\n",
        "\n",
        "inputTrainTensor = inputTrainTensor[mask]\n",
        "inputTrainTensor = inputTrainTensor[0:number_of_samples]\n",
        "trainLabelTensor = trainLabelTensor[mask]\n",
        "trainLabelTensor = trainLabelTensor[0:number_of_samples]\n",
        "print(len(mask), len(inputTrainTensor), len(trainLabelTensor), inputTrainTensor.shape)\n",
        "print(trainLabelTensor)\n",
        "\n",
        "label_map = {1:0, 2:1, 8:2, 12:3, 14:4, 22:5, 23:6, 30:7, 33:8, 35:9}\n",
        "trainLabelTensorNew = torch.tensor([label_map[i.item()] for i in trainLabelTensor])\n",
        "trainLabelTensor = trainLabelTensorNew\n",
        "\n",
        "inputTestTensor = convert_data_to_tensor(inputTest)\n",
        "testLabelTensor = convert_data_to_tensor(testLabel)\n",
        "\n",
        "mask = (testLabelTensor == 1) | (testLabelTensor == 2) | (testLabelTensor == 8) | (testLabelTensor == 12) | (testLabelTensor == 14) | (testLabelTensor == 22) | (testLabelTensor == 23) | (testLabelTensor == 30)| (testLabelTensor == 33) | (testLabelTensor == 35)\n",
        "\n",
        "\n",
        "number_of_samples = 100\n",
        "\n",
        "inputTestTensor = inputTestTensor[mask]\n",
        "inputTestTensor = inputTestTensor[0:number_of_samples]\n",
        "testLabelTensor = testLabelTensor[mask]\n",
        "testLabelTensor = testLabelTensor[0:number_of_samples]\n",
        "print(len(first10mask), len(inputTestTensor), len(testLabelTensor), testLabelTensor.shape)\n",
        "\n",
        "testLabelTensorNew = torch.tensor([label_map[i.item()] for i in testLabelTensor])\n",
        "testLabelTensor = testLabelTensorNew\n",
        "print(testLabelTensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9840 2097 2097 torch.Size([2097, 1024, 3])\n",
            "tensor([30., 30., 22.,  ..., 30.,  8.,  8.])\n",
            "2468 100 100 torch.Size([100])\n",
            "tensor([1, 2, 6, 9, 9, 8, 9, 7, 2, 0, 6, 9, 5, 1, 3, 8, 7, 6, 7, 4, 4, 8, 2, 4,\n",
            "        5, 8, 6, 9, 9, 1, 5, 5, 1, 3, 8, 1, 9, 1, 7, 3, 7, 3, 9, 3, 9, 9, 2, 6,\n",
            "        2, 9, 9, 9, 5, 3, 1, 5, 5, 4, 4, 1, 2, 0, 3, 3, 6, 7, 4, 9, 9, 9, 2, 9,\n",
            "        7, 0, 7, 5, 8, 0, 2, 4, 5, 4, 6, 4, 9, 9, 4, 8, 2, 5, 5, 0, 4, 1, 6, 9,\n",
            "        8, 1, 1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdRVuDqnBAxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelNetDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, inputData, inputLabel):\n",
        "    self.data = inputData\n",
        "    self.labels = inputLabel\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.data.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = {'data': self.data[index], 'label': self.labels[index]}\n",
        "    return sample\n",
        "\n",
        "modelnetdata = ModelNetDataset(inputTrainTensor, trainLabelTensor)\n",
        "modelnetdataTest = ModelNetDataset(inputTestTensor, testLabelTensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGJ1eq_18Oyo",
        "colab_type": "code",
        "outputId": "f7c06a7f-c43c-43f5-c924-9ece481ebac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "set_batch_size = 32\n",
        "training_data_batches = DataLoader(modelnetdata, batch_size = set_batch_size, shuffle = True)\n",
        "test_data_batches = DataLoader(modelnetdataTest, batch_size = 1, shuffle = False)\n",
        "print(training_data_batches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f081c113358>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BY-MEYQxB9l",
        "colab_type": "code",
        "outputId": "82f5c4f5-e223-42e4-9373-8dd44bf858a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3y4GCz3rGm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class globalPooling(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, featureNumber):\n",
        "    super().__init__()\n",
        "    self.featureNumber = featureNumber\n",
        "\n",
        "  def forward(self, gcnOutput):\n",
        "    var = torch.var(gcnOutput, dim = 1) ** 2\n",
        "    max_f = torch.max(gcnOutput, dim = 1)\n",
        "    max_f = max_f.values\n",
        "    poolingOutput = torch.cat((max_f, var), dim = 1)\n",
        "    return poolingOutput\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class gcnLayer(torch.nn.Module):\n",
        "\n",
        "  def __init__(self,pointNumber, inputFeatureN, outputFeatureN, chebyshev_order):\n",
        "    super().__init__()\n",
        "    self.pointNumber      = pointNumber\n",
        "    self.inputFeatureN    = inputFeatureN\n",
        "    self.outputFeatureN   = outputFeatureN\n",
        "    self.chebyshev_order  = chebyshev_order\n",
        "    self.relu             = nn.ReLU()\n",
        "\n",
        "    self.biasWeight = nn.Parameter(torch.normal(torch.zeros(self.outputFeatureN), 0.05 * torch.ones(self.outputFeatureN)))\n",
        "    self.chebyshevCoeff = dict()\n",
        "    for i in range(chebyshev_order):\n",
        "        self.chebyshevCoeff['w_' + str(i)] = nn.Parameter(torch.normal(torch.zeros(self.inputFeatureN,self.outputFeatureN), 0.05 * torch.ones(self.inputFeatureN,self.outputFeatureN)))\n",
        "        self.chebyshevCoeff['w_' + str(i)] = self.chebyshevCoeff['w_' + str(i)].to(device)\n",
        "\n",
        "  def forward(self,inputPC, scaledLaplacian):\n",
        "    chebyPoly = []\n",
        "    cheby_K_Minus_1 = torch.matmul(scaledLaplacian, inputPC)\n",
        "    cheby_K_Minus_2 = inputPC\n",
        "    chebyPoly.append(cheby_K_Minus_2)\n",
        "    chebyPoly.append(cheby_K_Minus_1)\n",
        "    for i in range(2, self.chebyshev_order):\n",
        "        chebyK = 2 * (torch.matmul(scaledLaplacian, cheby_K_Minus_1)) - cheby_K_Minus_2\n",
        "        chebyPoly.append(chebyK)\n",
        "        cheby_K_Minus_2 = cheby_K_Minus_1\n",
        "        cheby_K_Minus_1 = chebyK\n",
        "\n",
        "    gcnOutput = self.biasWeight\n",
        "    chebyOutput = []\n",
        "    for i in range(self.chebyshev_order):\n",
        "        weights = self.chebyshevCoeff['w_' + str(i)]\n",
        "        chebyPolyReshape = torch.reshape(chebyPoly[i], (-1, self.inputFeatureN))\n",
        "        output = torch.matmul(chebyPolyReshape, weights)\n",
        "        output = torch.reshape(output, (-1, self.pointNumber, self.outputFeatureN))\n",
        "        gcnOutput = gcnOutput + output\n",
        "        # chebyOutput.append(output)\n",
        "\n",
        "    gcnOutput = self.relu(gcnOutput)\n",
        "    \n",
        "    return gcnOutput\n",
        "\n",
        "\n",
        "class pointGCN(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, para):\n",
        "    super().__init__()\n",
        "    self.globalFeatureN                   = (para.gcn_1_filter_n + para.gcn_2_filter_n)*2 \n",
        "    self.gcn1_layer                       = gcnLayer(para.pointNumber, 3, para.gcn_1_filter_n, para.chebyshev_1_Order)\n",
        "    self.gcn1_dropout_layer               = nn.Dropout(p = 1 - para.keep_prob_1)\n",
        "    self.gcn1_global_pooling_layer        = globalPooling(para.gcn_1_filter_n)\n",
        "    self.gcn2_layer                       = gcnLayer(para.pointNumber, para.gcn_1_filter_n, para.gcn_2_filter_n, para.chebyshev_2_Order)\n",
        "    self.gcn2_dropout_layer               = nn.Dropout(p = 1 - para.keep_prob_1)\n",
        "    self.gcn2_global_pooling_layer        = globalPooling(para.gcn_2_filter_n)\n",
        "    self.global_features_dropout_layer    = nn.Dropout(p = 1 - para.keep_prob_2)\n",
        "    globalFeatureN = (para.gcn_1_filter_n + para.gcn_2_filter_n)*2 \n",
        "    self.fc1                              = nn.Linear(globalFeatureN, para.fc_1_n)\n",
        "    self.relu                             = nn.ReLU()\n",
        "    self.fc1_droput                       = nn.Dropout(p = 1 - para.keep_prob_2)\n",
        "    self.fc2                              = nn.Linear(para.fc_1_n, para.outputClassN)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, inputPC, scaledLaplacian):\n",
        "    \n",
        "    gcn_1         = self.gcn1_layer(inputPC, scaledLaplacian)\n",
        "    gcn_1_output  = self.gcn1_dropout_layer(gcn_1)\n",
        "    \n",
        "    gcn_1_pooling = self.gcn1_global_pooling_layer(gcn_1_output)\n",
        "    \n",
        "    gcn_2         = self.gcn2_layer(gcn_1_output, scaledLaplacian)\n",
        "    gcn_2_output  = self.gcn2_dropout_layer(gcn_2)\n",
        "    gcn_2_pooling = self.gcn2_global_pooling_layer(gcn_2_output)\n",
        "\n",
        "    globalFeatures = torch.cat((gcn_1_pooling, gcn_2_pooling), dim=1)\n",
        "    globalFeatures = self.global_features_dropout_layer(globalFeatures)\n",
        "\n",
        "    # fully connected layer 1\n",
        "    fc_layer_1 = self.fc1(globalFeatures)\n",
        "    fc_layer_1 = self.relu(fc_layer_1)\n",
        "    fc_layer_1 = self.fc1_droput(fc_layer_1)\n",
        "\n",
        "    # fully connected layer 2\n",
        "    fc_layer_2 = self.fc2(fc_layer_1)\n",
        "\n",
        "    return fc_layer_2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA0eWDy26KY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def train(gcn_model, training_data_batches, ce_loss, optimizer, num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    start = time.time()\n",
        "    for i, sample in enumerate(training_data_batches):\n",
        "      batchInput = sample['data']\n",
        "      batchLabel = sample['label']\n",
        "      batchLap = torch.Tensor(get_laplacian_for_batch(sample['data']))\n",
        "\n",
        "      batchInput = batchInput.cuda()\n",
        "      batchLabel = batchLabel.cuda()\n",
        "      batchLap = batchLap.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batchOutput = gcn_model(batchInput, batchLap)\n",
        "\n",
        "      loss = ce_loss(batchOutput, batchLabel.long())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      print(\"Epoch \", epoch, \"Iteration \", i, \"Loss: \", loss.item())\n",
        "    end = time.time()\n",
        "    time_per_epoch = end - start\n",
        "    epoch_loss /= len(training_data_batches)\n",
        "    print(\"*********************************************************************\")\n",
        "    print(\"EPOCH \", epoch, \" ==> Loss: \", epoch_loss, \" Time taken ==> \", time_per_epoch)\n",
        "    print(\"*********************************************************************\")\n",
        "  return gcn_model          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0iv2rmKDwn8",
        "colab_type": "code",
        "outputId": "852f7e74-9a3f-441e-fae5-bed75af773fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "gcn_model = pointGCN(para).to(device)\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(gcn_model.parameters(), lr = 2e-3)\n",
        "\n",
        "trained_gcn_model = train(gcn_model, training_data_batches, ce_loss, optimizer, num_epochs) "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0 Iteration  0 Loss:  2.2876498699188232\n",
            "Epoch  0 Iteration  1 Loss:  2.2292990684509277\n",
            "Epoch  0 Iteration  2 Loss:  2.394721508026123\n",
            "Epoch  0 Iteration  3 Loss:  2.2200677394866943\n",
            "Epoch  0 Iteration  4 Loss:  2.1301491260528564\n",
            "Epoch  0 Iteration  5 Loss:  2.1640663146972656\n",
            "Epoch  0 Iteration  6 Loss:  2.083338975906372\n",
            "Epoch  0 Iteration  7 Loss:  2.0299620628356934\n",
            "Epoch  0 Iteration  8 Loss:  2.056279182434082\n",
            "Epoch  0 Iteration  9 Loss:  1.9493346214294434\n",
            "Epoch  0 Iteration  10 Loss:  1.9061641693115234\n",
            "Epoch  0 Iteration  11 Loss:  2.25230073928833\n",
            "Epoch  0 Iteration  12 Loss:  2.155426025390625\n",
            "Epoch  0 Iteration  13 Loss:  2.2294809818267822\n",
            "Epoch  0 Iteration  14 Loss:  1.8437827825546265\n",
            "Epoch  0 Iteration  15 Loss:  1.8910883665084839\n",
            "Epoch  0 Iteration  16 Loss:  1.7508856058120728\n",
            "Epoch  0 Iteration  17 Loss:  1.9524320363998413\n",
            "Epoch  0 Iteration  18 Loss:  1.851224660873413\n",
            "Epoch  0 Iteration  19 Loss:  1.8271737098693848\n",
            "Epoch  0 Iteration  20 Loss:  2.0221188068389893\n",
            "Epoch  0 Iteration  21 Loss:  2.116086483001709\n",
            "Epoch  0 Iteration  22 Loss:  1.8237378597259521\n",
            "Epoch  0 Iteration  23 Loss:  1.8351411819458008\n",
            "Epoch  0 Iteration  24 Loss:  1.797989010810852\n",
            "Epoch  0 Iteration  25 Loss:  1.3782916069030762\n",
            "Epoch  0 Iteration  26 Loss:  1.483760118484497\n",
            "Epoch  0 Iteration  27 Loss:  1.8277006149291992\n",
            "Epoch  0 Iteration  28 Loss:  1.928460955619812\n",
            "Epoch  0 Iteration  29 Loss:  1.8236706256866455\n",
            "Epoch  0 Iteration  30 Loss:  1.6511493921279907\n",
            "Epoch  0 Iteration  31 Loss:  1.3181930780410767\n",
            "Epoch  0 Iteration  32 Loss:  1.544147253036499\n",
            "Epoch  0 Iteration  33 Loss:  1.5678943395614624\n",
            "Epoch  0 Iteration  34 Loss:  1.4549479484558105\n",
            "Epoch  0 Iteration  35 Loss:  1.6734058856964111\n",
            "Epoch  0 Iteration  36 Loss:  1.8214309215545654\n",
            "Epoch  0 Iteration  37 Loss:  1.40340256690979\n",
            "Epoch  0 Iteration  38 Loss:  1.5996960401535034\n",
            "Epoch  0 Iteration  39 Loss:  1.7632893323898315\n",
            "Epoch  0 Iteration  40 Loss:  1.5603095293045044\n",
            "Epoch  0 Iteration  41 Loss:  1.5421559810638428\n",
            "Epoch  0 Iteration  42 Loss:  1.498359203338623\n",
            "Epoch  0 Iteration  43 Loss:  1.3066394329071045\n",
            "Epoch  0 Iteration  44 Loss:  1.344444751739502\n",
            "Epoch  0 Iteration  45 Loss:  1.6167709827423096\n",
            "Epoch  0 Iteration  46 Loss:  1.1618953943252563\n",
            "Epoch  0 Iteration  47 Loss:  1.3866671323776245\n",
            "Epoch  0 Iteration  48 Loss:  1.3136662244796753\n",
            "Epoch  0 Iteration  49 Loss:  1.2260746955871582\n",
            "Epoch  0 Iteration  50 Loss:  1.125045895576477\n",
            "Epoch  0 Iteration  51 Loss:  1.46516752243042\n",
            "Epoch  0 Iteration  52 Loss:  1.2475355863571167\n",
            "Epoch  0 Iteration  53 Loss:  1.4335803985595703\n",
            "Epoch  0 Iteration  54 Loss:  1.276760220527649\n",
            "Epoch  0 Iteration  55 Loss:  1.3149653673171997\n",
            "Epoch  0 Iteration  56 Loss:  1.2521710395812988\n",
            "Epoch  0 Iteration  57 Loss:  1.332345724105835\n",
            "Epoch  0 Iteration  58 Loss:  1.2675703763961792\n",
            "Epoch  0 Iteration  59 Loss:  1.3172800540924072\n",
            "Epoch  0 Iteration  60 Loss:  1.5546588897705078\n",
            "Epoch  0 Iteration  61 Loss:  1.4202338457107544\n",
            "Epoch  0 Iteration  62 Loss:  1.1139426231384277\n",
            "Epoch  0 Iteration  63 Loss:  1.287452220916748\n",
            "Epoch  0 Iteration  64 Loss:  1.4176801443099976\n",
            "Epoch  0 Iteration  65 Loss:  1.1067501306533813\n",
            "*********************************************************************\n",
            "EPOCH  0  ==> Loss:  1.6655979535796426  Time taken ==>  229.62684059143066\n",
            "*********************************************************************\n",
            "Epoch  1 Iteration  0 Loss:  1.29001784324646\n",
            "Epoch  1 Iteration  1 Loss:  1.1789568662643433\n",
            "Epoch  1 Iteration  2 Loss:  1.1462246179580688\n",
            "Epoch  1 Iteration  3 Loss:  1.2528258562088013\n",
            "Epoch  1 Iteration  4 Loss:  1.1549581289291382\n",
            "Epoch  1 Iteration  5 Loss:  1.284937858581543\n",
            "Epoch  1 Iteration  6 Loss:  1.0232477188110352\n",
            "Epoch  1 Iteration  7 Loss:  1.099085807800293\n",
            "Epoch  1 Iteration  8 Loss:  1.200837254524231\n",
            "Epoch  1 Iteration  9 Loss:  1.513486623764038\n",
            "Epoch  1 Iteration  10 Loss:  1.1972687244415283\n",
            "Epoch  1 Iteration  11 Loss:  1.5020126104354858\n",
            "Epoch  1 Iteration  12 Loss:  1.2941383123397827\n",
            "Epoch  1 Iteration  13 Loss:  1.2462103366851807\n",
            "Epoch  1 Iteration  14 Loss:  1.3279389142990112\n",
            "Epoch  1 Iteration  15 Loss:  1.3659454584121704\n",
            "Epoch  1 Iteration  16 Loss:  1.396047830581665\n",
            "Epoch  1 Iteration  17 Loss:  0.9926707744598389\n",
            "Epoch  1 Iteration  18 Loss:  1.2410225868225098\n",
            "Epoch  1 Iteration  19 Loss:  1.1312843561172485\n",
            "Epoch  1 Iteration  20 Loss:  1.0953314304351807\n",
            "Epoch  1 Iteration  21 Loss:  1.1529549360275269\n",
            "Epoch  1 Iteration  22 Loss:  1.2470206022262573\n",
            "Epoch  1 Iteration  23 Loss:  1.431674599647522\n",
            "Epoch  1 Iteration  24 Loss:  1.1858001947402954\n",
            "Epoch  1 Iteration  25 Loss:  1.2968021631240845\n",
            "Epoch  1 Iteration  26 Loss:  1.2240103483200073\n",
            "Epoch  1 Iteration  27 Loss:  1.0673233270645142\n",
            "Epoch  1 Iteration  28 Loss:  0.9444971084594727\n",
            "Epoch  1 Iteration  29 Loss:  1.189180850982666\n",
            "Epoch  1 Iteration  30 Loss:  1.0521515607833862\n",
            "Epoch  1 Iteration  31 Loss:  1.1923476457595825\n",
            "Epoch  1 Iteration  32 Loss:  1.0662264823913574\n",
            "Epoch  1 Iteration  33 Loss:  1.272465467453003\n",
            "Epoch  1 Iteration  34 Loss:  0.7776903510093689\n",
            "Epoch  1 Iteration  35 Loss:  1.0149294137954712\n",
            "Epoch  1 Iteration  36 Loss:  0.8179896473884583\n",
            "Epoch  1 Iteration  37 Loss:  1.2502411603927612\n",
            "Epoch  1 Iteration  38 Loss:  1.1367043256759644\n",
            "Epoch  1 Iteration  39 Loss:  1.0768951177597046\n",
            "Epoch  1 Iteration  40 Loss:  1.0706387758255005\n",
            "Epoch  1 Iteration  41 Loss:  1.2788710594177246\n",
            "Epoch  1 Iteration  42 Loss:  1.0635366439819336\n",
            "Epoch  1 Iteration  43 Loss:  1.4518930912017822\n",
            "Epoch  1 Iteration  44 Loss:  0.9804328680038452\n",
            "Epoch  1 Iteration  45 Loss:  1.1405785083770752\n",
            "Epoch  1 Iteration  46 Loss:  1.2587664127349854\n",
            "Epoch  1 Iteration  47 Loss:  1.1491236686706543\n",
            "Epoch  1 Iteration  48 Loss:  0.8868978023529053\n",
            "Epoch  1 Iteration  49 Loss:  1.0923622846603394\n",
            "Epoch  1 Iteration  50 Loss:  1.1902132034301758\n",
            "Epoch  1 Iteration  51 Loss:  1.0464030504226685\n",
            "Epoch  1 Iteration  52 Loss:  1.2471967935562134\n",
            "Epoch  1 Iteration  53 Loss:  0.9932067394256592\n",
            "Epoch  1 Iteration  54 Loss:  1.1097896099090576\n",
            "Epoch  1 Iteration  55 Loss:  0.9446988105773926\n",
            "Epoch  1 Iteration  56 Loss:  0.838167667388916\n",
            "Epoch  1 Iteration  57 Loss:  1.1547563076019287\n",
            "Epoch  1 Iteration  58 Loss:  1.0305529832839966\n",
            "Epoch  1 Iteration  59 Loss:  1.16094172000885\n",
            "Epoch  1 Iteration  60 Loss:  0.8962872624397278\n",
            "Epoch  1 Iteration  61 Loss:  0.5958569049835205\n",
            "Epoch  1 Iteration  62 Loss:  1.0287761688232422\n",
            "Epoch  1 Iteration  63 Loss:  1.2796032428741455\n",
            "Epoch  1 Iteration  64 Loss:  1.138508677482605\n",
            "Epoch  1 Iteration  65 Loss:  1.0860425233840942\n",
            "*********************************************************************\n",
            "EPOCH  1  ==> Loss:  1.1431129999233014  Time taken ==>  228.67706418037415\n",
            "*********************************************************************\n",
            "Epoch  2 Iteration  0 Loss:  0.8072487711906433\n",
            "Epoch  2 Iteration  1 Loss:  1.137020230293274\n",
            "Epoch  2 Iteration  2 Loss:  1.1094967126846313\n",
            "Epoch  2 Iteration  3 Loss:  1.2822158336639404\n",
            "Epoch  2 Iteration  4 Loss:  1.0779805183410645\n",
            "Epoch  2 Iteration  5 Loss:  0.9089998006820679\n",
            "Epoch  2 Iteration  6 Loss:  0.9654262065887451\n",
            "Epoch  2 Iteration  7 Loss:  0.8958764672279358\n",
            "Epoch  2 Iteration  8 Loss:  0.9268984198570251\n",
            "Epoch  2 Iteration  9 Loss:  1.0060882568359375\n",
            "Epoch  2 Iteration  10 Loss:  0.9228481650352478\n",
            "Epoch  2 Iteration  11 Loss:  1.1207667589187622\n",
            "Epoch  2 Iteration  12 Loss:  1.2447654008865356\n",
            "Epoch  2 Iteration  13 Loss:  1.2159889936447144\n",
            "Epoch  2 Iteration  14 Loss:  0.7761818766593933\n",
            "Epoch  2 Iteration  15 Loss:  0.9115197658538818\n",
            "Epoch  2 Iteration  16 Loss:  1.0966672897338867\n",
            "Epoch  2 Iteration  17 Loss:  1.0855133533477783\n",
            "Epoch  2 Iteration  18 Loss:  1.1775035858154297\n",
            "Epoch  2 Iteration  19 Loss:  1.1587841510772705\n",
            "Epoch  2 Iteration  20 Loss:  0.9967862367630005\n",
            "Epoch  2 Iteration  21 Loss:  1.0460803508758545\n",
            "Epoch  2 Iteration  22 Loss:  1.1783592700958252\n",
            "Epoch  2 Iteration  23 Loss:  1.274398684501648\n",
            "Epoch  2 Iteration  24 Loss:  1.2075797319412231\n",
            "Epoch  2 Iteration  25 Loss:  0.9964463710784912\n",
            "Epoch  2 Iteration  26 Loss:  0.7986658215522766\n",
            "Epoch  2 Iteration  27 Loss:  0.8906122446060181\n",
            "Epoch  2 Iteration  28 Loss:  0.7278032302856445\n",
            "Epoch  2 Iteration  29 Loss:  1.0325363874435425\n",
            "Epoch  2 Iteration  30 Loss:  0.9588325023651123\n",
            "Epoch  2 Iteration  31 Loss:  1.022387146949768\n",
            "Epoch  2 Iteration  32 Loss:  0.9219834804534912\n",
            "Epoch  2 Iteration  33 Loss:  0.951129674911499\n",
            "Epoch  2 Iteration  34 Loss:  0.8654152750968933\n",
            "Epoch  2 Iteration  35 Loss:  0.7253332138061523\n",
            "Epoch  2 Iteration  36 Loss:  0.545648992061615\n",
            "Epoch  2 Iteration  37 Loss:  0.7858630418777466\n",
            "Epoch  2 Iteration  38 Loss:  0.8648337125778198\n",
            "Epoch  2 Iteration  39 Loss:  1.0607954263687134\n",
            "Epoch  2 Iteration  40 Loss:  0.9013619422912598\n",
            "Epoch  2 Iteration  41 Loss:  0.8629679679870605\n",
            "Epoch  2 Iteration  42 Loss:  0.8689067363739014\n",
            "Epoch  2 Iteration  43 Loss:  1.0262889862060547\n",
            "Epoch  2 Iteration  44 Loss:  0.8031971454620361\n",
            "Epoch  2 Iteration  45 Loss:  0.805761456489563\n",
            "Epoch  2 Iteration  46 Loss:  1.0810942649841309\n",
            "Epoch  2 Iteration  47 Loss:  1.0633102655410767\n",
            "Epoch  2 Iteration  48 Loss:  0.637091875076294\n",
            "Epoch  2 Iteration  49 Loss:  1.3368438482284546\n",
            "Epoch  2 Iteration  50 Loss:  0.5148165225982666\n",
            "Epoch  2 Iteration  51 Loss:  0.9798736572265625\n",
            "Epoch  2 Iteration  52 Loss:  0.6321280598640442\n",
            "Epoch  2 Iteration  53 Loss:  0.538999080657959\n",
            "Epoch  2 Iteration  54 Loss:  0.8147001266479492\n",
            "Epoch  2 Iteration  55 Loss:  0.8064650893211365\n",
            "Epoch  2 Iteration  56 Loss:  0.6347509622573853\n",
            "Epoch  2 Iteration  57 Loss:  0.6149171590805054\n",
            "Epoch  2 Iteration  58 Loss:  0.71913743019104\n",
            "Epoch  2 Iteration  59 Loss:  1.0627330541610718\n",
            "Epoch  2 Iteration  60 Loss:  0.8148006200790405\n",
            "Epoch  2 Iteration  61 Loss:  0.9882795214653015\n",
            "Epoch  2 Iteration  62 Loss:  1.0007941722869873\n",
            "Epoch  2 Iteration  63 Loss:  1.001214861869812\n",
            "Epoch  2 Iteration  64 Loss:  1.3736376762390137\n",
            "Epoch  2 Iteration  65 Loss:  1.0273993015289307\n",
            "*********************************************************************\n",
            "EPOCH  2  ==> Loss:  0.9483447445161415  Time taken ==>  227.66904640197754\n",
            "*********************************************************************\n",
            "Epoch  3 Iteration  0 Loss:  1.3147244453430176\n",
            "Epoch  3 Iteration  1 Loss:  0.8450431227684021\n",
            "Epoch  3 Iteration  2 Loss:  0.5433235168457031\n",
            "Epoch  3 Iteration  3 Loss:  0.807750940322876\n",
            "Epoch  3 Iteration  4 Loss:  0.5836614370346069\n",
            "Epoch  3 Iteration  5 Loss:  0.9562309384346008\n",
            "Epoch  3 Iteration  6 Loss:  1.3394132852554321\n",
            "Epoch  3 Iteration  7 Loss:  0.8953015208244324\n",
            "Epoch  3 Iteration  8 Loss:  1.133770227432251\n",
            "Epoch  3 Iteration  9 Loss:  0.7829161882400513\n",
            "Epoch  3 Iteration  10 Loss:  0.772178053855896\n",
            "Epoch  3 Iteration  11 Loss:  0.9386346936225891\n",
            "Epoch  3 Iteration  12 Loss:  0.8933106660842896\n",
            "Epoch  3 Iteration  13 Loss:  1.133059024810791\n",
            "Epoch  3 Iteration  14 Loss:  0.8395044803619385\n",
            "Epoch  3 Iteration  15 Loss:  0.6307492256164551\n",
            "Epoch  3 Iteration  16 Loss:  0.47403016686439514\n",
            "Epoch  3 Iteration  17 Loss:  0.8477274775505066\n",
            "Epoch  3 Iteration  18 Loss:  0.9800635576248169\n",
            "Epoch  3 Iteration  19 Loss:  1.123692274093628\n",
            "Epoch  3 Iteration  20 Loss:  0.6566648483276367\n",
            "Epoch  3 Iteration  21 Loss:  0.8233765959739685\n",
            "Epoch  3 Iteration  22 Loss:  0.7512085437774658\n",
            "Epoch  3 Iteration  23 Loss:  0.9211317896842957\n",
            "Epoch  3 Iteration  24 Loss:  0.8077807426452637\n",
            "Epoch  3 Iteration  25 Loss:  0.7488194704055786\n",
            "Epoch  3 Iteration  26 Loss:  0.8149980902671814\n",
            "Epoch  3 Iteration  27 Loss:  0.493438720703125\n",
            "Epoch  3 Iteration  28 Loss:  0.9499889016151428\n",
            "Epoch  3 Iteration  29 Loss:  0.9733635783195496\n",
            "Epoch  3 Iteration  30 Loss:  1.0010218620300293\n",
            "Epoch  3 Iteration  31 Loss:  0.6561955809593201\n",
            "Epoch  3 Iteration  32 Loss:  0.9781303405761719\n",
            "Epoch  3 Iteration  33 Loss:  1.0539230108261108\n",
            "Epoch  3 Iteration  34 Loss:  0.7984368801116943\n",
            "Epoch  3 Iteration  35 Loss:  0.8801171183586121\n",
            "Epoch  3 Iteration  36 Loss:  0.7506449222564697\n",
            "Epoch  3 Iteration  37 Loss:  1.079756736755371\n",
            "Epoch  3 Iteration  38 Loss:  0.7744907140731812\n",
            "Epoch  3 Iteration  39 Loss:  0.6396756172180176\n",
            "Epoch  3 Iteration  40 Loss:  0.7397834658622742\n",
            "Epoch  3 Iteration  41 Loss:  0.7687821388244629\n",
            "Epoch  3 Iteration  42 Loss:  0.8352391719818115\n",
            "Epoch  3 Iteration  43 Loss:  0.7270182371139526\n",
            "Epoch  3 Iteration  44 Loss:  0.7542508244514465\n",
            "Epoch  3 Iteration  45 Loss:  1.2345952987670898\n",
            "Epoch  3 Iteration  46 Loss:  0.9945628046989441\n",
            "Epoch  3 Iteration  47 Loss:  0.7944990396499634\n",
            "Epoch  3 Iteration  48 Loss:  0.7882986664772034\n",
            "Epoch  3 Iteration  49 Loss:  0.6060340404510498\n",
            "Epoch  3 Iteration  50 Loss:  0.9013702869415283\n",
            "Epoch  3 Iteration  51 Loss:  0.9910570383071899\n",
            "Epoch  3 Iteration  52 Loss:  0.9901764988899231\n",
            "Epoch  3 Iteration  53 Loss:  0.7107651829719543\n",
            "Epoch  3 Iteration  54 Loss:  0.7368085980415344\n",
            "Epoch  3 Iteration  55 Loss:  0.7595921754837036\n",
            "Epoch  3 Iteration  56 Loss:  0.7811184525489807\n",
            "Epoch  3 Iteration  57 Loss:  0.708740234375\n",
            "Epoch  3 Iteration  58 Loss:  1.1539713144302368\n",
            "Epoch  3 Iteration  59 Loss:  0.8965821266174316\n",
            "Epoch  3 Iteration  60 Loss:  0.7426140308380127\n",
            "Epoch  3 Iteration  61 Loss:  1.0786588191986084\n",
            "Epoch  3 Iteration  62 Loss:  0.41883331537246704\n",
            "Epoch  3 Iteration  63 Loss:  0.5995728373527527\n",
            "Epoch  3 Iteration  64 Loss:  0.8043423295021057\n",
            "Epoch  3 Iteration  65 Loss:  0.8007659912109375\n",
            "*********************************************************************\n",
            "EPOCH  3  ==> Loss:  0.8440345792156277  Time taken ==>  227.76983618736267\n",
            "*********************************************************************\n",
            "Epoch  4 Iteration  0 Loss:  0.5952505469322205\n",
            "Epoch  4 Iteration  1 Loss:  0.9410243034362793\n",
            "Epoch  4 Iteration  2 Loss:  0.7834429740905762\n",
            "Epoch  4 Iteration  3 Loss:  0.7431726455688477\n",
            "Epoch  4 Iteration  4 Loss:  0.861957848072052\n",
            "Epoch  4 Iteration  5 Loss:  0.6755658388137817\n",
            "Epoch  4 Iteration  6 Loss:  0.8249111175537109\n",
            "Epoch  4 Iteration  7 Loss:  0.6839556694030762\n",
            "Epoch  4 Iteration  8 Loss:  0.6498360633850098\n",
            "Epoch  4 Iteration  9 Loss:  0.6994172930717468\n",
            "Epoch  4 Iteration  10 Loss:  0.8194727897644043\n",
            "Epoch  4 Iteration  11 Loss:  0.6841893792152405\n",
            "Epoch  4 Iteration  12 Loss:  0.8616831302642822\n",
            "Epoch  4 Iteration  13 Loss:  0.8136276006698608\n",
            "Epoch  4 Iteration  14 Loss:  1.0201352834701538\n",
            "Epoch  4 Iteration  15 Loss:  0.8451752066612244\n",
            "Epoch  4 Iteration  16 Loss:  0.7408225536346436\n",
            "Epoch  4 Iteration  17 Loss:  0.7366080284118652\n",
            "Epoch  4 Iteration  18 Loss:  0.5571489334106445\n",
            "Epoch  4 Iteration  19 Loss:  0.7812926173210144\n",
            "Epoch  4 Iteration  20 Loss:  0.8597383499145508\n",
            "Epoch  4 Iteration  21 Loss:  0.7441133260726929\n",
            "Epoch  4 Iteration  22 Loss:  0.8048185110092163\n",
            "Epoch  4 Iteration  23 Loss:  0.9024156928062439\n",
            "Epoch  4 Iteration  24 Loss:  0.6762374043464661\n",
            "Epoch  4 Iteration  25 Loss:  0.8673307299613953\n",
            "Epoch  4 Iteration  26 Loss:  0.7288191318511963\n",
            "Epoch  4 Iteration  27 Loss:  0.41871002316474915\n",
            "Epoch  4 Iteration  28 Loss:  0.8664827346801758\n",
            "Epoch  4 Iteration  29 Loss:  0.8579697608947754\n",
            "Epoch  4 Iteration  30 Loss:  0.7791808843612671\n",
            "Epoch  4 Iteration  31 Loss:  0.5778638124465942\n",
            "Epoch  4 Iteration  32 Loss:  0.6663028597831726\n",
            "Epoch  4 Iteration  33 Loss:  0.7444530129432678\n",
            "Epoch  4 Iteration  34 Loss:  1.0016546249389648\n",
            "Epoch  4 Iteration  35 Loss:  0.8264505863189697\n",
            "Epoch  4 Iteration  36 Loss:  0.7073246240615845\n",
            "Epoch  4 Iteration  37 Loss:  0.7685301899909973\n",
            "Epoch  4 Iteration  38 Loss:  0.7429632544517517\n",
            "Epoch  4 Iteration  39 Loss:  0.7568764686584473\n",
            "Epoch  4 Iteration  40 Loss:  0.7929438948631287\n",
            "Epoch  4 Iteration  41 Loss:  0.7230245471000671\n",
            "Epoch  4 Iteration  42 Loss:  0.8481842279434204\n",
            "Epoch  4 Iteration  43 Loss:  0.7335652709007263\n",
            "Epoch  4 Iteration  44 Loss:  0.7462528347969055\n",
            "Epoch  4 Iteration  45 Loss:  0.8092999458312988\n",
            "Epoch  4 Iteration  46 Loss:  0.6644060015678406\n",
            "Epoch  4 Iteration  47 Loss:  0.8714677691459656\n",
            "Epoch  4 Iteration  48 Loss:  1.0075528621673584\n",
            "Epoch  4 Iteration  49 Loss:  0.6432685852050781\n",
            "Epoch  4 Iteration  50 Loss:  0.623737633228302\n",
            "Epoch  4 Iteration  51 Loss:  0.484964519739151\n",
            "Epoch  4 Iteration  52 Loss:  0.8171720504760742\n",
            "Epoch  4 Iteration  53 Loss:  0.8452559113502502\n",
            "Epoch  4 Iteration  54 Loss:  0.6316150426864624\n",
            "Epoch  4 Iteration  55 Loss:  0.4497533142566681\n",
            "Epoch  4 Iteration  56 Loss:  0.8120148181915283\n",
            "Epoch  4 Iteration  57 Loss:  0.8748448491096497\n",
            "Epoch  4 Iteration  58 Loss:  0.740578293800354\n",
            "Epoch  4 Iteration  59 Loss:  0.749986469745636\n",
            "Epoch  4 Iteration  60 Loss:  0.4621182084083557\n",
            "Epoch  4 Iteration  61 Loss:  0.5513806939125061\n",
            "Epoch  4 Iteration  62 Loss:  0.47115403413772583\n",
            "Epoch  4 Iteration  63 Loss:  0.5639113187789917\n",
            "Epoch  4 Iteration  64 Loss:  0.7839670777320862\n",
            "Epoch  4 Iteration  65 Loss:  1.0133333206176758\n",
            "*********************************************************************\n",
            "EPOCH  4  ==> Loss:  0.74746483790152  Time taken ==>  226.7720010280609\n",
            "*********************************************************************\n",
            "Epoch  5 Iteration  0 Loss:  0.8454421758651733\n",
            "Epoch  5 Iteration  1 Loss:  0.5063205361366272\n",
            "Epoch  5 Iteration  2 Loss:  0.3914330005645752\n",
            "Epoch  5 Iteration  3 Loss:  0.7733062505722046\n",
            "Epoch  5 Iteration  4 Loss:  0.6311792135238647\n",
            "Epoch  5 Iteration  5 Loss:  0.8571016788482666\n",
            "Epoch  5 Iteration  6 Loss:  0.6706219911575317\n",
            "Epoch  5 Iteration  7 Loss:  0.8737364411354065\n",
            "Epoch  5 Iteration  8 Loss:  0.5205932855606079\n",
            "Epoch  5 Iteration  9 Loss:  0.825333833694458\n",
            "Epoch  5 Iteration  10 Loss:  0.4736693501472473\n",
            "Epoch  5 Iteration  11 Loss:  0.7358564138412476\n",
            "Epoch  5 Iteration  12 Loss:  0.8152850866317749\n",
            "Epoch  5 Iteration  13 Loss:  0.7676087617874146\n",
            "Epoch  5 Iteration  14 Loss:  0.5169719457626343\n",
            "Epoch  5 Iteration  15 Loss:  0.6095730662345886\n",
            "Epoch  5 Iteration  16 Loss:  0.715675950050354\n",
            "Epoch  5 Iteration  17 Loss:  0.9149708151817322\n",
            "Epoch  5 Iteration  18 Loss:  0.752987802028656\n",
            "Epoch  5 Iteration  19 Loss:  0.3445753753185272\n",
            "Epoch  5 Iteration  20 Loss:  0.7716452479362488\n",
            "Epoch  5 Iteration  21 Loss:  0.7080416679382324\n",
            "Epoch  5 Iteration  22 Loss:  0.7723924517631531\n",
            "Epoch  5 Iteration  23 Loss:  0.5568469166755676\n",
            "Epoch  5 Iteration  24 Loss:  0.7821398377418518\n",
            "Epoch  5 Iteration  25 Loss:  0.46265530586242676\n",
            "Epoch  5 Iteration  26 Loss:  0.664649486541748\n",
            "Epoch  5 Iteration  27 Loss:  0.6963393688201904\n",
            "Epoch  5 Iteration  28 Loss:  0.6037381887435913\n",
            "Epoch  5 Iteration  29 Loss:  0.7390857934951782\n",
            "Epoch  5 Iteration  30 Loss:  0.4873543977737427\n",
            "Epoch  5 Iteration  31 Loss:  0.6976669430732727\n",
            "Epoch  5 Iteration  32 Loss:  0.7817387580871582\n",
            "Epoch  5 Iteration  33 Loss:  0.3203190565109253\n",
            "Epoch  5 Iteration  34 Loss:  0.6575989127159119\n",
            "Epoch  5 Iteration  35 Loss:  0.4877583682537079\n",
            "Epoch  5 Iteration  36 Loss:  0.727463960647583\n",
            "Epoch  5 Iteration  37 Loss:  0.3265950381755829\n",
            "Epoch  5 Iteration  38 Loss:  0.8568324446678162\n",
            "Epoch  5 Iteration  39 Loss:  0.7982383966445923\n",
            "Epoch  5 Iteration  40 Loss:  0.6813884973526001\n",
            "Epoch  5 Iteration  41 Loss:  0.7173163890838623\n",
            "Epoch  5 Iteration  42 Loss:  1.064802646636963\n",
            "Epoch  5 Iteration  43 Loss:  0.45739036798477173\n",
            "Epoch  5 Iteration  44 Loss:  0.7063901424407959\n",
            "Epoch  5 Iteration  45 Loss:  0.5135038495063782\n",
            "Epoch  5 Iteration  46 Loss:  0.6332055330276489\n",
            "Epoch  5 Iteration  47 Loss:  0.7343438863754272\n",
            "Epoch  5 Iteration  48 Loss:  0.7240778207778931\n",
            "Epoch  5 Iteration  49 Loss:  0.7212446928024292\n",
            "Epoch  5 Iteration  50 Loss:  0.9466562867164612\n",
            "Epoch  5 Iteration  51 Loss:  0.6988211274147034\n",
            "Epoch  5 Iteration  52 Loss:  0.8652104139328003\n",
            "Epoch  5 Iteration  53 Loss:  0.33503368496894836\n",
            "Epoch  5 Iteration  54 Loss:  0.8936240673065186\n",
            "Epoch  5 Iteration  55 Loss:  0.633787989616394\n",
            "Epoch  5 Iteration  56 Loss:  0.7812695503234863\n",
            "Epoch  5 Iteration  57 Loss:  0.7008959650993347\n",
            "Epoch  5 Iteration  58 Loss:  0.5006899237632751\n",
            "Epoch  5 Iteration  59 Loss:  0.880215585231781\n",
            "Epoch  5 Iteration  60 Loss:  0.8433474898338318\n",
            "Epoch  5 Iteration  61 Loss:  0.508922278881073\n",
            "Epoch  5 Iteration  62 Loss:  0.7369378209114075\n",
            "Epoch  5 Iteration  63 Loss:  0.5292261242866516\n",
            "Epoch  5 Iteration  64 Loss:  0.5821021795272827\n",
            "Epoch  5 Iteration  65 Loss:  0.816084623336792\n",
            "*********************************************************************\n",
            "EPOCH  5  ==> Loss:  0.6764823098977407  Time taken ==>  227.95094847679138\n",
            "*********************************************************************\n",
            "Epoch  6 Iteration  0 Loss:  0.8150997757911682\n",
            "Epoch  6 Iteration  1 Loss:  0.4274574816226959\n",
            "Epoch  6 Iteration  2 Loss:  0.6349413990974426\n",
            "Epoch  6 Iteration  3 Loss:  0.6092774271965027\n",
            "Epoch  6 Iteration  4 Loss:  0.4105488061904907\n",
            "Epoch  6 Iteration  5 Loss:  0.7949076890945435\n",
            "Epoch  6 Iteration  6 Loss:  0.49272629618644714\n",
            "Epoch  6 Iteration  7 Loss:  0.4769892692565918\n",
            "Epoch  6 Iteration  8 Loss:  0.7238335609436035\n",
            "Epoch  6 Iteration  9 Loss:  0.5157382488250732\n",
            "Epoch  6 Iteration  10 Loss:  1.0952246189117432\n",
            "Epoch  6 Iteration  11 Loss:  0.6282268166542053\n",
            "Epoch  6 Iteration  12 Loss:  0.8484042882919312\n",
            "Epoch  6 Iteration  13 Loss:  0.9043389558792114\n",
            "Epoch  6 Iteration  14 Loss:  0.6082360148429871\n",
            "Epoch  6 Iteration  15 Loss:  0.8651173114776611\n",
            "Epoch  6 Iteration  16 Loss:  0.5466589331626892\n",
            "Epoch  6 Iteration  17 Loss:  0.7034739851951599\n",
            "Epoch  6 Iteration  18 Loss:  0.5781797170639038\n",
            "Epoch  6 Iteration  19 Loss:  0.5131564736366272\n",
            "Epoch  6 Iteration  20 Loss:  0.5839383006095886\n",
            "Epoch  6 Iteration  21 Loss:  0.9255455136299133\n",
            "Epoch  6 Iteration  22 Loss:  0.6308366060256958\n",
            "Epoch  6 Iteration  23 Loss:  0.6671025156974792\n",
            "Epoch  6 Iteration  24 Loss:  0.47738778591156006\n",
            "Epoch  6 Iteration  25 Loss:  0.4825432002544403\n",
            "Epoch  6 Iteration  26 Loss:  0.48644548654556274\n",
            "Epoch  6 Iteration  27 Loss:  1.196933388710022\n",
            "Epoch  6 Iteration  28 Loss:  0.6924272775650024\n",
            "Epoch  6 Iteration  29 Loss:  0.723436176776886\n",
            "Epoch  6 Iteration  30 Loss:  0.6680374145507812\n",
            "Epoch  6 Iteration  31 Loss:  0.6333411931991577\n",
            "Epoch  6 Iteration  32 Loss:  0.6013930439949036\n",
            "Epoch  6 Iteration  33 Loss:  0.431270569562912\n",
            "Epoch  6 Iteration  34 Loss:  0.7924257516860962\n",
            "Epoch  6 Iteration  35 Loss:  0.49340522289276123\n",
            "Epoch  6 Iteration  36 Loss:  0.5684888958930969\n",
            "Epoch  6 Iteration  37 Loss:  0.7563648223876953\n",
            "Epoch  6 Iteration  38 Loss:  0.5605980157852173\n",
            "Epoch  6 Iteration  39 Loss:  0.33048009872436523\n",
            "Epoch  6 Iteration  40 Loss:  0.5564566850662231\n",
            "Epoch  6 Iteration  41 Loss:  0.5909600257873535\n",
            "Epoch  6 Iteration  42 Loss:  0.4945294260978699\n",
            "Epoch  6 Iteration  43 Loss:  0.45535916090011597\n",
            "Epoch  6 Iteration  44 Loss:  0.6794034242630005\n",
            "Epoch  6 Iteration  45 Loss:  0.5906189680099487\n",
            "Epoch  6 Iteration  46 Loss:  1.0124609470367432\n",
            "Epoch  6 Iteration  47 Loss:  0.788931131362915\n",
            "Epoch  6 Iteration  48 Loss:  0.8080679178237915\n",
            "Epoch  6 Iteration  49 Loss:  0.5749748945236206\n",
            "Epoch  6 Iteration  50 Loss:  0.5745819807052612\n",
            "Epoch  6 Iteration  51 Loss:  0.4638301134109497\n",
            "Epoch  6 Iteration  52 Loss:  0.7831811308860779\n",
            "Epoch  6 Iteration  53 Loss:  0.4250040054321289\n",
            "Epoch  6 Iteration  54 Loss:  0.6952465176582336\n",
            "Epoch  6 Iteration  55 Loss:  0.7564324140548706\n",
            "Epoch  6 Iteration  56 Loss:  0.5755279660224915\n",
            "Epoch  6 Iteration  57 Loss:  0.5386548638343811\n",
            "Epoch  6 Iteration  58 Loss:  0.8547455072402954\n",
            "Epoch  6 Iteration  59 Loss:  0.46451619267463684\n",
            "Epoch  6 Iteration  60 Loss:  0.7625845074653625\n",
            "Epoch  6 Iteration  61 Loss:  0.7432950139045715\n",
            "Epoch  6 Iteration  62 Loss:  0.48950517177581787\n",
            "Epoch  6 Iteration  63 Loss:  0.580602765083313\n",
            "Epoch  6 Iteration  64 Loss:  0.4737575054168701\n",
            "Epoch  6 Iteration  65 Loss:  0.6995356678962708\n",
            "*********************************************************************\n",
            "EPOCH  6  ==> Loss:  0.6413288220311656  Time taken ==>  228.69835567474365\n",
            "*********************************************************************\n",
            "Epoch  7 Iteration  0 Loss:  0.42932814359664917\n",
            "Epoch  7 Iteration  1 Loss:  0.46083927154541016\n",
            "Epoch  7 Iteration  2 Loss:  0.7839316129684448\n",
            "Epoch  7 Iteration  3 Loss:  0.6407762169837952\n",
            "Epoch  7 Iteration  4 Loss:  0.5236311554908752\n",
            "Epoch  7 Iteration  5 Loss:  0.5902096033096313\n",
            "Epoch  7 Iteration  6 Loss:  0.5439509749412537\n",
            "Epoch  7 Iteration  7 Loss:  0.7218530178070068\n",
            "Epoch  7 Iteration  8 Loss:  0.27841171622276306\n",
            "Epoch  7 Iteration  9 Loss:  0.5626388788223267\n",
            "Epoch  7 Iteration  10 Loss:  0.6651983857154846\n",
            "Epoch  7 Iteration  11 Loss:  0.7367537021636963\n",
            "Epoch  7 Iteration  12 Loss:  0.5749794840812683\n",
            "Epoch  7 Iteration  13 Loss:  0.6101998090744019\n",
            "Epoch  7 Iteration  14 Loss:  0.4332316517829895\n",
            "Epoch  7 Iteration  15 Loss:  0.6368705034255981\n",
            "Epoch  7 Iteration  16 Loss:  0.7475327849388123\n",
            "Epoch  7 Iteration  17 Loss:  0.5479061007499695\n",
            "Epoch  7 Iteration  18 Loss:  0.4126482605934143\n",
            "Epoch  7 Iteration  19 Loss:  0.7182098627090454\n",
            "Epoch  7 Iteration  20 Loss:  0.4184114336967468\n",
            "Epoch  7 Iteration  21 Loss:  0.37415608763694763\n",
            "Epoch  7 Iteration  22 Loss:  0.6282687187194824\n",
            "Epoch  7 Iteration  23 Loss:  0.542291522026062\n",
            "Epoch  7 Iteration  24 Loss:  0.5645785331726074\n",
            "Epoch  7 Iteration  25 Loss:  0.7096670269966125\n",
            "Epoch  7 Iteration  26 Loss:  0.5886895656585693\n",
            "Epoch  7 Iteration  27 Loss:  0.7816639542579651\n",
            "Epoch  7 Iteration  28 Loss:  0.6208242177963257\n",
            "Epoch  7 Iteration  29 Loss:  0.5792601704597473\n",
            "Epoch  7 Iteration  30 Loss:  0.5299230217933655\n",
            "Epoch  7 Iteration  31 Loss:  0.8462203741073608\n",
            "Epoch  7 Iteration  32 Loss:  0.46990036964416504\n",
            "Epoch  7 Iteration  33 Loss:  0.38869738578796387\n",
            "Epoch  7 Iteration  34 Loss:  0.5975066423416138\n",
            "Epoch  7 Iteration  35 Loss:  0.700840950012207\n",
            "Epoch  7 Iteration  36 Loss:  0.9164945483207703\n",
            "Epoch  7 Iteration  37 Loss:  0.5218603610992432\n",
            "Epoch  7 Iteration  38 Loss:  0.4798695147037506\n",
            "Epoch  7 Iteration  39 Loss:  0.5252013206481934\n",
            "Epoch  7 Iteration  40 Loss:  0.3287951946258545\n",
            "Epoch  7 Iteration  41 Loss:  0.6658684015274048\n",
            "Epoch  7 Iteration  42 Loss:  0.7707890868186951\n",
            "Epoch  7 Iteration  43 Loss:  0.8903620839118958\n",
            "Epoch  7 Iteration  44 Loss:  0.5381231307983398\n",
            "Epoch  7 Iteration  45 Loss:  0.4999721050262451\n",
            "Epoch  7 Iteration  46 Loss:  0.35125482082366943\n",
            "Epoch  7 Iteration  47 Loss:  0.4969044625759125\n",
            "Epoch  7 Iteration  48 Loss:  0.6871188282966614\n",
            "Epoch  7 Iteration  49 Loss:  0.6520038843154907\n",
            "Epoch  7 Iteration  50 Loss:  0.5609063506126404\n",
            "Epoch  7 Iteration  51 Loss:  0.4705369472503662\n",
            "Epoch  7 Iteration  52 Loss:  0.5028926134109497\n",
            "Epoch  7 Iteration  53 Loss:  0.5584027767181396\n",
            "Epoch  7 Iteration  54 Loss:  0.6831289529800415\n",
            "Epoch  7 Iteration  55 Loss:  0.5410352945327759\n",
            "Epoch  7 Iteration  56 Loss:  0.7493910789489746\n",
            "Epoch  7 Iteration  57 Loss:  0.6623284816741943\n",
            "Epoch  7 Iteration  58 Loss:  0.6293711066246033\n",
            "Epoch  7 Iteration  59 Loss:  0.4303409457206726\n",
            "Epoch  7 Iteration  60 Loss:  0.41820603609085083\n",
            "Epoch  7 Iteration  61 Loss:  0.6462829113006592\n",
            "Epoch  7 Iteration  62 Loss:  0.7354921698570251\n",
            "Epoch  7 Iteration  63 Loss:  0.7662383317947388\n",
            "Epoch  7 Iteration  64 Loss:  0.6665436029434204\n",
            "Epoch  7 Iteration  65 Loss:  0.46190738677978516\n",
            "*********************************************************************\n",
            "EPOCH  7  ==> Loss:  0.5873882400267052  Time taken ==>  227.6727809906006\n",
            "*********************************************************************\n",
            "Epoch  8 Iteration  0 Loss:  0.6375590562820435\n",
            "Epoch  8 Iteration  1 Loss:  0.5497280359268188\n",
            "Epoch  8 Iteration  2 Loss:  0.44498276710510254\n",
            "Epoch  8 Iteration  3 Loss:  0.25703904032707214\n",
            "Epoch  8 Iteration  4 Loss:  0.7092016935348511\n",
            "Epoch  8 Iteration  5 Loss:  0.7036457657814026\n",
            "Epoch  8 Iteration  6 Loss:  0.5456081628799438\n",
            "Epoch  8 Iteration  7 Loss:  0.41659456491470337\n",
            "Epoch  8 Iteration  8 Loss:  0.553763747215271\n",
            "Epoch  8 Iteration  9 Loss:  0.6626559495925903\n",
            "Epoch  8 Iteration  10 Loss:  0.7196431159973145\n",
            "Epoch  8 Iteration  11 Loss:  1.0325024127960205\n",
            "Epoch  8 Iteration  12 Loss:  0.47997888922691345\n",
            "Epoch  8 Iteration  13 Loss:  0.8082787394523621\n",
            "Epoch  8 Iteration  14 Loss:  0.5708130598068237\n",
            "Epoch  8 Iteration  15 Loss:  0.6491406559944153\n",
            "Epoch  8 Iteration  16 Loss:  0.6651906967163086\n",
            "Epoch  8 Iteration  17 Loss:  0.8696860074996948\n",
            "Epoch  8 Iteration  18 Loss:  0.7327392101287842\n",
            "Epoch  8 Iteration  19 Loss:  0.6852830648422241\n",
            "Epoch  8 Iteration  20 Loss:  0.44643378257751465\n",
            "Epoch  8 Iteration  21 Loss:  0.5855174660682678\n",
            "Epoch  8 Iteration  22 Loss:  0.35034626722335815\n",
            "Epoch  8 Iteration  23 Loss:  0.40879026055336\n",
            "Epoch  8 Iteration  24 Loss:  0.6246552467346191\n",
            "Epoch  8 Iteration  25 Loss:  0.8472253084182739\n",
            "Epoch  8 Iteration  26 Loss:  0.5040562152862549\n",
            "Epoch  8 Iteration  27 Loss:  0.5698708295822144\n",
            "Epoch  8 Iteration  28 Loss:  0.5455498695373535\n",
            "Epoch  8 Iteration  29 Loss:  0.8497896194458008\n",
            "Epoch  8 Iteration  30 Loss:  0.33025169372558594\n",
            "Epoch  8 Iteration  31 Loss:  0.5620976090431213\n",
            "Epoch  8 Iteration  32 Loss:  0.3852980434894562\n",
            "Epoch  8 Iteration  33 Loss:  0.7258449792861938\n",
            "Epoch  8 Iteration  34 Loss:  0.2673889100551605\n",
            "Epoch  8 Iteration  35 Loss:  0.6422148942947388\n",
            "Epoch  8 Iteration  36 Loss:  0.794469952583313\n",
            "Epoch  8 Iteration  37 Loss:  0.5956486463546753\n",
            "Epoch  8 Iteration  38 Loss:  0.8009636402130127\n",
            "Epoch  8 Iteration  39 Loss:  0.324383020401001\n",
            "Epoch  8 Iteration  40 Loss:  0.3828777074813843\n",
            "Epoch  8 Iteration  41 Loss:  0.44161003828048706\n",
            "Epoch  8 Iteration  42 Loss:  0.46414420008659363\n",
            "Epoch  8 Iteration  43 Loss:  0.621813178062439\n",
            "Epoch  8 Iteration  44 Loss:  0.7532833218574524\n",
            "Epoch  8 Iteration  45 Loss:  0.5343235731124878\n",
            "Epoch  8 Iteration  46 Loss:  0.4775910973548889\n",
            "Epoch  8 Iteration  47 Loss:  0.47606855630874634\n",
            "Epoch  8 Iteration  48 Loss:  0.5896673798561096\n",
            "Epoch  8 Iteration  49 Loss:  0.38856419920921326\n",
            "Epoch  8 Iteration  50 Loss:  0.499702125787735\n",
            "Epoch  8 Iteration  51 Loss:  0.43957415223121643\n",
            "Epoch  8 Iteration  52 Loss:  0.5061589479446411\n",
            "Epoch  8 Iteration  53 Loss:  0.5228918790817261\n",
            "Epoch  8 Iteration  54 Loss:  0.5887863039970398\n",
            "Epoch  8 Iteration  55 Loss:  0.6188781261444092\n",
            "Epoch  8 Iteration  56 Loss:  0.502065122127533\n",
            "Epoch  8 Iteration  57 Loss:  0.6513206958770752\n",
            "Epoch  8 Iteration  58 Loss:  0.4005849361419678\n",
            "Epoch  8 Iteration  59 Loss:  0.7245422005653381\n",
            "Epoch  8 Iteration  60 Loss:  0.5332757234573364\n",
            "Epoch  8 Iteration  61 Loss:  0.6522431373596191\n",
            "Epoch  8 Iteration  62 Loss:  0.32004231214523315\n",
            "Epoch  8 Iteration  63 Loss:  0.3758890628814697\n",
            "Epoch  8 Iteration  64 Loss:  0.6725493669509888\n",
            "Epoch  8 Iteration  65 Loss:  0.5735006928443909\n",
            "*********************************************************************\n",
            "EPOCH  8  ==> Loss:  0.5691936201218403  Time taken ==>  229.3972294330597\n",
            "*********************************************************************\n",
            "Epoch  9 Iteration  0 Loss:  0.9001412391662598\n",
            "Epoch  9 Iteration  1 Loss:  0.4595322012901306\n",
            "Epoch  9 Iteration  2 Loss:  0.469780832529068\n",
            "Epoch  9 Iteration  3 Loss:  0.6816936731338501\n",
            "Epoch  9 Iteration  4 Loss:  0.7432494163513184\n",
            "Epoch  9 Iteration  5 Loss:  0.47760009765625\n",
            "Epoch  9 Iteration  6 Loss:  0.3469255566596985\n",
            "Epoch  9 Iteration  7 Loss:  0.42199844121932983\n",
            "Epoch  9 Iteration  8 Loss:  0.38572269678115845\n",
            "Epoch  9 Iteration  9 Loss:  0.6348584890365601\n",
            "Epoch  9 Iteration  10 Loss:  0.48131969571113586\n",
            "Epoch  9 Iteration  11 Loss:  0.30615586042404175\n",
            "Epoch  9 Iteration  12 Loss:  0.3763551115989685\n",
            "Epoch  9 Iteration  13 Loss:  0.5031597018241882\n",
            "Epoch  9 Iteration  14 Loss:  0.6647374033927917\n",
            "Epoch  9 Iteration  15 Loss:  0.5229007601737976\n",
            "Epoch  9 Iteration  16 Loss:  0.3028219938278198\n",
            "Epoch  9 Iteration  17 Loss:  0.8055384159088135\n",
            "Epoch  9 Iteration  18 Loss:  0.3840809464454651\n",
            "Epoch  9 Iteration  19 Loss:  0.6249763369560242\n",
            "Epoch  9 Iteration  20 Loss:  0.5518033504486084\n",
            "Epoch  9 Iteration  21 Loss:  0.4623146057128906\n",
            "Epoch  9 Iteration  22 Loss:  0.7476903200149536\n",
            "Epoch  9 Iteration  23 Loss:  0.5721556544303894\n",
            "Epoch  9 Iteration  24 Loss:  0.5494024753570557\n",
            "Epoch  9 Iteration  25 Loss:  0.5768612623214722\n",
            "Epoch  9 Iteration  26 Loss:  0.39750927686691284\n",
            "Epoch  9 Iteration  27 Loss:  0.5225470662117004\n",
            "Epoch  9 Iteration  28 Loss:  0.4684956669807434\n",
            "Epoch  9 Iteration  29 Loss:  0.45537394285202026\n",
            "Epoch  9 Iteration  30 Loss:  0.552413821220398\n",
            "Epoch  9 Iteration  31 Loss:  0.4393095374107361\n",
            "Epoch  9 Iteration  32 Loss:  0.4509865641593933\n",
            "Epoch  9 Iteration  33 Loss:  0.5316339135169983\n",
            "Epoch  9 Iteration  34 Loss:  0.44568997621536255\n",
            "Epoch  9 Iteration  35 Loss:  0.6590742468833923\n",
            "Epoch  9 Iteration  36 Loss:  0.4525384306907654\n",
            "Epoch  9 Iteration  37 Loss:  0.36455345153808594\n",
            "Epoch  9 Iteration  38 Loss:  0.6419854760169983\n",
            "Epoch  9 Iteration  39 Loss:  0.5729051828384399\n",
            "Epoch  9 Iteration  40 Loss:  0.6821408271789551\n",
            "Epoch  9 Iteration  41 Loss:  0.42466193437576294\n",
            "Epoch  9 Iteration  42 Loss:  0.449710488319397\n",
            "Epoch  9 Iteration  43 Loss:  0.3163761794567108\n",
            "Epoch  9 Iteration  44 Loss:  0.5205544233322144\n",
            "Epoch  9 Iteration  45 Loss:  0.47743842005729675\n",
            "Epoch  9 Iteration  46 Loss:  0.42068779468536377\n",
            "Epoch  9 Iteration  47 Loss:  0.45862066745758057\n",
            "Epoch  9 Iteration  48 Loss:  0.7999552488327026\n",
            "Epoch  9 Iteration  49 Loss:  0.7994809746742249\n",
            "Epoch  9 Iteration  50 Loss:  0.4014121890068054\n",
            "Epoch  9 Iteration  51 Loss:  0.6832769513130188\n",
            "Epoch  9 Iteration  52 Loss:  0.5661864280700684\n",
            "Epoch  9 Iteration  53 Loss:  0.3026231527328491\n",
            "Epoch  9 Iteration  54 Loss:  0.698195219039917\n",
            "Epoch  9 Iteration  55 Loss:  0.47200077772140503\n",
            "Epoch  9 Iteration  56 Loss:  0.5077115297317505\n",
            "Epoch  9 Iteration  57 Loss:  1.0312681198120117\n",
            "Epoch  9 Iteration  58 Loss:  0.5129674077033997\n",
            "Epoch  9 Iteration  59 Loss:  0.7347065210342407\n",
            "Epoch  9 Iteration  60 Loss:  0.4260503649711609\n",
            "Epoch  9 Iteration  61 Loss:  0.4719061553478241\n",
            "Epoch  9 Iteration  62 Loss:  0.43417370319366455\n",
            "Epoch  9 Iteration  63 Loss:  0.6508371233940125\n",
            "Epoch  9 Iteration  64 Loss:  0.4067673087120056\n",
            "Epoch  9 Iteration  65 Loss:  0.35655346512794495\n",
            "*********************************************************************\n",
            "EPOCH  9  ==> Loss:  0.5290160070766102  Time taken ==>  229.8266887664795\n",
            "*********************************************************************\n",
            "Epoch  10 Iteration  0 Loss:  0.42459285259246826\n",
            "Epoch  10 Iteration  1 Loss:  0.8127210140228271\n",
            "Epoch  10 Iteration  2 Loss:  0.430494487285614\n",
            "Epoch  10 Iteration  3 Loss:  0.7932872176170349\n",
            "Epoch  10 Iteration  4 Loss:  0.612830638885498\n",
            "Epoch  10 Iteration  5 Loss:  0.45453912019729614\n",
            "Epoch  10 Iteration  6 Loss:  0.4866434335708618\n",
            "Epoch  10 Iteration  7 Loss:  0.5380669236183167\n",
            "Epoch  10 Iteration  8 Loss:  0.5632022023200989\n",
            "Epoch  10 Iteration  9 Loss:  0.43892428278923035\n",
            "Epoch  10 Iteration  10 Loss:  0.3490351736545563\n",
            "Epoch  10 Iteration  11 Loss:  0.7300477027893066\n",
            "Epoch  10 Iteration  12 Loss:  0.5018485188484192\n",
            "Epoch  10 Iteration  13 Loss:  0.2978302836418152\n",
            "Epoch  10 Iteration  14 Loss:  0.5587164163589478\n",
            "Epoch  10 Iteration  15 Loss:  0.8129145503044128\n",
            "Epoch  10 Iteration  16 Loss:  0.40880584716796875\n",
            "Epoch  10 Iteration  17 Loss:  0.6644023656845093\n",
            "Epoch  10 Iteration  18 Loss:  0.3922704756259918\n",
            "Epoch  10 Iteration  19 Loss:  0.37675848603248596\n",
            "Epoch  10 Iteration  20 Loss:  0.3605802059173584\n",
            "Epoch  10 Iteration  21 Loss:  0.3411194086074829\n",
            "Epoch  10 Iteration  22 Loss:  0.5141840577125549\n",
            "Epoch  10 Iteration  23 Loss:  0.560595691204071\n",
            "Epoch  10 Iteration  24 Loss:  0.4756954312324524\n",
            "Epoch  10 Iteration  25 Loss:  0.4309126138687134\n",
            "Epoch  10 Iteration  26 Loss:  0.7401250600814819\n",
            "Epoch  10 Iteration  27 Loss:  0.5472010374069214\n",
            "Epoch  10 Iteration  28 Loss:  0.6126771569252014\n",
            "Epoch  10 Iteration  29 Loss:  0.44247275590896606\n",
            "Epoch  10 Iteration  30 Loss:  0.6146308183670044\n",
            "Epoch  10 Iteration  31 Loss:  0.41924965381622314\n",
            "Epoch  10 Iteration  32 Loss:  0.4505763649940491\n",
            "Epoch  10 Iteration  33 Loss:  0.4386198818683624\n",
            "Epoch  10 Iteration  34 Loss:  0.5224350094795227\n",
            "Epoch  10 Iteration  35 Loss:  0.24131926894187927\n",
            "Epoch  10 Iteration  36 Loss:  0.5081470608711243\n",
            "Epoch  10 Iteration  37 Loss:  0.5656506419181824\n",
            "Epoch  10 Iteration  38 Loss:  0.5963847041130066\n",
            "Epoch  10 Iteration  39 Loss:  0.5113778114318848\n",
            "Epoch  10 Iteration  40 Loss:  0.5913254022598267\n",
            "Epoch  10 Iteration  41 Loss:  0.4447855055332184\n",
            "Epoch  10 Iteration  42 Loss:  0.6828442215919495\n",
            "Epoch  10 Iteration  43 Loss:  0.4901963472366333\n",
            "Epoch  10 Iteration  44 Loss:  0.47579437494277954\n",
            "Epoch  10 Iteration  45 Loss:  0.5062770247459412\n",
            "Epoch  10 Iteration  46 Loss:  0.837381899356842\n",
            "Epoch  10 Iteration  47 Loss:  0.6419275999069214\n",
            "Epoch  10 Iteration  48 Loss:  0.27956458926200867\n",
            "Epoch  10 Iteration  49 Loss:  0.38311728835105896\n",
            "Epoch  10 Iteration  50 Loss:  0.614795446395874\n",
            "Epoch  10 Iteration  51 Loss:  0.5997834801673889\n",
            "Epoch  10 Iteration  52 Loss:  0.5214600563049316\n",
            "Epoch  10 Iteration  53 Loss:  0.7512674331665039\n",
            "Epoch  10 Iteration  54 Loss:  0.5795013904571533\n",
            "Epoch  10 Iteration  55 Loss:  0.47216811776161194\n",
            "Epoch  10 Iteration  56 Loss:  0.4740505814552307\n",
            "Epoch  10 Iteration  57 Loss:  0.490378737449646\n",
            "Epoch  10 Iteration  58 Loss:  0.48248347640037537\n",
            "Epoch  10 Iteration  59 Loss:  0.5290758013725281\n",
            "Epoch  10 Iteration  60 Loss:  0.6389645338058472\n",
            "Epoch  10 Iteration  61 Loss:  0.46723872423171997\n",
            "Epoch  10 Iteration  62 Loss:  0.37522774934768677\n",
            "Epoch  10 Iteration  63 Loss:  0.3077279031276703\n",
            "Epoch  10 Iteration  64 Loss:  0.289378821849823\n",
            "Epoch  10 Iteration  65 Loss:  0.4660452902317047\n",
            "*********************************************************************\n",
            "EPOCH  10  ==> Loss:  0.514161339763439  Time taken ==>  228.9814715385437\n",
            "*********************************************************************\n",
            "Epoch  11 Iteration  0 Loss:  0.4376533627510071\n",
            "Epoch  11 Iteration  1 Loss:  0.5675250291824341\n",
            "Epoch  11 Iteration  2 Loss:  0.34857410192489624\n",
            "Epoch  11 Iteration  3 Loss:  0.519196629524231\n",
            "Epoch  11 Iteration  4 Loss:  0.3625391721725464\n",
            "Epoch  11 Iteration  5 Loss:  0.5505338907241821\n",
            "Epoch  11 Iteration  6 Loss:  0.31076186895370483\n",
            "Epoch  11 Iteration  7 Loss:  0.568005383014679\n",
            "Epoch  11 Iteration  8 Loss:  0.7482365369796753\n",
            "Epoch  11 Iteration  9 Loss:  0.62589430809021\n",
            "Epoch  11 Iteration  10 Loss:  0.49317729473114014\n",
            "Epoch  11 Iteration  11 Loss:  0.4748299717903137\n",
            "Epoch  11 Iteration  12 Loss:  0.4056939482688904\n",
            "Epoch  11 Iteration  13 Loss:  0.3145643472671509\n",
            "Epoch  11 Iteration  14 Loss:  0.46969956159591675\n",
            "Epoch  11 Iteration  15 Loss:  0.49775683879852295\n",
            "Epoch  11 Iteration  16 Loss:  0.36358803510665894\n",
            "Epoch  11 Iteration  17 Loss:  0.3197096288204193\n",
            "Epoch  11 Iteration  18 Loss:  0.6425977945327759\n",
            "Epoch  11 Iteration  19 Loss:  0.46462011337280273\n",
            "Epoch  11 Iteration  20 Loss:  0.45991334319114685\n",
            "Epoch  11 Iteration  21 Loss:  0.36745601892471313\n",
            "Epoch  11 Iteration  22 Loss:  0.30385392904281616\n",
            "Epoch  11 Iteration  23 Loss:  0.32072001695632935\n",
            "Epoch  11 Iteration  24 Loss:  0.5519349575042725\n",
            "Epoch  11 Iteration  25 Loss:  0.3277401328086853\n",
            "Epoch  11 Iteration  26 Loss:  0.22995121777057648\n",
            "Epoch  11 Iteration  27 Loss:  0.5103346109390259\n",
            "Epoch  11 Iteration  28 Loss:  0.38540080189704895\n",
            "Epoch  11 Iteration  29 Loss:  0.4719710946083069\n",
            "Epoch  11 Iteration  30 Loss:  0.6150996088981628\n",
            "Epoch  11 Iteration  31 Loss:  0.38783732056617737\n",
            "Epoch  11 Iteration  32 Loss:  0.5174793601036072\n",
            "Epoch  11 Iteration  33 Loss:  0.41884011030197144\n",
            "Epoch  11 Iteration  34 Loss:  0.4997733533382416\n",
            "Epoch  11 Iteration  35 Loss:  0.43341171741485596\n",
            "Epoch  11 Iteration  36 Loss:  0.5511175394058228\n",
            "Epoch  11 Iteration  37 Loss:  0.44416511058807373\n",
            "Epoch  11 Iteration  38 Loss:  0.34853705763816833\n",
            "Epoch  11 Iteration  39 Loss:  0.6093528270721436\n",
            "Epoch  11 Iteration  40 Loss:  0.3265398144721985\n",
            "Epoch  11 Iteration  41 Loss:  0.483175665140152\n",
            "Epoch  11 Iteration  42 Loss:  0.5145049095153809\n",
            "Epoch  11 Iteration  43 Loss:  0.295124351978302\n",
            "Epoch  11 Iteration  44 Loss:  0.555747389793396\n",
            "Epoch  11 Iteration  45 Loss:  0.47475743293762207\n",
            "Epoch  11 Iteration  46 Loss:  0.781753420829773\n",
            "Epoch  11 Iteration  47 Loss:  0.601912796497345\n",
            "Epoch  11 Iteration  48 Loss:  0.23338720202445984\n",
            "Epoch  11 Iteration  49 Loss:  0.3722958564758301\n",
            "Epoch  11 Iteration  50 Loss:  0.35059836506843567\n",
            "Epoch  11 Iteration  51 Loss:  0.345363974571228\n",
            "Epoch  11 Iteration  52 Loss:  0.815453290939331\n",
            "Epoch  11 Iteration  53 Loss:  0.5897243022918701\n",
            "Epoch  11 Iteration  54 Loss:  0.5715641379356384\n",
            "Epoch  11 Iteration  55 Loss:  0.5001686215400696\n",
            "Epoch  11 Iteration  56 Loss:  0.36283963918685913\n",
            "Epoch  11 Iteration  57 Loss:  0.6932337880134583\n",
            "Epoch  11 Iteration  58 Loss:  0.496829628944397\n",
            "Epoch  11 Iteration  59 Loss:  0.3482295572757721\n",
            "Epoch  11 Iteration  60 Loss:  1.1153697967529297\n",
            "Epoch  11 Iteration  61 Loss:  0.549553632736206\n",
            "Epoch  11 Iteration  62 Loss:  0.43106958270072937\n",
            "Epoch  11 Iteration  63 Loss:  0.6203355193138123\n",
            "Epoch  11 Iteration  64 Loss:  0.5368615388870239\n",
            "Epoch  11 Iteration  65 Loss:  0.2949017882347107\n",
            "*********************************************************************\n",
            "EPOCH  11  ==> Loss:  0.477293090161049  Time taken ==>  229.43180346488953\n",
            "*********************************************************************\n",
            "Epoch  12 Iteration  0 Loss:  0.6291836500167847\n",
            "Epoch  12 Iteration  1 Loss:  0.6063407063484192\n",
            "Epoch  12 Iteration  2 Loss:  0.4766182005405426\n",
            "Epoch  12 Iteration  3 Loss:  0.6188594698905945\n",
            "Epoch  12 Iteration  4 Loss:  0.5084705948829651\n",
            "Epoch  12 Iteration  5 Loss:  0.7541105151176453\n",
            "Epoch  12 Iteration  6 Loss:  0.4386534094810486\n",
            "Epoch  12 Iteration  7 Loss:  0.4085562229156494\n",
            "Epoch  12 Iteration  8 Loss:  0.5731265544891357\n",
            "Epoch  12 Iteration  9 Loss:  0.5474600791931152\n",
            "Epoch  12 Iteration  10 Loss:  0.34928953647613525\n",
            "Epoch  12 Iteration  11 Loss:  0.6831470727920532\n",
            "Epoch  12 Iteration  12 Loss:  0.4908010959625244\n",
            "Epoch  12 Iteration  13 Loss:  0.3292295038700104\n",
            "Epoch  12 Iteration  14 Loss:  0.40209001302719116\n",
            "Epoch  12 Iteration  15 Loss:  0.4647042155265808\n",
            "Epoch  12 Iteration  16 Loss:  0.571554958820343\n",
            "Epoch  12 Iteration  17 Loss:  0.31055083870887756\n",
            "Epoch  12 Iteration  18 Loss:  0.47510963678359985\n",
            "Epoch  12 Iteration  19 Loss:  0.3197556436061859\n",
            "Epoch  12 Iteration  20 Loss:  0.5070055723190308\n",
            "Epoch  12 Iteration  21 Loss:  0.43114906549453735\n",
            "Epoch  12 Iteration  22 Loss:  0.6210682392120361\n",
            "Epoch  12 Iteration  23 Loss:  0.37634819746017456\n",
            "Epoch  12 Iteration  24 Loss:  0.46347421407699585\n",
            "Epoch  12 Iteration  25 Loss:  0.3229951858520508\n",
            "Epoch  12 Iteration  26 Loss:  0.6114861965179443\n",
            "Epoch  12 Iteration  27 Loss:  0.42190027236938477\n",
            "Epoch  12 Iteration  28 Loss:  0.5746412873268127\n",
            "Epoch  12 Iteration  29 Loss:  0.5151162147521973\n",
            "Epoch  12 Iteration  30 Loss:  0.15964548289775848\n",
            "Epoch  12 Iteration  31 Loss:  0.3508880138397217\n",
            "Epoch  12 Iteration  32 Loss:  0.3847561478614807\n",
            "Epoch  12 Iteration  33 Loss:  0.517775297164917\n",
            "Epoch  12 Iteration  34 Loss:  0.42535799741744995\n",
            "Epoch  12 Iteration  35 Loss:  0.41931408643722534\n",
            "Epoch  12 Iteration  36 Loss:  0.6764557361602783\n",
            "Epoch  12 Iteration  37 Loss:  0.3548305928707123\n",
            "Epoch  12 Iteration  38 Loss:  0.28299614787101746\n",
            "Epoch  12 Iteration  39 Loss:  0.6054717302322388\n",
            "Epoch  12 Iteration  40 Loss:  0.32895180583000183\n",
            "Epoch  12 Iteration  41 Loss:  0.6965537667274475\n",
            "Epoch  12 Iteration  42 Loss:  0.3374309837818146\n",
            "Epoch  12 Iteration  43 Loss:  0.6407182216644287\n",
            "Epoch  12 Iteration  44 Loss:  0.40852829813957214\n",
            "Epoch  12 Iteration  45 Loss:  0.28560853004455566\n",
            "Epoch  12 Iteration  46 Loss:  0.3938361406326294\n",
            "Epoch  12 Iteration  47 Loss:  0.19911421835422516\n",
            "Epoch  12 Iteration  48 Loss:  0.5389405488967896\n",
            "Epoch  12 Iteration  49 Loss:  0.27968016266822815\n",
            "Epoch  12 Iteration  50 Loss:  0.506078839302063\n",
            "Epoch  12 Iteration  51 Loss:  0.5795696377754211\n",
            "Epoch  12 Iteration  52 Loss:  0.3509361147880554\n",
            "Epoch  12 Iteration  53 Loss:  0.3727157711982727\n",
            "Epoch  12 Iteration  54 Loss:  0.6187611222267151\n",
            "Epoch  12 Iteration  55 Loss:  0.5026265978813171\n",
            "Epoch  12 Iteration  56 Loss:  0.39144933223724365\n",
            "Epoch  12 Iteration  57 Loss:  0.5582037568092346\n",
            "Epoch  12 Iteration  58 Loss:  0.48212045431137085\n",
            "Epoch  12 Iteration  59 Loss:  0.4543788433074951\n",
            "Epoch  12 Iteration  60 Loss:  0.3348856270313263\n",
            "Epoch  12 Iteration  61 Loss:  0.6039737462997437\n",
            "Epoch  12 Iteration  62 Loss:  0.6526479125022888\n",
            "Epoch  12 Iteration  63 Loss:  0.5005710124969482\n",
            "Epoch  12 Iteration  64 Loss:  0.36865705251693726\n",
            "Epoch  12 Iteration  65 Loss:  0.6487846374511719\n",
            "*********************************************************************\n",
            "EPOCH  12  ==> Loss:  0.4699395565372525  Time taken ==>  228.2088544368744\n",
            "*********************************************************************\n",
            "Epoch  13 Iteration  0 Loss:  0.4235486090183258\n",
            "Epoch  13 Iteration  1 Loss:  0.4299759864807129\n",
            "Epoch  13 Iteration  2 Loss:  0.24597835540771484\n",
            "Epoch  13 Iteration  3 Loss:  0.673080563545227\n",
            "Epoch  13 Iteration  4 Loss:  0.5403209924697876\n",
            "Epoch  13 Iteration  5 Loss:  0.5350029468536377\n",
            "Epoch  13 Iteration  6 Loss:  0.342987060546875\n",
            "Epoch  13 Iteration  7 Loss:  0.5851912498474121\n",
            "Epoch  13 Iteration  8 Loss:  0.39220130443573\n",
            "Epoch  13 Iteration  9 Loss:  0.7155529260635376\n",
            "Epoch  13 Iteration  10 Loss:  0.5499173998832703\n",
            "Epoch  13 Iteration  11 Loss:  0.4307923913002014\n",
            "Epoch  13 Iteration  12 Loss:  0.8528896570205688\n",
            "Epoch  13 Iteration  13 Loss:  0.7695938348770142\n",
            "Epoch  13 Iteration  14 Loss:  0.696138322353363\n",
            "Epoch  13 Iteration  15 Loss:  0.42851531505584717\n",
            "Epoch  13 Iteration  16 Loss:  0.35511624813079834\n",
            "Epoch  13 Iteration  17 Loss:  0.5950867533683777\n",
            "Epoch  13 Iteration  18 Loss:  0.5051039457321167\n",
            "Epoch  13 Iteration  19 Loss:  0.7438303232192993\n",
            "Epoch  13 Iteration  20 Loss:  0.5766316056251526\n",
            "Epoch  13 Iteration  21 Loss:  0.5675756931304932\n",
            "Epoch  13 Iteration  22 Loss:  0.43240857124328613\n",
            "Epoch  13 Iteration  23 Loss:  0.4596349596977234\n",
            "Epoch  13 Iteration  24 Loss:  0.30849602818489075\n",
            "Epoch  13 Iteration  25 Loss:  0.25966712832450867\n",
            "Epoch  13 Iteration  26 Loss:  0.40130501985549927\n",
            "Epoch  13 Iteration  27 Loss:  0.6010090112686157\n",
            "Epoch  13 Iteration  28 Loss:  0.5662854909896851\n",
            "Epoch  13 Iteration  29 Loss:  0.1799529492855072\n",
            "Epoch  13 Iteration  30 Loss:  0.42611774802207947\n",
            "Epoch  13 Iteration  31 Loss:  0.34054604172706604\n",
            "Epoch  13 Iteration  32 Loss:  0.2960926294326782\n",
            "Epoch  13 Iteration  33 Loss:  0.46768397092819214\n",
            "Epoch  13 Iteration  34 Loss:  0.36157745122909546\n",
            "Epoch  13 Iteration  35 Loss:  0.4907064139842987\n",
            "Epoch  13 Iteration  36 Loss:  0.4749726951122284\n",
            "Epoch  13 Iteration  37 Loss:  0.5825442671775818\n",
            "Epoch  13 Iteration  38 Loss:  0.5348253846168518\n",
            "Epoch  13 Iteration  39 Loss:  0.5068365931510925\n",
            "Epoch  13 Iteration  40 Loss:  0.24111446738243103\n",
            "Epoch  13 Iteration  41 Loss:  0.5549982190132141\n",
            "Epoch  13 Iteration  42 Loss:  0.3617914021015167\n",
            "Epoch  13 Iteration  43 Loss:  0.5526494383811951\n",
            "Epoch  13 Iteration  44 Loss:  0.4886191487312317\n",
            "Epoch  13 Iteration  45 Loss:  0.2813463807106018\n",
            "Epoch  13 Iteration  46 Loss:  0.3933362662792206\n",
            "Epoch  13 Iteration  47 Loss:  0.4916991889476776\n",
            "Epoch  13 Iteration  48 Loss:  0.4595385491847992\n",
            "Epoch  13 Iteration  49 Loss:  0.4655160903930664\n",
            "Epoch  13 Iteration  50 Loss:  0.25087058544158936\n",
            "Epoch  13 Iteration  51 Loss:  0.2925000488758087\n",
            "Epoch  13 Iteration  52 Loss:  0.5557419061660767\n",
            "Epoch  13 Iteration  53 Loss:  0.4417371153831482\n",
            "Epoch  13 Iteration  54 Loss:  0.6846719980239868\n",
            "Epoch  13 Iteration  55 Loss:  0.5059803128242493\n",
            "Epoch  13 Iteration  56 Loss:  0.6593047976493835\n",
            "Epoch  13 Iteration  57 Loss:  0.44760870933532715\n",
            "Epoch  13 Iteration  58 Loss:  0.47231826186180115\n",
            "Epoch  13 Iteration  59 Loss:  0.5970745086669922\n",
            "Epoch  13 Iteration  60 Loss:  0.4615183174610138\n",
            "Epoch  13 Iteration  61 Loss:  0.2831988036632538\n",
            "Epoch  13 Iteration  62 Loss:  0.5375885367393494\n",
            "Epoch  13 Iteration  63 Loss:  0.38171350955963135\n",
            "Epoch  13 Iteration  64 Loss:  0.25988635420799255\n",
            "Epoch  13 Iteration  65 Loss:  0.5297691226005554\n",
            "*********************************************************************\n",
            "EPOCH  13  ==> Loss:  0.4742088769421433  Time taken ==>  228.13396096229553\n",
            "*********************************************************************\n",
            "Epoch  14 Iteration  0 Loss:  0.2644233703613281\n",
            "Epoch  14 Iteration  1 Loss:  0.3698838949203491\n",
            "Epoch  14 Iteration  2 Loss:  0.3352380394935608\n",
            "Epoch  14 Iteration  3 Loss:  0.6484317779541016\n",
            "Epoch  14 Iteration  4 Loss:  0.49703747034072876\n",
            "Epoch  14 Iteration  5 Loss:  0.46567773818969727\n",
            "Epoch  14 Iteration  6 Loss:  0.21096663177013397\n",
            "Epoch  14 Iteration  7 Loss:  0.2832736074924469\n",
            "Epoch  14 Iteration  8 Loss:  0.4129592180252075\n",
            "Epoch  14 Iteration  9 Loss:  0.4874841868877411\n",
            "Epoch  14 Iteration  10 Loss:  0.36975017189979553\n",
            "Epoch  14 Iteration  11 Loss:  0.3083120584487915\n",
            "Epoch  14 Iteration  12 Loss:  0.37711238861083984\n",
            "Epoch  14 Iteration  13 Loss:  0.40491679310798645\n",
            "Epoch  14 Iteration  14 Loss:  0.6603088974952698\n",
            "Epoch  14 Iteration  15 Loss:  0.3565521538257599\n",
            "Epoch  14 Iteration  16 Loss:  0.4205433130264282\n",
            "Epoch  14 Iteration  17 Loss:  0.5077853202819824\n",
            "Epoch  14 Iteration  18 Loss:  0.6433406472206116\n",
            "Epoch  14 Iteration  19 Loss:  0.4145642817020416\n",
            "Epoch  14 Iteration  20 Loss:  0.46205100417137146\n",
            "Epoch  14 Iteration  21 Loss:  0.4934963583946228\n",
            "Epoch  14 Iteration  22 Loss:  0.5952200293540955\n",
            "Epoch  14 Iteration  23 Loss:  0.4689224362373352\n",
            "Epoch  14 Iteration  24 Loss:  0.2702651917934418\n",
            "Epoch  14 Iteration  25 Loss:  0.6249569654464722\n",
            "Epoch  14 Iteration  26 Loss:  0.5263019800186157\n",
            "Epoch  14 Iteration  27 Loss:  0.33135873079299927\n",
            "Epoch  14 Iteration  28 Loss:  0.34679585695266724\n",
            "Epoch  14 Iteration  29 Loss:  0.5279936790466309\n",
            "Epoch  14 Iteration  30 Loss:  0.2820507287979126\n",
            "Epoch  14 Iteration  31 Loss:  0.6123273372650146\n",
            "Epoch  14 Iteration  32 Loss:  0.24960997700691223\n",
            "Epoch  14 Iteration  33 Loss:  0.9452626705169678\n",
            "Epoch  14 Iteration  34 Loss:  0.5747873187065125\n",
            "Epoch  14 Iteration  35 Loss:  0.42215922474861145\n",
            "Epoch  14 Iteration  36 Loss:  0.48400646448135376\n",
            "Epoch  14 Iteration  37 Loss:  0.2836400270462036\n",
            "Epoch  14 Iteration  38 Loss:  0.5191432237625122\n",
            "Epoch  14 Iteration  39 Loss:  0.36611664295196533\n",
            "Epoch  14 Iteration  40 Loss:  0.2746991813182831\n",
            "Epoch  14 Iteration  41 Loss:  0.7036447525024414\n",
            "Epoch  14 Iteration  42 Loss:  0.8420194387435913\n",
            "Epoch  14 Iteration  43 Loss:  0.5902084112167358\n",
            "Epoch  14 Iteration  44 Loss:  0.43539586663246155\n",
            "Epoch  14 Iteration  45 Loss:  0.469760000705719\n",
            "Epoch  14 Iteration  46 Loss:  0.3729236125946045\n",
            "Epoch  14 Iteration  47 Loss:  0.39448082447052\n",
            "Epoch  14 Iteration  48 Loss:  0.34800970554351807\n",
            "Epoch  14 Iteration  49 Loss:  0.4424004852771759\n",
            "Epoch  14 Iteration  50 Loss:  0.3171702027320862\n",
            "Epoch  14 Iteration  51 Loss:  0.6644763946533203\n",
            "Epoch  14 Iteration  52 Loss:  0.40544429421424866\n",
            "Epoch  14 Iteration  53 Loss:  0.31989508867263794\n",
            "Epoch  14 Iteration  54 Loss:  0.5454536080360413\n",
            "Epoch  14 Iteration  55 Loss:  0.2903189957141876\n",
            "Epoch  14 Iteration  56 Loss:  0.6660915017127991\n",
            "Epoch  14 Iteration  57 Loss:  0.35305869579315186\n",
            "Epoch  14 Iteration  58 Loss:  0.3622986078262329\n",
            "Epoch  14 Iteration  59 Loss:  0.2630045711994171\n",
            "Epoch  14 Iteration  60 Loss:  0.39569801092147827\n",
            "Epoch  14 Iteration  61 Loss:  0.6536903381347656\n",
            "Epoch  14 Iteration  62 Loss:  0.5794798135757446\n",
            "Epoch  14 Iteration  63 Loss:  0.559728741645813\n",
            "Epoch  14 Iteration  64 Loss:  0.2587697207927704\n",
            "Epoch  14 Iteration  65 Loss:  0.34857115149497986\n",
            "*********************************************************************\n",
            "EPOCH  14  ==> Loss:  0.4497230276465416  Time taken ==>  226.6240632534027\n",
            "*********************************************************************\n",
            "Epoch  15 Iteration  0 Loss:  0.5345451831817627\n",
            "Epoch  15 Iteration  1 Loss:  0.4482191205024719\n",
            "Epoch  15 Iteration  2 Loss:  0.38303089141845703\n",
            "Epoch  15 Iteration  3 Loss:  0.4311840534210205\n",
            "Epoch  15 Iteration  4 Loss:  0.5590366721153259\n",
            "Epoch  15 Iteration  5 Loss:  0.3340287506580353\n",
            "Epoch  15 Iteration  6 Loss:  0.4893617331981659\n",
            "Epoch  15 Iteration  7 Loss:  0.3347761631011963\n",
            "Epoch  15 Iteration  8 Loss:  0.3861348032951355\n",
            "Epoch  15 Iteration  9 Loss:  0.4715002477169037\n",
            "Epoch  15 Iteration  10 Loss:  0.325437068939209\n",
            "Epoch  15 Iteration  11 Loss:  0.3909723162651062\n",
            "Epoch  15 Iteration  12 Loss:  0.37244030833244324\n",
            "Epoch  15 Iteration  13 Loss:  0.37548282742500305\n",
            "Epoch  15 Iteration  14 Loss:  0.3227826952934265\n",
            "Epoch  15 Iteration  15 Loss:  0.5665393471717834\n",
            "Epoch  15 Iteration  16 Loss:  0.44022226333618164\n",
            "Epoch  15 Iteration  17 Loss:  0.3974829316139221\n",
            "Epoch  15 Iteration  18 Loss:  0.46478456258773804\n",
            "Epoch  15 Iteration  19 Loss:  0.6707199811935425\n",
            "Epoch  15 Iteration  20 Loss:  0.33386215567588806\n",
            "Epoch  15 Iteration  21 Loss:  0.4908157289028168\n",
            "Epoch  15 Iteration  22 Loss:  0.28855782747268677\n",
            "Epoch  15 Iteration  23 Loss:  0.6472744345664978\n",
            "Epoch  15 Iteration  24 Loss:  0.40688157081604004\n",
            "Epoch  15 Iteration  25 Loss:  0.4841339588165283\n",
            "Epoch  15 Iteration  26 Loss:  0.47739267349243164\n",
            "Epoch  15 Iteration  27 Loss:  0.26962265372276306\n",
            "Epoch  15 Iteration  28 Loss:  0.7091724276542664\n",
            "Epoch  15 Iteration  29 Loss:  0.4465935230255127\n",
            "Epoch  15 Iteration  30 Loss:  0.5618993043899536\n",
            "Epoch  15 Iteration  31 Loss:  0.6654460430145264\n",
            "Epoch  15 Iteration  32 Loss:  0.24811892211437225\n",
            "Epoch  15 Iteration  33 Loss:  0.45200419425964355\n",
            "Epoch  15 Iteration  34 Loss:  0.5087295770645142\n",
            "Epoch  15 Iteration  35 Loss:  0.9184813499450684\n",
            "Epoch  15 Iteration  36 Loss:  0.5683821439743042\n",
            "Epoch  15 Iteration  37 Loss:  0.3639411926269531\n",
            "Epoch  15 Iteration  38 Loss:  0.41173049807548523\n",
            "Epoch  15 Iteration  39 Loss:  0.3206021189689636\n",
            "Epoch  15 Iteration  40 Loss:  0.43862414360046387\n",
            "Epoch  15 Iteration  41 Loss:  0.41562068462371826\n",
            "Epoch  15 Iteration  42 Loss:  0.6883335113525391\n",
            "Epoch  15 Iteration  43 Loss:  0.2210596799850464\n",
            "Epoch  15 Iteration  44 Loss:  0.33540278673171997\n",
            "Epoch  15 Iteration  45 Loss:  0.30554986000061035\n",
            "Epoch  15 Iteration  46 Loss:  0.3598232567310333\n",
            "Epoch  15 Iteration  47 Loss:  0.5289708375930786\n",
            "Epoch  15 Iteration  48 Loss:  0.4109858572483063\n",
            "Epoch  15 Iteration  49 Loss:  0.3556697964668274\n",
            "Epoch  15 Iteration  50 Loss:  0.2966025471687317\n",
            "Epoch  15 Iteration  51 Loss:  0.2907217741012573\n",
            "Epoch  15 Iteration  52 Loss:  0.41914278268814087\n",
            "Epoch  15 Iteration  53 Loss:  0.4108233153820038\n",
            "Epoch  15 Iteration  54 Loss:  0.3030945956707001\n",
            "Epoch  15 Iteration  55 Loss:  0.34214818477630615\n",
            "Epoch  15 Iteration  56 Loss:  0.3445349335670471\n",
            "Epoch  15 Iteration  57 Loss:  0.38685131072998047\n",
            "Epoch  15 Iteration  58 Loss:  0.475009560585022\n",
            "Epoch  15 Iteration  59 Loss:  0.40931910276412964\n",
            "Epoch  15 Iteration  60 Loss:  0.22788380086421967\n",
            "Epoch  15 Iteration  61 Loss:  0.5712085962295532\n",
            "Epoch  15 Iteration  62 Loss:  0.4585772156715393\n",
            "Epoch  15 Iteration  63 Loss:  0.3730347156524658\n",
            "Epoch  15 Iteration  64 Loss:  0.3379402756690979\n",
            "Epoch  15 Iteration  65 Loss:  0.41150963306427\n",
            "*********************************************************************\n",
            "EPOCH  15  ==> Loss:  0.43016313603430084  Time taken ==>  225.08771443367004\n",
            "*********************************************************************\n",
            "Epoch  16 Iteration  0 Loss:  0.6785872578620911\n",
            "Epoch  16 Iteration  1 Loss:  0.4106369614601135\n",
            "Epoch  16 Iteration  2 Loss:  0.3930845856666565\n",
            "Epoch  16 Iteration  3 Loss:  0.540066659450531\n",
            "Epoch  16 Iteration  4 Loss:  0.3634040951728821\n",
            "Epoch  16 Iteration  5 Loss:  0.30387353897094727\n",
            "Epoch  16 Iteration  6 Loss:  0.4927481412887573\n",
            "Epoch  16 Iteration  7 Loss:  0.4386114478111267\n",
            "Epoch  16 Iteration  8 Loss:  0.4704766869544983\n",
            "Epoch  16 Iteration  9 Loss:  0.5475676655769348\n",
            "Epoch  16 Iteration  10 Loss:  0.3432256877422333\n",
            "Epoch  16 Iteration  11 Loss:  0.46587711572647095\n",
            "Epoch  16 Iteration  12 Loss:  0.3082887530326843\n",
            "Epoch  16 Iteration  13 Loss:  0.44908052682876587\n",
            "Epoch  16 Iteration  14 Loss:  0.30518537759780884\n",
            "Epoch  16 Iteration  15 Loss:  0.5499601364135742\n",
            "Epoch  16 Iteration  16 Loss:  0.5474097728729248\n",
            "Epoch  16 Iteration  17 Loss:  0.5524560213088989\n",
            "Epoch  16 Iteration  18 Loss:  0.5268088579177856\n",
            "Epoch  16 Iteration  19 Loss:  0.36242562532424927\n",
            "Epoch  16 Iteration  20 Loss:  0.7155979871749878\n",
            "Epoch  16 Iteration  21 Loss:  0.4892544746398926\n",
            "Epoch  16 Iteration  22 Loss:  0.43706005811691284\n",
            "Epoch  16 Iteration  23 Loss:  0.554381251335144\n",
            "Epoch  16 Iteration  24 Loss:  0.3831908702850342\n",
            "Epoch  16 Iteration  25 Loss:  0.45744621753692627\n",
            "Epoch  16 Iteration  26 Loss:  0.45743483304977417\n",
            "Epoch  16 Iteration  27 Loss:  0.3542148470878601\n",
            "Epoch  16 Iteration  28 Loss:  0.4840273857116699\n",
            "Epoch  16 Iteration  29 Loss:  0.4421261250972748\n",
            "Epoch  16 Iteration  30 Loss:  0.5521422624588013\n",
            "Epoch  16 Iteration  31 Loss:  0.4162519574165344\n",
            "Epoch  16 Iteration  32 Loss:  0.3317195773124695\n",
            "Epoch  16 Iteration  33 Loss:  0.28271716833114624\n",
            "Epoch  16 Iteration  34 Loss:  0.3680098354816437\n",
            "Epoch  16 Iteration  35 Loss:  0.5425907373428345\n",
            "Epoch  16 Iteration  36 Loss:  0.321520060300827\n",
            "Epoch  16 Iteration  37 Loss:  0.6014507412910461\n",
            "Epoch  16 Iteration  38 Loss:  0.19029945135116577\n",
            "Epoch  16 Iteration  39 Loss:  0.31400248408317566\n",
            "Epoch  16 Iteration  40 Loss:  0.3407769203186035\n",
            "Epoch  16 Iteration  41 Loss:  0.5097416639328003\n",
            "Epoch  16 Iteration  42 Loss:  0.5244367718696594\n",
            "Epoch  16 Iteration  43 Loss:  0.2887171506881714\n",
            "Epoch  16 Iteration  44 Loss:  0.4865036606788635\n",
            "Epoch  16 Iteration  45 Loss:  0.4385022819042206\n",
            "Epoch  16 Iteration  46 Loss:  0.4390817880630493\n",
            "Epoch  16 Iteration  47 Loss:  0.3531985878944397\n",
            "Epoch  16 Iteration  48 Loss:  0.29248273372650146\n",
            "Epoch  16 Iteration  49 Loss:  0.466097354888916\n",
            "Epoch  16 Iteration  50 Loss:  0.6170352101325989\n",
            "Epoch  16 Iteration  51 Loss:  0.4028225541114807\n",
            "Epoch  16 Iteration  52 Loss:  0.47711461782455444\n",
            "Epoch  16 Iteration  53 Loss:  0.4880344569683075\n",
            "Epoch  16 Iteration  54 Loss:  0.40969473123550415\n",
            "Epoch  16 Iteration  55 Loss:  0.5970317721366882\n",
            "Epoch  16 Iteration  56 Loss:  0.33761000633239746\n",
            "Epoch  16 Iteration  57 Loss:  0.39524126052856445\n",
            "Epoch  16 Iteration  58 Loss:  0.5062569379806519\n",
            "Epoch  16 Iteration  59 Loss:  0.36721640825271606\n",
            "Epoch  16 Iteration  60 Loss:  0.18966084718704224\n",
            "Epoch  16 Iteration  61 Loss:  0.5519025921821594\n",
            "Epoch  16 Iteration  62 Loss:  0.22399960458278656\n",
            "Epoch  16 Iteration  63 Loss:  0.18390941619873047\n",
            "Epoch  16 Iteration  64 Loss:  0.6628144383430481\n",
            "Epoch  16 Iteration  65 Loss:  0.558591365814209\n",
            "*********************************************************************\n",
            "EPOCH  16  ==> Loss:  0.43717709657820786  Time taken ==>  224.97095394134521\n",
            "*********************************************************************\n",
            "Epoch  17 Iteration  0 Loss:  0.4174831509590149\n",
            "Epoch  17 Iteration  1 Loss:  0.3995211720466614\n",
            "Epoch  17 Iteration  2 Loss:  0.45827025175094604\n",
            "Epoch  17 Iteration  3 Loss:  0.562248945236206\n",
            "Epoch  17 Iteration  4 Loss:  0.6330026388168335\n",
            "Epoch  17 Iteration  5 Loss:  0.3961697816848755\n",
            "Epoch  17 Iteration  6 Loss:  0.3962762653827667\n",
            "Epoch  17 Iteration  7 Loss:  0.4426151216030121\n",
            "Epoch  17 Iteration  8 Loss:  0.5421658754348755\n",
            "Epoch  17 Iteration  9 Loss:  0.4090340733528137\n",
            "Epoch  17 Iteration  10 Loss:  0.4959195852279663\n",
            "Epoch  17 Iteration  11 Loss:  0.30457907915115356\n",
            "Epoch  17 Iteration  12 Loss:  0.4079954922199249\n",
            "Epoch  17 Iteration  13 Loss:  0.3519330620765686\n",
            "Epoch  17 Iteration  14 Loss:  0.46159636974334717\n",
            "Epoch  17 Iteration  15 Loss:  0.2643967568874359\n",
            "Epoch  17 Iteration  16 Loss:  0.49392151832580566\n",
            "Epoch  17 Iteration  17 Loss:  1.0004457235336304\n",
            "Epoch  17 Iteration  18 Loss:  0.2532816529273987\n",
            "Epoch  17 Iteration  19 Loss:  0.39872896671295166\n",
            "Epoch  17 Iteration  20 Loss:  0.48910120129585266\n",
            "Epoch  17 Iteration  21 Loss:  0.48676109313964844\n",
            "Epoch  17 Iteration  22 Loss:  0.3252331018447876\n",
            "Epoch  17 Iteration  23 Loss:  0.3546014428138733\n",
            "Epoch  17 Iteration  24 Loss:  0.3278689980506897\n",
            "Epoch  17 Iteration  25 Loss:  0.3330625295639038\n",
            "Epoch  17 Iteration  26 Loss:  0.3765386939048767\n",
            "Epoch  17 Iteration  27 Loss:  0.3197416663169861\n",
            "Epoch  17 Iteration  28 Loss:  0.4178166389465332\n",
            "Epoch  17 Iteration  29 Loss:  0.5970174670219421\n",
            "Epoch  17 Iteration  30 Loss:  0.40956568717956543\n",
            "Epoch  17 Iteration  31 Loss:  0.29578182101249695\n",
            "Epoch  17 Iteration  32 Loss:  0.3938060998916626\n",
            "Epoch  17 Iteration  33 Loss:  0.36282503604888916\n",
            "Epoch  17 Iteration  34 Loss:  0.47813984751701355\n",
            "Epoch  17 Iteration  35 Loss:  0.5030779242515564\n",
            "Epoch  17 Iteration  36 Loss:  0.4864181876182556\n",
            "Epoch  17 Iteration  37 Loss:  0.4289206266403198\n",
            "Epoch  17 Iteration  38 Loss:  0.5198755264282227\n",
            "Epoch  17 Iteration  39 Loss:  0.2666902542114258\n",
            "Epoch  17 Iteration  40 Loss:  0.276340126991272\n",
            "Epoch  17 Iteration  41 Loss:  0.3492416441440582\n",
            "Epoch  17 Iteration  42 Loss:  0.2760946750640869\n",
            "Epoch  17 Iteration  43 Loss:  0.4344175457954407\n",
            "Epoch  17 Iteration  44 Loss:  0.2398480474948883\n",
            "Epoch  17 Iteration  45 Loss:  0.3558826744556427\n",
            "Epoch  17 Iteration  46 Loss:  0.3585823178291321\n",
            "Epoch  17 Iteration  47 Loss:  0.2114010751247406\n",
            "Epoch  17 Iteration  48 Loss:  0.18256618082523346\n",
            "Epoch  17 Iteration  49 Loss:  0.4125538766384125\n",
            "Epoch  17 Iteration  50 Loss:  0.42509186267852783\n",
            "Epoch  17 Iteration  51 Loss:  0.35127317905426025\n",
            "Epoch  17 Iteration  52 Loss:  0.3050054609775543\n",
            "Epoch  17 Iteration  53 Loss:  0.46756690740585327\n",
            "Epoch  17 Iteration  54 Loss:  0.4413713812828064\n",
            "Epoch  17 Iteration  55 Loss:  0.5006673336029053\n",
            "Epoch  17 Iteration  56 Loss:  0.4838268756866455\n",
            "Epoch  17 Iteration  57 Loss:  0.5054587125778198\n",
            "Epoch  17 Iteration  58 Loss:  0.49228912591934204\n",
            "Epoch  17 Iteration  59 Loss:  0.32217639684677124\n",
            "Epoch  17 Iteration  60 Loss:  0.4729776084423065\n",
            "Epoch  17 Iteration  61 Loss:  0.23315948247909546\n",
            "Epoch  17 Iteration  62 Loss:  0.5426080226898193\n",
            "Epoch  17 Iteration  63 Loss:  0.3245568871498108\n",
            "Epoch  17 Iteration  64 Loss:  0.5519315004348755\n",
            "Epoch  17 Iteration  65 Loss:  0.6592731475830078\n",
            "*********************************************************************\n",
            "EPOCH  17  ==> Loss:  0.4157362329688939  Time taken ==>  224.71886467933655\n",
            "*********************************************************************\n",
            "Epoch  18 Iteration  0 Loss:  0.46075689792633057\n",
            "Epoch  18 Iteration  1 Loss:  0.35129737854003906\n",
            "Epoch  18 Iteration  2 Loss:  0.5871292352676392\n",
            "Epoch  18 Iteration  3 Loss:  0.2211422622203827\n",
            "Epoch  18 Iteration  4 Loss:  0.47778254747390747\n",
            "Epoch  18 Iteration  5 Loss:  0.5051867365837097\n",
            "Epoch  18 Iteration  6 Loss:  0.5491690635681152\n",
            "Epoch  18 Iteration  7 Loss:  0.3384985327720642\n",
            "Epoch  18 Iteration  8 Loss:  0.471498966217041\n",
            "Epoch  18 Iteration  9 Loss:  0.4083467721939087\n",
            "Epoch  18 Iteration  10 Loss:  0.2439676970243454\n",
            "Epoch  18 Iteration  11 Loss:  0.4682556390762329\n",
            "Epoch  18 Iteration  12 Loss:  0.40212783217430115\n",
            "Epoch  18 Iteration  13 Loss:  0.2703515887260437\n",
            "Epoch  18 Iteration  14 Loss:  0.26178890466690063\n",
            "Epoch  18 Iteration  15 Loss:  0.2650703191757202\n",
            "Epoch  18 Iteration  16 Loss:  0.2841811776161194\n",
            "Epoch  18 Iteration  17 Loss:  0.33165812492370605\n",
            "Epoch  18 Iteration  18 Loss:  0.7532954216003418\n",
            "Epoch  18 Iteration  19 Loss:  0.4035085439682007\n",
            "Epoch  18 Iteration  20 Loss:  0.4444064795970917\n",
            "Epoch  18 Iteration  21 Loss:  0.43035373091697693\n",
            "Epoch  18 Iteration  22 Loss:  0.40203237533569336\n",
            "Epoch  18 Iteration  23 Loss:  0.369444340467453\n",
            "Epoch  18 Iteration  24 Loss:  0.27855515480041504\n",
            "Epoch  18 Iteration  25 Loss:  0.16609938442707062\n",
            "Epoch  18 Iteration  26 Loss:  0.3026135265827179\n",
            "Epoch  18 Iteration  27 Loss:  0.445481538772583\n",
            "Epoch  18 Iteration  28 Loss:  0.28699809312820435\n",
            "Epoch  18 Iteration  29 Loss:  0.4105775058269501\n",
            "Epoch  18 Iteration  30 Loss:  0.40207332372665405\n",
            "Epoch  18 Iteration  31 Loss:  0.4408639669418335\n",
            "Epoch  18 Iteration  32 Loss:  0.27081209421157837\n",
            "Epoch  18 Iteration  33 Loss:  0.30550307035446167\n",
            "Epoch  18 Iteration  34 Loss:  0.35728663206100464\n",
            "Epoch  18 Iteration  35 Loss:  0.40659502148628235\n",
            "Epoch  18 Iteration  36 Loss:  0.2519378960132599\n",
            "Epoch  18 Iteration  37 Loss:  0.4945400357246399\n",
            "Epoch  18 Iteration  38 Loss:  0.42462095618247986\n",
            "Epoch  18 Iteration  39 Loss:  0.39884233474731445\n",
            "Epoch  18 Iteration  40 Loss:  0.5192691087722778\n",
            "Epoch  18 Iteration  41 Loss:  0.4078603684902191\n",
            "Epoch  18 Iteration  42 Loss:  0.2057776153087616\n",
            "Epoch  18 Iteration  43 Loss:  0.5863006114959717\n",
            "Epoch  18 Iteration  44 Loss:  0.4448627531528473\n",
            "Epoch  18 Iteration  45 Loss:  0.516596794128418\n",
            "Epoch  18 Iteration  46 Loss:  0.17236611247062683\n",
            "Epoch  18 Iteration  47 Loss:  0.307762086391449\n",
            "Epoch  18 Iteration  48 Loss:  0.5911848545074463\n",
            "Epoch  18 Iteration  49 Loss:  0.6180124878883362\n",
            "Epoch  18 Iteration  50 Loss:  0.6787155866622925\n",
            "Epoch  18 Iteration  51 Loss:  0.3222997188568115\n",
            "Epoch  18 Iteration  52 Loss:  0.3220725655555725\n",
            "Epoch  18 Iteration  53 Loss:  0.5211179852485657\n",
            "Epoch  18 Iteration  54 Loss:  0.4899440109729767\n",
            "Epoch  18 Iteration  55 Loss:  0.6961954236030579\n",
            "Epoch  18 Iteration  56 Loss:  0.5295356512069702\n",
            "Epoch  18 Iteration  57 Loss:  0.4719315469264984\n",
            "Epoch  18 Iteration  58 Loss:  0.41772106289863586\n",
            "Epoch  18 Iteration  59 Loss:  0.5062994956970215\n",
            "Epoch  18 Iteration  60 Loss:  0.5248111486434937\n",
            "Epoch  18 Iteration  61 Loss:  0.29837632179260254\n",
            "Epoch  18 Iteration  62 Loss:  0.2538415193557739\n",
            "Epoch  18 Iteration  63 Loss:  0.3661046326160431\n",
            "Epoch  18 Iteration  64 Loss:  0.4325517416000366\n",
            "Epoch  18 Iteration  65 Loss:  0.5195257663726807\n",
            "*********************************************************************\n",
            "EPOCH  18  ==> Loss:  0.4100861829338652  Time taken ==>  225.63813424110413\n",
            "*********************************************************************\n",
            "Epoch  19 Iteration  0 Loss:  0.3381863832473755\n",
            "Epoch  19 Iteration  1 Loss:  0.20400001108646393\n",
            "Epoch  19 Iteration  2 Loss:  0.5236225724220276\n",
            "Epoch  19 Iteration  3 Loss:  0.32718604803085327\n",
            "Epoch  19 Iteration  4 Loss:  0.3723391890525818\n",
            "Epoch  19 Iteration  5 Loss:  0.2338947355747223\n",
            "Epoch  19 Iteration  6 Loss:  0.4313219487667084\n",
            "Epoch  19 Iteration  7 Loss:  0.3727931082248688\n",
            "Epoch  19 Iteration  8 Loss:  0.37588685750961304\n",
            "Epoch  19 Iteration  9 Loss:  0.3585817217826843\n",
            "Epoch  19 Iteration  10 Loss:  0.43279027938842773\n",
            "Epoch  19 Iteration  11 Loss:  0.3538649082183838\n",
            "Epoch  19 Iteration  12 Loss:  0.6036155223846436\n",
            "Epoch  19 Iteration  13 Loss:  0.44621366262435913\n",
            "Epoch  19 Iteration  14 Loss:  0.3998417556285858\n",
            "Epoch  19 Iteration  15 Loss:  0.2533618211746216\n",
            "Epoch  19 Iteration  16 Loss:  0.44856321811676025\n",
            "Epoch  19 Iteration  17 Loss:  0.512390673160553\n",
            "Epoch  19 Iteration  18 Loss:  0.4282262921333313\n",
            "Epoch  19 Iteration  19 Loss:  0.3610045611858368\n",
            "Epoch  19 Iteration  20 Loss:  0.42969363927841187\n",
            "Epoch  19 Iteration  21 Loss:  0.17335224151611328\n",
            "Epoch  19 Iteration  22 Loss:  0.31584787368774414\n",
            "Epoch  19 Iteration  23 Loss:  0.3225906491279602\n",
            "Epoch  19 Iteration  24 Loss:  0.46824750304222107\n",
            "Epoch  19 Iteration  25 Loss:  0.3207014501094818\n",
            "Epoch  19 Iteration  26 Loss:  0.18210864067077637\n",
            "Epoch  19 Iteration  27 Loss:  0.4787312150001526\n",
            "Epoch  19 Iteration  28 Loss:  0.429654061794281\n",
            "Epoch  19 Iteration  29 Loss:  0.34785956144332886\n",
            "Epoch  19 Iteration  30 Loss:  0.552981436252594\n",
            "Epoch  19 Iteration  31 Loss:  0.4269266426563263\n",
            "Epoch  19 Iteration  32 Loss:  0.5649091005325317\n",
            "Epoch  19 Iteration  33 Loss:  0.5118308663368225\n",
            "Epoch  19 Iteration  34 Loss:  0.4951351284980774\n",
            "Epoch  19 Iteration  35 Loss:  0.4118582010269165\n",
            "Epoch  19 Iteration  36 Loss:  0.25605401396751404\n",
            "Epoch  19 Iteration  37 Loss:  0.4014524221420288\n",
            "Epoch  19 Iteration  38 Loss:  0.15790849924087524\n",
            "Epoch  19 Iteration  39 Loss:  0.3924010694026947\n",
            "Epoch  19 Iteration  40 Loss:  0.5401496291160583\n",
            "Epoch  19 Iteration  41 Loss:  0.31087708473205566\n",
            "Epoch  19 Iteration  42 Loss:  0.38678887486457825\n",
            "Epoch  19 Iteration  43 Loss:  0.37383654713630676\n",
            "Epoch  19 Iteration  44 Loss:  0.5231138467788696\n",
            "Epoch  19 Iteration  45 Loss:  0.459360271692276\n",
            "Epoch  19 Iteration  46 Loss:  0.3979080319404602\n",
            "Epoch  19 Iteration  47 Loss:  0.8001368641853333\n",
            "Epoch  19 Iteration  48 Loss:  0.781782865524292\n",
            "Epoch  19 Iteration  49 Loss:  0.6773905158042908\n",
            "Epoch  19 Iteration  50 Loss:  0.17348474264144897\n",
            "Epoch  19 Iteration  51 Loss:  0.2567714750766754\n",
            "Epoch  19 Iteration  52 Loss:  0.25564044713974\n",
            "Epoch  19 Iteration  53 Loss:  0.25315141677856445\n",
            "Epoch  19 Iteration  54 Loss:  0.21507562696933746\n",
            "Epoch  19 Iteration  55 Loss:  0.24598555266857147\n",
            "Epoch  19 Iteration  56 Loss:  0.7554582953453064\n",
            "Epoch  19 Iteration  57 Loss:  0.4608815908432007\n",
            "Epoch  19 Iteration  58 Loss:  0.19022125005722046\n",
            "Epoch  19 Iteration  59 Loss:  0.5999964475631714\n",
            "Epoch  19 Iteration  60 Loss:  0.24581339955329895\n",
            "Epoch  19 Iteration  61 Loss:  0.5103281140327454\n",
            "Epoch  19 Iteration  62 Loss:  0.3537060022354126\n",
            "Epoch  19 Iteration  63 Loss:  0.3937179446220398\n",
            "Epoch  19 Iteration  64 Loss:  0.5037236213684082\n",
            "Epoch  19 Iteration  65 Loss:  0.6692075133323669\n",
            "*********************************************************************\n",
            "EPOCH  19  ==> Loss:  0.4053399614763982  Time taken ==>  224.36993741989136\n",
            "*********************************************************************\n",
            "Epoch  20 Iteration  0 Loss:  0.36445605754852295\n",
            "Epoch  20 Iteration  1 Loss:  0.3552209436893463\n",
            "Epoch  20 Iteration  2 Loss:  0.3865257501602173\n",
            "Epoch  20 Iteration  3 Loss:  0.34726276993751526\n",
            "Epoch  20 Iteration  4 Loss:  0.36046138405799866\n",
            "Epoch  20 Iteration  5 Loss:  0.4498170018196106\n",
            "Epoch  20 Iteration  6 Loss:  0.6435303688049316\n",
            "Epoch  20 Iteration  7 Loss:  0.42057523131370544\n",
            "Epoch  20 Iteration  8 Loss:  0.674597978591919\n",
            "Epoch  20 Iteration  9 Loss:  0.3565765619277954\n",
            "Epoch  20 Iteration  10 Loss:  0.30816566944122314\n",
            "Epoch  20 Iteration  11 Loss:  0.2544569969177246\n",
            "Epoch  20 Iteration  12 Loss:  0.3220027685165405\n",
            "Epoch  20 Iteration  13 Loss:  0.40874746441841125\n",
            "Epoch  20 Iteration  14 Loss:  0.5718845725059509\n",
            "Epoch  20 Iteration  15 Loss:  0.3959429860115051\n",
            "Epoch  20 Iteration  16 Loss:  0.3634990453720093\n",
            "Epoch  20 Iteration  17 Loss:  0.5991799831390381\n",
            "Epoch  20 Iteration  18 Loss:  0.3878611922264099\n",
            "Epoch  20 Iteration  19 Loss:  0.7445676326751709\n",
            "Epoch  20 Iteration  20 Loss:  0.3384096324443817\n",
            "Epoch  20 Iteration  21 Loss:  0.288407564163208\n",
            "Epoch  20 Iteration  22 Loss:  0.6866114735603333\n",
            "Epoch  20 Iteration  23 Loss:  0.3257482349872589\n",
            "Epoch  20 Iteration  24 Loss:  0.42910534143447876\n",
            "Epoch  20 Iteration  25 Loss:  0.3752710819244385\n",
            "Epoch  20 Iteration  26 Loss:  0.3114060163497925\n",
            "Epoch  20 Iteration  27 Loss:  0.18858779966831207\n",
            "Epoch  20 Iteration  28 Loss:  0.3962114453315735\n",
            "Epoch  20 Iteration  29 Loss:  0.2664859890937805\n",
            "Epoch  20 Iteration  30 Loss:  0.3746584355831146\n",
            "Epoch  20 Iteration  31 Loss:  0.5661871433258057\n",
            "Epoch  20 Iteration  32 Loss:  0.32470494508743286\n",
            "Epoch  20 Iteration  33 Loss:  0.3338095545768738\n",
            "Epoch  20 Iteration  34 Loss:  0.4660903215408325\n",
            "Epoch  20 Iteration  35 Loss:  0.22728347778320312\n",
            "Epoch  20 Iteration  36 Loss:  0.2275988757610321\n",
            "Epoch  20 Iteration  37 Loss:  0.608311653137207\n",
            "Epoch  20 Iteration  38 Loss:  0.24593399465084076\n",
            "Epoch  20 Iteration  39 Loss:  0.25518789887428284\n",
            "Epoch  20 Iteration  40 Loss:  0.6963614225387573\n",
            "Epoch  20 Iteration  41 Loss:  0.19643673300743103\n",
            "Epoch  20 Iteration  42 Loss:  0.3227557837963104\n",
            "Epoch  20 Iteration  43 Loss:  0.6916626691818237\n",
            "Epoch  20 Iteration  44 Loss:  0.3425903916358948\n",
            "Epoch  20 Iteration  45 Loss:  0.3157380521297455\n",
            "Epoch  20 Iteration  46 Loss:  0.6562944650650024\n",
            "Epoch  20 Iteration  47 Loss:  0.2760583460330963\n",
            "Epoch  20 Iteration  48 Loss:  0.2473468780517578\n",
            "Epoch  20 Iteration  49 Loss:  0.6241660118103027\n",
            "Epoch  20 Iteration  50 Loss:  0.38134145736694336\n",
            "Epoch  20 Iteration  51 Loss:  0.3655514419078827\n",
            "Epoch  20 Iteration  52 Loss:  0.3078467845916748\n",
            "Epoch  20 Iteration  53 Loss:  0.39862385392189026\n",
            "Epoch  20 Iteration  54 Loss:  0.36139699816703796\n",
            "Epoch  20 Iteration  55 Loss:  0.26385581493377686\n",
            "Epoch  20 Iteration  56 Loss:  0.47906386852264404\n",
            "Epoch  20 Iteration  57 Loss:  0.30968838930130005\n",
            "Epoch  20 Iteration  58 Loss:  0.3504274785518646\n",
            "Epoch  20 Iteration  59 Loss:  0.5000523328781128\n",
            "Epoch  20 Iteration  60 Loss:  0.6122869253158569\n",
            "Epoch  20 Iteration  61 Loss:  0.1543041616678238\n",
            "Epoch  20 Iteration  62 Loss:  0.562432050704956\n",
            "Epoch  20 Iteration  63 Loss:  0.28133100271224976\n",
            "Epoch  20 Iteration  64 Loss:  0.2787569761276245\n",
            "Epoch  20 Iteration  65 Loss:  0.5370237827301025\n",
            "*********************************************************************\n",
            "EPOCH  20  ==> Loss:  0.4009808683485696  Time taken ==>  225.18961262702942\n",
            "*********************************************************************\n",
            "Epoch  21 Iteration  0 Loss:  0.23380479216575623\n",
            "Epoch  21 Iteration  1 Loss:  0.4371885061264038\n",
            "Epoch  21 Iteration  2 Loss:  0.42261359095573425\n",
            "Epoch  21 Iteration  3 Loss:  0.22140714526176453\n",
            "Epoch  21 Iteration  4 Loss:  0.5088751316070557\n",
            "Epoch  21 Iteration  5 Loss:  0.40327584743499756\n",
            "Epoch  21 Iteration  6 Loss:  0.31160008907318115\n",
            "Epoch  21 Iteration  7 Loss:  0.32861530780792236\n",
            "Epoch  21 Iteration  8 Loss:  0.42710012197494507\n",
            "Epoch  21 Iteration  9 Loss:  0.21961182355880737\n",
            "Epoch  21 Iteration  10 Loss:  0.3243162930011749\n",
            "Epoch  21 Iteration  11 Loss:  0.10483105480670929\n",
            "Epoch  21 Iteration  12 Loss:  0.27114784717559814\n",
            "Epoch  21 Iteration  13 Loss:  0.2763409912586212\n",
            "Epoch  21 Iteration  14 Loss:  0.3096415400505066\n",
            "Epoch  21 Iteration  15 Loss:  0.5303047299385071\n",
            "Epoch  21 Iteration  16 Loss:  0.2614232897758484\n",
            "Epoch  21 Iteration  17 Loss:  0.575901985168457\n",
            "Epoch  21 Iteration  18 Loss:  0.28152716159820557\n",
            "Epoch  21 Iteration  19 Loss:  0.3893449604511261\n",
            "Epoch  21 Iteration  20 Loss:  0.26174497604370117\n",
            "Epoch  21 Iteration  21 Loss:  0.6201040744781494\n",
            "Epoch  21 Iteration  22 Loss:  0.4156703054904938\n",
            "Epoch  21 Iteration  23 Loss:  0.49458205699920654\n",
            "Epoch  21 Iteration  24 Loss:  0.3536224365234375\n",
            "Epoch  21 Iteration  25 Loss:  0.3674330711364746\n",
            "Epoch  21 Iteration  26 Loss:  0.5024808645248413\n",
            "Epoch  21 Iteration  27 Loss:  0.4983368217945099\n",
            "Epoch  21 Iteration  28 Loss:  0.7137353420257568\n",
            "Epoch  21 Iteration  29 Loss:  0.3714779019355774\n",
            "Epoch  21 Iteration  30 Loss:  0.3595695495605469\n",
            "Epoch  21 Iteration  31 Loss:  0.301275372505188\n",
            "Epoch  21 Iteration  32 Loss:  0.4364127218723297\n",
            "Epoch  21 Iteration  33 Loss:  0.25389525294303894\n",
            "Epoch  21 Iteration  34 Loss:  0.6529947519302368\n",
            "Epoch  21 Iteration  35 Loss:  0.3876219093799591\n",
            "Epoch  21 Iteration  36 Loss:  0.49489134550094604\n",
            "Epoch  21 Iteration  37 Loss:  0.2447093278169632\n",
            "Epoch  21 Iteration  38 Loss:  0.46172282099723816\n",
            "Epoch  21 Iteration  39 Loss:  0.33182796835899353\n",
            "Epoch  21 Iteration  40 Loss:  0.20462577044963837\n",
            "Epoch  21 Iteration  41 Loss:  0.40428444743156433\n",
            "Epoch  21 Iteration  42 Loss:  0.6254107356071472\n",
            "Epoch  21 Iteration  43 Loss:  0.27086037397384644\n",
            "Epoch  21 Iteration  44 Loss:  0.7781859636306763\n",
            "Epoch  21 Iteration  45 Loss:  0.4769755005836487\n",
            "Epoch  21 Iteration  46 Loss:  0.46963539719581604\n",
            "Epoch  21 Iteration  47 Loss:  0.3690776228904724\n",
            "Epoch  21 Iteration  48 Loss:  0.35597628355026245\n",
            "Epoch  21 Iteration  49 Loss:  0.28314408659935\n",
            "Epoch  21 Iteration  50 Loss:  0.27780041098594666\n",
            "Epoch  21 Iteration  51 Loss:  0.33774691820144653\n",
            "Epoch  21 Iteration  52 Loss:  0.445568323135376\n",
            "Epoch  21 Iteration  53 Loss:  0.23187768459320068\n",
            "Epoch  21 Iteration  54 Loss:  0.4248393774032593\n",
            "Epoch  21 Iteration  55 Loss:  0.4360249638557434\n",
            "Epoch  21 Iteration  56 Loss:  0.4524633288383484\n",
            "Epoch  21 Iteration  57 Loss:  0.7064223289489746\n",
            "Epoch  21 Iteration  58 Loss:  0.20715636014938354\n",
            "Epoch  21 Iteration  59 Loss:  0.5237430334091187\n",
            "Epoch  21 Iteration  60 Loss:  0.39496690034866333\n",
            "Epoch  21 Iteration  61 Loss:  0.39954063296318054\n",
            "Epoch  21 Iteration  62 Loss:  0.44188159704208374\n",
            "Epoch  21 Iteration  63 Loss:  0.1619579941034317\n",
            "Epoch  21 Iteration  64 Loss:  0.34104257822036743\n",
            "Epoch  21 Iteration  65 Loss:  0.21109160780906677\n",
            "*********************************************************************\n",
            "EPOCH  21  ==> Loss:  0.38780729246862006  Time taken ==>  225.76347994804382\n",
            "*********************************************************************\n",
            "Epoch  22 Iteration  0 Loss:  0.40521788597106934\n",
            "Epoch  22 Iteration  1 Loss:  0.3157925009727478\n",
            "Epoch  22 Iteration  2 Loss:  0.21470609307289124\n",
            "Epoch  22 Iteration  3 Loss:  0.36249780654907227\n",
            "Epoch  22 Iteration  4 Loss:  0.26316800713539124\n",
            "Epoch  22 Iteration  5 Loss:  0.5698978900909424\n",
            "Epoch  22 Iteration  6 Loss:  0.2563583254814148\n",
            "Epoch  22 Iteration  7 Loss:  0.5637872219085693\n",
            "Epoch  22 Iteration  8 Loss:  0.22232909500598907\n",
            "Epoch  22 Iteration  9 Loss:  0.343636691570282\n",
            "Epoch  22 Iteration  10 Loss:  0.5320407748222351\n",
            "Epoch  22 Iteration  11 Loss:  0.2517537474632263\n",
            "Epoch  22 Iteration  12 Loss:  0.4971037209033966\n",
            "Epoch  22 Iteration  13 Loss:  0.3130984306335449\n",
            "Epoch  22 Iteration  14 Loss:  0.29561689496040344\n",
            "Epoch  22 Iteration  15 Loss:  0.6506430506706238\n",
            "Epoch  22 Iteration  16 Loss:  0.790148138999939\n",
            "Epoch  22 Iteration  17 Loss:  0.1301267147064209\n",
            "Epoch  22 Iteration  18 Loss:  0.3300980031490326\n",
            "Epoch  22 Iteration  19 Loss:  0.23407503962516785\n",
            "Epoch  22 Iteration  20 Loss:  0.21623243391513824\n",
            "Epoch  22 Iteration  21 Loss:  0.278464138507843\n",
            "Epoch  22 Iteration  22 Loss:  0.37185990810394287\n",
            "Epoch  22 Iteration  23 Loss:  0.6677597165107727\n",
            "Epoch  22 Iteration  24 Loss:  0.16531209647655487\n",
            "Epoch  22 Iteration  25 Loss:  0.7252213358879089\n",
            "Epoch  22 Iteration  26 Loss:  0.1882694959640503\n",
            "Epoch  22 Iteration  27 Loss:  0.16492091119289398\n",
            "Epoch  22 Iteration  28 Loss:  0.4520464539527893\n",
            "Epoch  22 Iteration  29 Loss:  0.41029292345046997\n",
            "Epoch  22 Iteration  30 Loss:  0.7731471657752991\n",
            "Epoch  22 Iteration  31 Loss:  0.3859148621559143\n",
            "Epoch  22 Iteration  32 Loss:  0.4551103413105011\n",
            "Epoch  22 Iteration  33 Loss:  0.4210020899772644\n",
            "Epoch  22 Iteration  34 Loss:  0.46981069445610046\n",
            "Epoch  22 Iteration  35 Loss:  0.2898566424846649\n",
            "Epoch  22 Iteration  36 Loss:  0.6204110383987427\n",
            "Epoch  22 Iteration  37 Loss:  0.1712515950202942\n",
            "Epoch  22 Iteration  38 Loss:  0.2735673785209656\n",
            "Epoch  22 Iteration  39 Loss:  0.3731541037559509\n",
            "Epoch  22 Iteration  40 Loss:  0.3061791956424713\n",
            "Epoch  22 Iteration  41 Loss:  0.5597051978111267\n",
            "Epoch  22 Iteration  42 Loss:  0.4241200089454651\n",
            "Epoch  22 Iteration  43 Loss:  0.3364536762237549\n",
            "Epoch  22 Iteration  44 Loss:  0.4548811912536621\n",
            "Epoch  22 Iteration  45 Loss:  0.434889018535614\n",
            "Epoch  22 Iteration  46 Loss:  0.4595927894115448\n",
            "Epoch  22 Iteration  47 Loss:  0.48146137595176697\n",
            "Epoch  22 Iteration  48 Loss:  0.3001369833946228\n",
            "Epoch  22 Iteration  49 Loss:  0.4624926745891571\n",
            "Epoch  22 Iteration  50 Loss:  0.2907673716545105\n",
            "Epoch  22 Iteration  51 Loss:  0.178701251745224\n",
            "Epoch  22 Iteration  52 Loss:  0.3208715319633484\n",
            "Epoch  22 Iteration  53 Loss:  0.3523654341697693\n",
            "Epoch  22 Iteration  54 Loss:  0.22393997013568878\n",
            "Epoch  22 Iteration  55 Loss:  0.2586098313331604\n",
            "Epoch  22 Iteration  56 Loss:  0.4375602900981903\n",
            "Epoch  22 Iteration  57 Loss:  0.29606541991233826\n",
            "Epoch  22 Iteration  58 Loss:  0.2818138599395752\n",
            "Epoch  22 Iteration  59 Loss:  0.23860199749469757\n",
            "Epoch  22 Iteration  60 Loss:  0.36798739433288574\n",
            "Epoch  22 Iteration  61 Loss:  0.14549274742603302\n",
            "Epoch  22 Iteration  62 Loss:  0.36066964268684387\n",
            "Epoch  22 Iteration  63 Loss:  0.5615811347961426\n",
            "Epoch  22 Iteration  64 Loss:  0.6383458971977234\n",
            "Epoch  22 Iteration  65 Loss:  0.20509444177150726\n",
            "*********************************************************************\n",
            "EPOCH  22  ==> Loss:  0.37566790436253406  Time taken ==>  226.62216973304749\n",
            "*********************************************************************\n",
            "Epoch  23 Iteration  0 Loss:  0.5924095511436462\n",
            "Epoch  23 Iteration  1 Loss:  0.3472535014152527\n",
            "Epoch  23 Iteration  2 Loss:  0.19409966468811035\n",
            "Epoch  23 Iteration  3 Loss:  0.1967349648475647\n",
            "Epoch  23 Iteration  4 Loss:  0.27689677476882935\n",
            "Epoch  23 Iteration  5 Loss:  0.33816805481910706\n",
            "Epoch  23 Iteration  6 Loss:  0.4553987979888916\n",
            "Epoch  23 Iteration  7 Loss:  0.29294800758361816\n",
            "Epoch  23 Iteration  8 Loss:  0.3496328890323639\n",
            "Epoch  23 Iteration  9 Loss:  0.371390163898468\n",
            "Epoch  23 Iteration  10 Loss:  0.10645374655723572\n",
            "Epoch  23 Iteration  11 Loss:  0.646019458770752\n",
            "Epoch  23 Iteration  12 Loss:  0.3687754273414612\n",
            "Epoch  23 Iteration  13 Loss:  0.4229821264743805\n",
            "Epoch  23 Iteration  14 Loss:  0.31576186418533325\n",
            "Epoch  23 Iteration  15 Loss:  0.4849803149700165\n",
            "Epoch  23 Iteration  16 Loss:  0.31555062532424927\n",
            "Epoch  23 Iteration  17 Loss:  0.4280961751937866\n",
            "Epoch  23 Iteration  18 Loss:  0.49685755372047424\n",
            "Epoch  23 Iteration  19 Loss:  0.5757849216461182\n",
            "Epoch  23 Iteration  20 Loss:  0.4262118637561798\n",
            "Epoch  23 Iteration  21 Loss:  0.5793817043304443\n",
            "Epoch  23 Iteration  22 Loss:  0.4476621747016907\n",
            "Epoch  23 Iteration  23 Loss:  0.3282023072242737\n",
            "Epoch  23 Iteration  24 Loss:  0.15243710577487946\n",
            "Epoch  23 Iteration  25 Loss:  0.3266758620738983\n",
            "Epoch  23 Iteration  26 Loss:  0.3113502264022827\n",
            "Epoch  23 Iteration  27 Loss:  0.2612774670124054\n",
            "Epoch  23 Iteration  28 Loss:  0.3185672461986542\n",
            "Epoch  23 Iteration  29 Loss:  0.4329257607460022\n",
            "Epoch  23 Iteration  30 Loss:  0.37226736545562744\n",
            "Epoch  23 Iteration  31 Loss:  0.30591124296188354\n",
            "Epoch  23 Iteration  32 Loss:  0.3098696768283844\n",
            "Epoch  23 Iteration  33 Loss:  0.35210952162742615\n",
            "Epoch  23 Iteration  34 Loss:  0.2953885793685913\n",
            "Epoch  23 Iteration  35 Loss:  0.5329122543334961\n",
            "Epoch  23 Iteration  36 Loss:  0.44461071491241455\n",
            "Epoch  23 Iteration  37 Loss:  0.41573792695999146\n",
            "Epoch  23 Iteration  38 Loss:  0.32379651069641113\n",
            "Epoch  23 Iteration  39 Loss:  0.317444771528244\n",
            "Epoch  23 Iteration  40 Loss:  0.38483044505119324\n",
            "Epoch  23 Iteration  41 Loss:  0.2199462205171585\n",
            "Epoch  23 Iteration  42 Loss:  0.39079374074935913\n",
            "Epoch  23 Iteration  43 Loss:  0.24047797918319702\n",
            "Epoch  23 Iteration  44 Loss:  0.43656179308891296\n",
            "Epoch  23 Iteration  45 Loss:  0.3132070302963257\n",
            "Epoch  23 Iteration  46 Loss:  0.4351085424423218\n",
            "Epoch  23 Iteration  47 Loss:  0.17760248482227325\n",
            "Epoch  23 Iteration  48 Loss:  0.41921111941337585\n",
            "Epoch  23 Iteration  49 Loss:  0.5663805603981018\n",
            "Epoch  23 Iteration  50 Loss:  0.4993770718574524\n",
            "Epoch  23 Iteration  51 Loss:  0.6358383297920227\n",
            "Epoch  23 Iteration  52 Loss:  0.47573769092559814\n",
            "Epoch  23 Iteration  53 Loss:  0.31263893842697144\n",
            "Epoch  23 Iteration  54 Loss:  0.24026593565940857\n",
            "Epoch  23 Iteration  55 Loss:  0.306243360042572\n",
            "Epoch  23 Iteration  56 Loss:  0.396007239818573\n",
            "Epoch  23 Iteration  57 Loss:  0.4892948865890503\n",
            "Epoch  23 Iteration  58 Loss:  0.4488171339035034\n",
            "Epoch  23 Iteration  59 Loss:  0.27565592527389526\n",
            "Epoch  23 Iteration  60 Loss:  0.272493839263916\n",
            "Epoch  23 Iteration  61 Loss:  0.2396773099899292\n",
            "Epoch  23 Iteration  62 Loss:  0.40972304344177246\n",
            "Epoch  23 Iteration  63 Loss:  0.3864299952983856\n",
            "Epoch  23 Iteration  64 Loss:  0.4004049301147461\n",
            "Epoch  23 Iteration  65 Loss:  0.5315957069396973\n",
            "*********************************************************************\n",
            "EPOCH  23  ==> Loss:  0.37477660788731143  Time taken ==>  226.23221373558044\n",
            "*********************************************************************\n",
            "Epoch  24 Iteration  0 Loss:  0.160827174782753\n",
            "Epoch  24 Iteration  1 Loss:  0.5103907585144043\n",
            "Epoch  24 Iteration  2 Loss:  0.6538962125778198\n",
            "Epoch  24 Iteration  3 Loss:  0.20469315350055695\n",
            "Epoch  24 Iteration  4 Loss:  0.3409233093261719\n",
            "Epoch  24 Iteration  5 Loss:  0.2679675817489624\n",
            "Epoch  24 Iteration  6 Loss:  0.7629255056381226\n",
            "Epoch  24 Iteration  7 Loss:  0.34342288970947266\n",
            "Epoch  24 Iteration  8 Loss:  0.3540308475494385\n",
            "Epoch  24 Iteration  9 Loss:  0.45721757411956787\n",
            "Epoch  24 Iteration  10 Loss:  0.4681848883628845\n",
            "Epoch  24 Iteration  11 Loss:  0.15738970041275024\n",
            "Epoch  24 Iteration  12 Loss:  0.19464832544326782\n",
            "Epoch  24 Iteration  13 Loss:  0.3737662434577942\n",
            "Epoch  24 Iteration  14 Loss:  0.3653913140296936\n",
            "Epoch  24 Iteration  15 Loss:  0.4331105351448059\n",
            "Epoch  24 Iteration  16 Loss:  0.24860836565494537\n",
            "Epoch  24 Iteration  17 Loss:  0.4237889349460602\n",
            "Epoch  24 Iteration  18 Loss:  0.20686930418014526\n",
            "Epoch  24 Iteration  19 Loss:  0.2752918601036072\n",
            "Epoch  24 Iteration  20 Loss:  0.3793395757675171\n",
            "Epoch  24 Iteration  21 Loss:  0.19794435799121857\n",
            "Epoch  24 Iteration  22 Loss:  0.21327416598796844\n",
            "Epoch  24 Iteration  23 Loss:  0.3834521174430847\n",
            "Epoch  24 Iteration  24 Loss:  0.3955903649330139\n",
            "Epoch  24 Iteration  25 Loss:  0.580501914024353\n",
            "Epoch  24 Iteration  26 Loss:  0.24260015785694122\n",
            "Epoch  24 Iteration  27 Loss:  0.3073304295539856\n",
            "Epoch  24 Iteration  28 Loss:  0.7769564986228943\n",
            "Epoch  24 Iteration  29 Loss:  0.4882071316242218\n",
            "Epoch  24 Iteration  30 Loss:  0.5442816615104675\n",
            "Epoch  24 Iteration  31 Loss:  0.5332841277122498\n",
            "Epoch  24 Iteration  32 Loss:  0.24339550733566284\n",
            "Epoch  24 Iteration  33 Loss:  0.2744445502758026\n",
            "Epoch  24 Iteration  34 Loss:  0.17337194085121155\n",
            "Epoch  24 Iteration  35 Loss:  0.30292144417762756\n",
            "Epoch  24 Iteration  36 Loss:  0.15333029627799988\n",
            "Epoch  24 Iteration  37 Loss:  0.47558873891830444\n",
            "Epoch  24 Iteration  38 Loss:  0.19679361581802368\n",
            "Epoch  24 Iteration  39 Loss:  0.23969663679599762\n",
            "Epoch  24 Iteration  40 Loss:  0.2048034965991974\n",
            "Epoch  24 Iteration  41 Loss:  0.6061631441116333\n",
            "Epoch  24 Iteration  42 Loss:  0.379738450050354\n",
            "Epoch  24 Iteration  43 Loss:  0.24007724225521088\n",
            "Epoch  24 Iteration  44 Loss:  0.3231423795223236\n",
            "Epoch  24 Iteration  45 Loss:  0.571717381477356\n",
            "Epoch  24 Iteration  46 Loss:  0.26736271381378174\n",
            "Epoch  24 Iteration  47 Loss:  0.5355744957923889\n",
            "Epoch  24 Iteration  48 Loss:  0.47154176235198975\n",
            "Epoch  24 Iteration  49 Loss:  0.31337419152259827\n",
            "Epoch  24 Iteration  50 Loss:  0.15312768518924713\n",
            "Epoch  24 Iteration  51 Loss:  0.4424750804901123\n",
            "Epoch  24 Iteration  52 Loss:  0.5250570774078369\n",
            "Epoch  24 Iteration  53 Loss:  0.4370120167732239\n",
            "Epoch  24 Iteration  54 Loss:  0.4128345549106598\n",
            "Epoch  24 Iteration  55 Loss:  0.33223265409469604\n",
            "Epoch  24 Iteration  56 Loss:  0.34139177203178406\n",
            "Epoch  24 Iteration  57 Loss:  0.2907071113586426\n",
            "Epoch  24 Iteration  58 Loss:  0.5444137454032898\n",
            "Epoch  24 Iteration  59 Loss:  0.4485929310321808\n",
            "Epoch  24 Iteration  60 Loss:  0.3173026442527771\n",
            "Epoch  24 Iteration  61 Loss:  0.12462667375802994\n",
            "Epoch  24 Iteration  62 Loss:  0.22596704959869385\n",
            "Epoch  24 Iteration  63 Loss:  0.4143785238265991\n",
            "Epoch  24 Iteration  64 Loss:  0.2825412452220917\n",
            "Epoch  24 Iteration  65 Loss:  1.1005558967590332\n",
            "*********************************************************************\n",
            "EPOCH  24  ==> Loss:  0.3729145701861743  Time taken ==>  227.73024201393127\n",
            "*********************************************************************\n",
            "Epoch  25 Iteration  0 Loss:  0.23754286766052246\n",
            "Epoch  25 Iteration  1 Loss:  0.31503891944885254\n",
            "Epoch  25 Iteration  2 Loss:  0.38907352089881897\n",
            "Epoch  25 Iteration  3 Loss:  0.38785749673843384\n",
            "Epoch  25 Iteration  4 Loss:  0.28281527757644653\n",
            "Epoch  25 Iteration  5 Loss:  0.7463146448135376\n",
            "Epoch  25 Iteration  6 Loss:  0.6486037969589233\n",
            "Epoch  25 Iteration  7 Loss:  0.4709622263908386\n",
            "Epoch  25 Iteration  8 Loss:  0.4228806793689728\n",
            "Epoch  25 Iteration  9 Loss:  0.6667158007621765\n",
            "Epoch  25 Iteration  10 Loss:  0.35535216331481934\n",
            "Epoch  25 Iteration  11 Loss:  0.20342712104320526\n",
            "Epoch  25 Iteration  12 Loss:  0.4642769396305084\n",
            "Epoch  25 Iteration  13 Loss:  0.4789388179779053\n",
            "Epoch  25 Iteration  14 Loss:  0.4189578592777252\n",
            "Epoch  25 Iteration  15 Loss:  0.31100866198539734\n",
            "Epoch  25 Iteration  16 Loss:  0.44881346821784973\n",
            "Epoch  25 Iteration  17 Loss:  0.34355679154396057\n",
            "Epoch  25 Iteration  18 Loss:  0.4921172261238098\n",
            "Epoch  25 Iteration  19 Loss:  0.5481579303741455\n",
            "Epoch  25 Iteration  20 Loss:  0.2966771125793457\n",
            "Epoch  25 Iteration  21 Loss:  0.4260406792163849\n",
            "Epoch  25 Iteration  22 Loss:  0.49655282497406006\n",
            "Epoch  25 Iteration  23 Loss:  0.5834423303604126\n",
            "Epoch  25 Iteration  24 Loss:  0.33698779344558716\n",
            "Epoch  25 Iteration  25 Loss:  0.5753086805343628\n",
            "Epoch  25 Iteration  26 Loss:  0.5010369420051575\n",
            "Epoch  25 Iteration  27 Loss:  0.7566369771957397\n",
            "Epoch  25 Iteration  28 Loss:  0.3686126470565796\n",
            "Epoch  25 Iteration  29 Loss:  0.4350460171699524\n",
            "Epoch  25 Iteration  30 Loss:  0.4308058023452759\n",
            "Epoch  25 Iteration  31 Loss:  0.3383007049560547\n",
            "Epoch  25 Iteration  32 Loss:  0.5880244970321655\n",
            "Epoch  25 Iteration  33 Loss:  0.5544605851173401\n",
            "Epoch  25 Iteration  34 Loss:  0.6737103462219238\n",
            "Epoch  25 Iteration  35 Loss:  0.3867749571800232\n",
            "Epoch  25 Iteration  36 Loss:  0.3569866716861725\n",
            "Epoch  25 Iteration  37 Loss:  0.7148520350456238\n",
            "Epoch  25 Iteration  38 Loss:  0.2986772656440735\n",
            "Epoch  25 Iteration  39 Loss:  0.40973764657974243\n",
            "Epoch  25 Iteration  40 Loss:  0.5060146450996399\n",
            "Epoch  25 Iteration  41 Loss:  0.4521339535713196\n",
            "Epoch  25 Iteration  42 Loss:  0.5495913624763489\n",
            "Epoch  25 Iteration  43 Loss:  0.4587191343307495\n",
            "Epoch  25 Iteration  44 Loss:  0.18731006979942322\n",
            "Epoch  25 Iteration  45 Loss:  0.4665963649749756\n",
            "Epoch  25 Iteration  46 Loss:  0.20254302024841309\n",
            "Epoch  25 Iteration  47 Loss:  0.4183100759983063\n",
            "Epoch  25 Iteration  48 Loss:  0.6235196590423584\n",
            "Epoch  25 Iteration  49 Loss:  0.4380628764629364\n",
            "Epoch  25 Iteration  50 Loss:  0.5295532941818237\n",
            "Epoch  25 Iteration  51 Loss:  0.2815328538417816\n",
            "Epoch  25 Iteration  52 Loss:  0.39455321431159973\n",
            "Epoch  25 Iteration  53 Loss:  0.22112016379833221\n",
            "Epoch  25 Iteration  54 Loss:  0.3363457918167114\n",
            "Epoch  25 Iteration  55 Loss:  0.32014143466949463\n",
            "Epoch  25 Iteration  56 Loss:  0.24764224886894226\n",
            "Epoch  25 Iteration  57 Loss:  0.3722214698791504\n",
            "Epoch  25 Iteration  58 Loss:  0.4194663166999817\n",
            "Epoch  25 Iteration  59 Loss:  0.4384472966194153\n",
            "Epoch  25 Iteration  60 Loss:  0.36196285486221313\n",
            "Epoch  25 Iteration  61 Loss:  0.32878372073173523\n",
            "Epoch  25 Iteration  62 Loss:  0.3426090478897095\n",
            "Epoch  25 Iteration  63 Loss:  0.4014286994934082\n",
            "Epoch  25 Iteration  64 Loss:  0.3333401679992676\n",
            "Epoch  25 Iteration  65 Loss:  0.17242078483104706\n",
            "*********************************************************************\n",
            "EPOCH  25  ==> Loss:  0.4237190189235138  Time taken ==>  229.09296989440918\n",
            "*********************************************************************\n",
            "Epoch  26 Iteration  0 Loss:  0.5222583413124084\n",
            "Epoch  26 Iteration  1 Loss:  0.4001954197883606\n",
            "Epoch  26 Iteration  2 Loss:  0.3930424749851227\n",
            "Epoch  26 Iteration  3 Loss:  0.47260570526123047\n",
            "Epoch  26 Iteration  4 Loss:  0.38645535707473755\n",
            "Epoch  26 Iteration  5 Loss:  0.4270944595336914\n",
            "Epoch  26 Iteration  6 Loss:  0.4871031641960144\n",
            "Epoch  26 Iteration  7 Loss:  0.4304659366607666\n",
            "Epoch  26 Iteration  8 Loss:  0.2834528088569641\n",
            "Epoch  26 Iteration  9 Loss:  0.3647359311580658\n",
            "Epoch  26 Iteration  10 Loss:  0.2657507658004761\n",
            "Epoch  26 Iteration  11 Loss:  0.41977232694625854\n",
            "Epoch  26 Iteration  12 Loss:  0.42736151814460754\n",
            "Epoch  26 Iteration  13 Loss:  0.16154737770557404\n",
            "Epoch  26 Iteration  14 Loss:  0.38951778411865234\n",
            "Epoch  26 Iteration  15 Loss:  0.31772828102111816\n",
            "Epoch  26 Iteration  16 Loss:  0.46225371956825256\n",
            "Epoch  26 Iteration  17 Loss:  0.41989344358444214\n",
            "Epoch  26 Iteration  18 Loss:  0.21324725449085236\n",
            "Epoch  26 Iteration  19 Loss:  0.19307732582092285\n",
            "Epoch  26 Iteration  20 Loss:  0.27964577078819275\n",
            "Epoch  26 Iteration  21 Loss:  0.40591245889663696\n",
            "Epoch  26 Iteration  22 Loss:  0.5681752562522888\n",
            "Epoch  26 Iteration  23 Loss:  0.2135852873325348\n",
            "Epoch  26 Iteration  24 Loss:  0.31404516100883484\n",
            "Epoch  26 Iteration  25 Loss:  0.270751953125\n",
            "Epoch  26 Iteration  26 Loss:  0.3413974642753601\n",
            "Epoch  26 Iteration  27 Loss:  0.4158286154270172\n",
            "Epoch  26 Iteration  28 Loss:  0.31955885887145996\n",
            "Epoch  26 Iteration  29 Loss:  0.36224591732025146\n",
            "Epoch  26 Iteration  30 Loss:  0.18150381743907928\n",
            "Epoch  26 Iteration  31 Loss:  0.46256929636001587\n",
            "Epoch  26 Iteration  32 Loss:  0.5007213354110718\n",
            "Epoch  26 Iteration  33 Loss:  0.5228888988494873\n",
            "Epoch  26 Iteration  34 Loss:  0.2770082652568817\n",
            "Epoch  26 Iteration  35 Loss:  0.3780782222747803\n",
            "Epoch  26 Iteration  36 Loss:  0.45661890506744385\n",
            "Epoch  26 Iteration  37 Loss:  0.2519010305404663\n",
            "Epoch  26 Iteration  38 Loss:  0.35054337978363037\n",
            "Epoch  26 Iteration  39 Loss:  0.3050621747970581\n",
            "Epoch  26 Iteration  40 Loss:  0.11343524605035782\n",
            "Epoch  26 Iteration  41 Loss:  0.3599385917186737\n",
            "Epoch  26 Iteration  42 Loss:  0.2826690375804901\n",
            "Epoch  26 Iteration  43 Loss:  0.36115577816963196\n",
            "Epoch  26 Iteration  44 Loss:  0.35506701469421387\n",
            "Epoch  26 Iteration  45 Loss:  0.3804071545600891\n",
            "Epoch  26 Iteration  46 Loss:  0.4278722107410431\n",
            "Epoch  26 Iteration  47 Loss:  0.3422926366329193\n",
            "Epoch  26 Iteration  48 Loss:  0.3042879104614258\n",
            "Epoch  26 Iteration  49 Loss:  0.15901535749435425\n",
            "Epoch  26 Iteration  50 Loss:  0.264143168926239\n",
            "Epoch  26 Iteration  51 Loss:  0.30068713426589966\n",
            "Epoch  26 Iteration  52 Loss:  0.36789417266845703\n",
            "Epoch  26 Iteration  53 Loss:  0.35818779468536377\n",
            "Epoch  26 Iteration  54 Loss:  0.39049041271209717\n",
            "Epoch  26 Iteration  55 Loss:  0.39856797456741333\n",
            "Epoch  26 Iteration  56 Loss:  0.4165304899215698\n",
            "Epoch  26 Iteration  57 Loss:  0.3010260760784149\n",
            "Epoch  26 Iteration  58 Loss:  0.358897864818573\n",
            "Epoch  26 Iteration  59 Loss:  0.2339583784341812\n",
            "Epoch  26 Iteration  60 Loss:  0.39883196353912354\n",
            "Epoch  26 Iteration  61 Loss:  0.4359809458255768\n",
            "Epoch  26 Iteration  62 Loss:  0.4681812524795532\n",
            "Epoch  26 Iteration  63 Loss:  0.2797318994998932\n",
            "Epoch  26 Iteration  64 Loss:  0.275088369846344\n",
            "Epoch  26 Iteration  65 Loss:  0.4473019242286682\n",
            "*********************************************************************\n",
            "EPOCH  26  ==> Loss:  0.3545036700864633  Time taken ==>  229.94433665275574\n",
            "*********************************************************************\n",
            "Epoch  27 Iteration  0 Loss:  0.4151226282119751\n",
            "Epoch  27 Iteration  1 Loss:  0.22333306074142456\n",
            "Epoch  27 Iteration  2 Loss:  0.2902052402496338\n",
            "Epoch  27 Iteration  3 Loss:  0.5496479272842407\n",
            "Epoch  27 Iteration  4 Loss:  0.2635910212993622\n",
            "Epoch  27 Iteration  5 Loss:  0.35136669874191284\n",
            "Epoch  27 Iteration  6 Loss:  0.20939363539218903\n",
            "Epoch  27 Iteration  7 Loss:  0.23906973004341125\n",
            "Epoch  27 Iteration  8 Loss:  0.11072476208209991\n",
            "Epoch  27 Iteration  9 Loss:  0.5531469583511353\n",
            "Epoch  27 Iteration  10 Loss:  0.2819269299507141\n",
            "Epoch  27 Iteration  11 Loss:  0.40452319383621216\n",
            "Epoch  27 Iteration  12 Loss:  0.2733955681324005\n",
            "Epoch  27 Iteration  13 Loss:  0.3361506760120392\n",
            "Epoch  27 Iteration  14 Loss:  0.43439817428588867\n",
            "Epoch  27 Iteration  15 Loss:  0.47064727544784546\n",
            "Epoch  27 Iteration  16 Loss:  0.3199706971645355\n",
            "Epoch  27 Iteration  17 Loss:  0.31129539012908936\n",
            "Epoch  27 Iteration  18 Loss:  0.5366984605789185\n",
            "Epoch  27 Iteration  19 Loss:  0.28889644145965576\n",
            "Epoch  27 Iteration  20 Loss:  0.23744484782218933\n",
            "Epoch  27 Iteration  21 Loss:  0.4655190110206604\n",
            "Epoch  27 Iteration  22 Loss:  0.26750433444976807\n",
            "Epoch  27 Iteration  23 Loss:  0.2157486230134964\n",
            "Epoch  27 Iteration  24 Loss:  0.288728266954422\n",
            "Epoch  27 Iteration  25 Loss:  0.5428745746612549\n",
            "Epoch  27 Iteration  26 Loss:  0.4457261562347412\n",
            "Epoch  27 Iteration  27 Loss:  0.5250883102416992\n",
            "Epoch  27 Iteration  28 Loss:  0.14175379276275635\n",
            "Epoch  27 Iteration  29 Loss:  0.4294271469116211\n",
            "Epoch  27 Iteration  30 Loss:  0.39143696427345276\n",
            "Epoch  27 Iteration  31 Loss:  0.4022558331489563\n",
            "Epoch  27 Iteration  32 Loss:  0.49283885955810547\n",
            "Epoch  27 Iteration  33 Loss:  0.574371337890625\n",
            "Epoch  27 Iteration  34 Loss:  0.21959316730499268\n",
            "Epoch  27 Iteration  35 Loss:  0.3540562391281128\n",
            "Epoch  27 Iteration  36 Loss:  0.6575555205345154\n",
            "Epoch  27 Iteration  37 Loss:  0.22416827082633972\n",
            "Epoch  27 Iteration  38 Loss:  0.27929043769836426\n",
            "Epoch  27 Iteration  39 Loss:  0.43848925828933716\n",
            "Epoch  27 Iteration  40 Loss:  0.22066247463226318\n",
            "Epoch  27 Iteration  41 Loss:  0.5437471270561218\n",
            "Epoch  27 Iteration  42 Loss:  0.35758188366889954\n",
            "Epoch  27 Iteration  43 Loss:  0.2992208003997803\n",
            "Epoch  27 Iteration  44 Loss:  0.39332398772239685\n",
            "Epoch  27 Iteration  45 Loss:  0.5826058983802795\n",
            "Epoch  27 Iteration  46 Loss:  0.3241943120956421\n",
            "Epoch  27 Iteration  47 Loss:  0.5490177273750305\n",
            "Epoch  27 Iteration  48 Loss:  0.19584381580352783\n",
            "Epoch  27 Iteration  49 Loss:  0.38583582639694214\n",
            "Epoch  27 Iteration  50 Loss:  0.34403854608535767\n",
            "Epoch  27 Iteration  51 Loss:  0.31581488251686096\n",
            "Epoch  27 Iteration  52 Loss:  0.21803657710552216\n",
            "Epoch  27 Iteration  53 Loss:  0.15480506420135498\n",
            "Epoch  27 Iteration  54 Loss:  0.366393119096756\n",
            "Epoch  27 Iteration  55 Loss:  0.636175811290741\n",
            "Epoch  27 Iteration  56 Loss:  0.28634729981422424\n",
            "Epoch  27 Iteration  57 Loss:  0.2904741168022156\n",
            "Epoch  27 Iteration  58 Loss:  0.20686742663383484\n",
            "Epoch  27 Iteration  59 Loss:  0.3447538912296295\n",
            "Epoch  27 Iteration  60 Loss:  0.4273476302623749\n",
            "Epoch  27 Iteration  61 Loss:  0.21209432184696198\n",
            "Epoch  27 Iteration  62 Loss:  0.5334028005599976\n",
            "Epoch  27 Iteration  63 Loss:  0.1020728275179863\n",
            "Epoch  27 Iteration  64 Loss:  0.29788944125175476\n",
            "Epoch  27 Iteration  65 Loss:  0.33479809761047363\n",
            "*********************************************************************\n",
            "EPOCH  27  ==> Loss:  0.3542533504466216  Time taken ==>  229.34870791435242\n",
            "*********************************************************************\n",
            "Epoch  28 Iteration  0 Loss:  0.2317570000886917\n",
            "Epoch  28 Iteration  1 Loss:  0.3086623549461365\n",
            "Epoch  28 Iteration  2 Loss:  0.2840515077114105\n",
            "Epoch  28 Iteration  3 Loss:  0.4201870560646057\n",
            "Epoch  28 Iteration  4 Loss:  0.3456493020057678\n",
            "Epoch  28 Iteration  5 Loss:  0.38433873653411865\n",
            "Epoch  28 Iteration  6 Loss:  0.2602311074733734\n",
            "Epoch  28 Iteration  7 Loss:  0.23545847833156586\n",
            "Epoch  28 Iteration  8 Loss:  0.41126126050949097\n",
            "Epoch  28 Iteration  9 Loss:  0.37064316868782043\n",
            "Epoch  28 Iteration  10 Loss:  0.3084823787212372\n",
            "Epoch  28 Iteration  11 Loss:  0.3669606149196625\n",
            "Epoch  28 Iteration  12 Loss:  0.25603705644607544\n",
            "Epoch  28 Iteration  13 Loss:  0.2107267677783966\n",
            "Epoch  28 Iteration  14 Loss:  0.7676958441734314\n",
            "Epoch  28 Iteration  15 Loss:  0.2177530825138092\n",
            "Epoch  28 Iteration  16 Loss:  0.42840802669525146\n",
            "Epoch  28 Iteration  17 Loss:  0.18366289138793945\n",
            "Epoch  28 Iteration  18 Loss:  0.259775847196579\n",
            "Epoch  28 Iteration  19 Loss:  0.660839319229126\n",
            "Epoch  28 Iteration  20 Loss:  0.31168419122695923\n",
            "Epoch  28 Iteration  21 Loss:  0.35958021879196167\n",
            "Epoch  28 Iteration  22 Loss:  0.39504748582839966\n",
            "Epoch  28 Iteration  23 Loss:  0.34071269631385803\n",
            "Epoch  28 Iteration  24 Loss:  0.39497941732406616\n",
            "Epoch  28 Iteration  25 Loss:  0.28815779089927673\n",
            "Epoch  28 Iteration  26 Loss:  0.3416178822517395\n",
            "Epoch  28 Iteration  27 Loss:  0.20435552299022675\n",
            "Epoch  28 Iteration  28 Loss:  0.30748406052589417\n",
            "Epoch  28 Iteration  29 Loss:  0.29562410712242126\n",
            "Epoch  28 Iteration  30 Loss:  0.45824894309043884\n",
            "Epoch  28 Iteration  31 Loss:  0.4599917531013489\n",
            "Epoch  28 Iteration  32 Loss:  0.3669160008430481\n",
            "Epoch  28 Iteration  33 Loss:  0.18751195073127747\n",
            "Epoch  28 Iteration  34 Loss:  0.29302728176116943\n",
            "Epoch  28 Iteration  35 Loss:  0.33256733417510986\n",
            "Epoch  28 Iteration  36 Loss:  0.3170228898525238\n",
            "Epoch  28 Iteration  37 Loss:  0.4339553117752075\n",
            "Epoch  28 Iteration  38 Loss:  0.38740843534469604\n",
            "Epoch  28 Iteration  39 Loss:  0.225425586104393\n",
            "Epoch  28 Iteration  40 Loss:  0.21378198266029358\n",
            "Epoch  28 Iteration  41 Loss:  0.3866933584213257\n",
            "Epoch  28 Iteration  42 Loss:  0.32253193855285645\n",
            "Epoch  28 Iteration  43 Loss:  0.35043051838874817\n",
            "Epoch  28 Iteration  44 Loss:  0.2286268025636673\n",
            "Epoch  28 Iteration  45 Loss:  0.346349835395813\n",
            "Epoch  28 Iteration  46 Loss:  0.30113786458969116\n",
            "Epoch  28 Iteration  47 Loss:  0.39573508501052856\n",
            "Epoch  28 Iteration  48 Loss:  0.4223930239677429\n",
            "Epoch  28 Iteration  49 Loss:  0.4672881066799164\n",
            "Epoch  28 Iteration  50 Loss:  0.29145509004592896\n",
            "Epoch  28 Iteration  51 Loss:  0.13048037886619568\n",
            "Epoch  28 Iteration  52 Loss:  0.5167657136917114\n",
            "Epoch  28 Iteration  53 Loss:  0.3296510577201843\n",
            "Epoch  28 Iteration  54 Loss:  0.1905924379825592\n",
            "Epoch  28 Iteration  55 Loss:  0.1151181161403656\n",
            "Epoch  28 Iteration  56 Loss:  0.37791574001312256\n",
            "Epoch  28 Iteration  57 Loss:  0.5481226444244385\n",
            "Epoch  28 Iteration  58 Loss:  0.4617738723754883\n",
            "Epoch  28 Iteration  59 Loss:  0.34214168787002563\n",
            "Epoch  28 Iteration  60 Loss:  0.6942735314369202\n",
            "Epoch  28 Iteration  61 Loss:  0.5944183468818665\n",
            "Epoch  28 Iteration  62 Loss:  0.18649815022945404\n",
            "Epoch  28 Iteration  63 Loss:  0.6656014919281006\n",
            "Epoch  28 Iteration  64 Loss:  0.3014485239982605\n",
            "Epoch  28 Iteration  65 Loss:  0.3059001863002777\n",
            "*********************************************************************\n",
            "EPOCH  28  ==> Loss:  0.35001554769096954  Time taken ==>  227.38887810707092\n",
            "*********************************************************************\n",
            "Epoch  29 Iteration  0 Loss:  0.22857996821403503\n",
            "Epoch  29 Iteration  1 Loss:  0.4531189203262329\n",
            "Epoch  29 Iteration  2 Loss:  0.2811112701892853\n",
            "Epoch  29 Iteration  3 Loss:  0.39568838477134705\n",
            "Epoch  29 Iteration  4 Loss:  0.2617761492729187\n",
            "Epoch  29 Iteration  5 Loss:  0.5838466286659241\n",
            "Epoch  29 Iteration  6 Loss:  0.4532962441444397\n",
            "Epoch  29 Iteration  7 Loss:  0.28809815645217896\n",
            "Epoch  29 Iteration  8 Loss:  0.47417527437210083\n",
            "Epoch  29 Iteration  9 Loss:  0.39479750394821167\n",
            "Epoch  29 Iteration  10 Loss:  0.227732315659523\n",
            "Epoch  29 Iteration  11 Loss:  0.26555436849594116\n",
            "Epoch  29 Iteration  12 Loss:  0.2720804810523987\n",
            "Epoch  29 Iteration  13 Loss:  0.37879684567451477\n",
            "Epoch  29 Iteration  14 Loss:  0.16837887465953827\n",
            "Epoch  29 Iteration  15 Loss:  0.15138646960258484\n",
            "Epoch  29 Iteration  16 Loss:  0.5645259618759155\n",
            "Epoch  29 Iteration  17 Loss:  0.358568012714386\n",
            "Epoch  29 Iteration  18 Loss:  0.3423701822757721\n",
            "Epoch  29 Iteration  19 Loss:  0.2164599597454071\n",
            "Epoch  29 Iteration  20 Loss:  0.17975908517837524\n",
            "Epoch  29 Iteration  21 Loss:  0.16641972959041595\n",
            "Epoch  29 Iteration  22 Loss:  0.3002636432647705\n",
            "Epoch  29 Iteration  23 Loss:  0.6960216760635376\n",
            "Epoch  29 Iteration  24 Loss:  0.2984585762023926\n",
            "Epoch  29 Iteration  25 Loss:  0.42762213945388794\n",
            "Epoch  29 Iteration  26 Loss:  0.18557870388031006\n",
            "Epoch  29 Iteration  27 Loss:  0.4404841661453247\n",
            "Epoch  29 Iteration  28 Loss:  0.33512336015701294\n",
            "Epoch  29 Iteration  29 Loss:  0.2793453335762024\n",
            "Epoch  29 Iteration  30 Loss:  0.5123753547668457\n",
            "Epoch  29 Iteration  31 Loss:  0.33523258566856384\n",
            "Epoch  29 Iteration  32 Loss:  0.32987233996391296\n",
            "Epoch  29 Iteration  33 Loss:  0.20162227749824524\n",
            "Epoch  29 Iteration  34 Loss:  0.2389133721590042\n",
            "Epoch  29 Iteration  35 Loss:  0.8766725063323975\n",
            "Epoch  29 Iteration  36 Loss:  0.35334283113479614\n",
            "Epoch  29 Iteration  37 Loss:  0.17772844433784485\n",
            "Epoch  29 Iteration  38 Loss:  0.3978000581264496\n",
            "Epoch  29 Iteration  39 Loss:  0.24450881779193878\n",
            "Epoch  29 Iteration  40 Loss:  0.28985944390296936\n",
            "Epoch  29 Iteration  41 Loss:  0.390884667634964\n",
            "Epoch  29 Iteration  42 Loss:  0.2992991507053375\n",
            "Epoch  29 Iteration  43 Loss:  0.33479681611061096\n",
            "Epoch  29 Iteration  44 Loss:  0.3398664593696594\n",
            "Epoch  29 Iteration  45 Loss:  0.4410572052001953\n",
            "Epoch  29 Iteration  46 Loss:  0.2313881516456604\n",
            "Epoch  29 Iteration  47 Loss:  0.3403359055519104\n",
            "Epoch  29 Iteration  48 Loss:  0.45765313506126404\n",
            "Epoch  29 Iteration  49 Loss:  0.2567753493785858\n",
            "Epoch  29 Iteration  50 Loss:  0.3897439241409302\n",
            "Epoch  29 Iteration  51 Loss:  0.231201171875\n",
            "Epoch  29 Iteration  52 Loss:  0.4100174307823181\n",
            "Epoch  29 Iteration  53 Loss:  0.14223724603652954\n",
            "Epoch  29 Iteration  54 Loss:  0.3557419776916504\n",
            "Epoch  29 Iteration  55 Loss:  0.15154410898685455\n",
            "Epoch  29 Iteration  56 Loss:  0.13447639346122742\n",
            "Epoch  29 Iteration  57 Loss:  0.19885720312595367\n",
            "Epoch  29 Iteration  58 Loss:  0.30387580394744873\n",
            "Epoch  29 Iteration  59 Loss:  0.3886810541152954\n",
            "Epoch  29 Iteration  60 Loss:  0.290282279253006\n",
            "Epoch  29 Iteration  61 Loss:  0.24808551371097565\n",
            "Epoch  29 Iteration  62 Loss:  0.271785706281662\n",
            "Epoch  29 Iteration  63 Loss:  0.6723461151123047\n",
            "Epoch  29 Iteration  64 Loss:  0.25213494896888733\n",
            "Epoch  29 Iteration  65 Loss:  0.7529154419898987\n",
            "*********************************************************************\n",
            "EPOCH  29  ==> Loss:  0.338080751173424  Time taken ==>  226.35885381698608\n",
            "*********************************************************************\n",
            "Epoch  30 Iteration  0 Loss:  0.2439325451850891\n",
            "Epoch  30 Iteration  1 Loss:  0.33195698261260986\n",
            "Epoch  30 Iteration  2 Loss:  0.21600432693958282\n",
            "Epoch  30 Iteration  3 Loss:  0.4216536581516266\n",
            "Epoch  30 Iteration  4 Loss:  0.1644313633441925\n",
            "Epoch  30 Iteration  5 Loss:  0.4264441728591919\n",
            "Epoch  30 Iteration  6 Loss:  0.4443517327308655\n",
            "Epoch  30 Iteration  7 Loss:  0.27369236946105957\n",
            "Epoch  30 Iteration  8 Loss:  0.289548397064209\n",
            "Epoch  30 Iteration  9 Loss:  0.5696672201156616\n",
            "Epoch  30 Iteration  10 Loss:  0.35260576009750366\n",
            "Epoch  30 Iteration  11 Loss:  0.46592259407043457\n",
            "Epoch  30 Iteration  12 Loss:  0.3524438440799713\n",
            "Epoch  30 Iteration  13 Loss:  0.44173645973205566\n",
            "Epoch  30 Iteration  14 Loss:  0.3581691086292267\n",
            "Epoch  30 Iteration  15 Loss:  0.33549025654792786\n",
            "Epoch  30 Iteration  16 Loss:  0.24211695790290833\n",
            "Epoch  30 Iteration  17 Loss:  0.44717252254486084\n",
            "Epoch  30 Iteration  18 Loss:  0.3738115727901459\n",
            "Epoch  30 Iteration  19 Loss:  0.3473396897315979\n",
            "Epoch  30 Iteration  20 Loss:  0.5284693241119385\n",
            "Epoch  30 Iteration  21 Loss:  0.3859570622444153\n",
            "Epoch  30 Iteration  22 Loss:  0.3235667943954468\n",
            "Epoch  30 Iteration  23 Loss:  0.29671159386634827\n",
            "Epoch  30 Iteration  24 Loss:  0.17697331309318542\n",
            "Epoch  30 Iteration  25 Loss:  0.5902105569839478\n",
            "Epoch  30 Iteration  26 Loss:  0.45046958327293396\n",
            "Epoch  30 Iteration  27 Loss:  0.37237870693206787\n",
            "Epoch  30 Iteration  28 Loss:  0.5299301147460938\n",
            "Epoch  30 Iteration  29 Loss:  0.3158889412879944\n",
            "Epoch  30 Iteration  30 Loss:  0.4710477590560913\n",
            "Epoch  30 Iteration  31 Loss:  0.283258318901062\n",
            "Epoch  30 Iteration  32 Loss:  0.27151960134506226\n",
            "Epoch  30 Iteration  33 Loss:  0.3706305921077728\n",
            "Epoch  30 Iteration  34 Loss:  0.2112891525030136\n",
            "Epoch  30 Iteration  35 Loss:  0.38978883624076843\n",
            "Epoch  30 Iteration  36 Loss:  0.3623550534248352\n",
            "Epoch  30 Iteration  37 Loss:  0.13329415023326874\n",
            "Epoch  30 Iteration  38 Loss:  0.2600768804550171\n",
            "Epoch  30 Iteration  39 Loss:  0.3407290577888489\n",
            "Epoch  30 Iteration  40 Loss:  0.2656850814819336\n",
            "Epoch  30 Iteration  41 Loss:  0.18296194076538086\n",
            "Epoch  30 Iteration  42 Loss:  0.7728856205940247\n",
            "Epoch  30 Iteration  43 Loss:  0.38835614919662476\n",
            "Epoch  30 Iteration  44 Loss:  0.4819939434528351\n",
            "Epoch  30 Iteration  45 Loss:  0.14620989561080933\n",
            "Epoch  30 Iteration  46 Loss:  0.17343169450759888\n",
            "Epoch  30 Iteration  47 Loss:  0.2801935076713562\n",
            "Epoch  30 Iteration  48 Loss:  0.5007588863372803\n",
            "Epoch  30 Iteration  49 Loss:  0.49134957790374756\n",
            "Epoch  30 Iteration  50 Loss:  0.29695576429367065\n",
            "Epoch  30 Iteration  51 Loss:  0.42673856019973755\n",
            "Epoch  30 Iteration  52 Loss:  0.22182220220565796\n",
            "Epoch  30 Iteration  53 Loss:  0.397869348526001\n",
            "Epoch  30 Iteration  54 Loss:  0.4295782744884491\n",
            "Epoch  30 Iteration  55 Loss:  0.2455986738204956\n",
            "Epoch  30 Iteration  56 Loss:  0.244299978017807\n",
            "Epoch  30 Iteration  57 Loss:  0.33116042613983154\n",
            "Epoch  30 Iteration  58 Loss:  0.2289479374885559\n",
            "Epoch  30 Iteration  59 Loss:  0.19347703456878662\n",
            "Epoch  30 Iteration  60 Loss:  0.8671274185180664\n",
            "Epoch  30 Iteration  61 Loss:  0.3526180684566498\n",
            "Epoch  30 Iteration  62 Loss:  0.31222498416900635\n",
            "Epoch  30 Iteration  63 Loss:  0.3380126357078552\n",
            "Epoch  30 Iteration  64 Loss:  0.4630424976348877\n",
            "Epoch  30 Iteration  65 Loss:  0.7658445835113525\n",
            "*********************************************************************\n",
            "EPOCH  30  ==> Loss:  0.36306335777044296  Time taken ==>  227.7433705329895\n",
            "*********************************************************************\n",
            "Epoch  31 Iteration  0 Loss:  0.16536515951156616\n",
            "Epoch  31 Iteration  1 Loss:  0.2161533385515213\n",
            "Epoch  31 Iteration  2 Loss:  0.32147017121315\n",
            "Epoch  31 Iteration  3 Loss:  0.2283819019794464\n",
            "Epoch  31 Iteration  4 Loss:  0.23541051149368286\n",
            "Epoch  31 Iteration  5 Loss:  0.22160033881664276\n",
            "Epoch  31 Iteration  6 Loss:  0.37962427735328674\n",
            "Epoch  31 Iteration  7 Loss:  0.48089003562927246\n",
            "Epoch  31 Iteration  8 Loss:  0.18027052283287048\n",
            "Epoch  31 Iteration  9 Loss:  0.35122764110565186\n",
            "Epoch  31 Iteration  10 Loss:  0.3532204329967499\n",
            "Epoch  31 Iteration  11 Loss:  0.3853210210800171\n",
            "Epoch  31 Iteration  12 Loss:  0.33942121267318726\n",
            "Epoch  31 Iteration  13 Loss:  0.26753032207489014\n",
            "Epoch  31 Iteration  14 Loss:  0.34133943915367126\n",
            "Epoch  31 Iteration  15 Loss:  0.26528576016426086\n",
            "Epoch  31 Iteration  16 Loss:  0.43653246760368347\n",
            "Epoch  31 Iteration  17 Loss:  0.2755095064640045\n",
            "Epoch  31 Iteration  18 Loss:  0.26410162448883057\n",
            "Epoch  31 Iteration  19 Loss:  0.6181941032409668\n",
            "Epoch  31 Iteration  20 Loss:  0.2532294690608978\n",
            "Epoch  31 Iteration  21 Loss:  0.40768852829933167\n",
            "Epoch  31 Iteration  22 Loss:  0.3524371087551117\n",
            "Epoch  31 Iteration  23 Loss:  0.4157823622226715\n",
            "Epoch  31 Iteration  24 Loss:  0.2398124486207962\n",
            "Epoch  31 Iteration  25 Loss:  0.4634632468223572\n",
            "Epoch  31 Iteration  26 Loss:  0.35888275504112244\n",
            "Epoch  31 Iteration  27 Loss:  0.30478134751319885\n",
            "Epoch  31 Iteration  28 Loss:  0.3035758137702942\n",
            "Epoch  31 Iteration  29 Loss:  0.27073603868484497\n",
            "Epoch  31 Iteration  30 Loss:  0.27076077461242676\n",
            "Epoch  31 Iteration  31 Loss:  0.18489991128444672\n",
            "Epoch  31 Iteration  32 Loss:  0.5382766723632812\n",
            "Epoch  31 Iteration  33 Loss:  0.17180883884429932\n",
            "Epoch  31 Iteration  34 Loss:  0.32675307989120483\n",
            "Epoch  31 Iteration  35 Loss:  0.2738606929779053\n",
            "Epoch  31 Iteration  36 Loss:  0.33846327662467957\n",
            "Epoch  31 Iteration  37 Loss:  0.4206247329711914\n",
            "Epoch  31 Iteration  38 Loss:  0.36375892162323\n",
            "Epoch  31 Iteration  39 Loss:  0.3588677644729614\n",
            "Epoch  31 Iteration  40 Loss:  0.4361587464809418\n",
            "Epoch  31 Iteration  41 Loss:  0.5253504514694214\n",
            "Epoch  31 Iteration  42 Loss:  0.1999264806509018\n",
            "Epoch  31 Iteration  43 Loss:  0.382757306098938\n",
            "Epoch  31 Iteration  44 Loss:  0.5603199601173401\n",
            "Epoch  31 Iteration  45 Loss:  0.5710182785987854\n",
            "Epoch  31 Iteration  46 Loss:  0.6391578912734985\n",
            "Epoch  31 Iteration  47 Loss:  0.428960382938385\n",
            "Epoch  31 Iteration  48 Loss:  0.23020413517951965\n",
            "Epoch  31 Iteration  49 Loss:  0.6082007884979248\n",
            "Epoch  31 Iteration  50 Loss:  0.1665782332420349\n",
            "Epoch  31 Iteration  51 Loss:  0.4290669858455658\n",
            "Epoch  31 Iteration  52 Loss:  0.429004043340683\n",
            "Epoch  31 Iteration  53 Loss:  0.17529569566249847\n",
            "Epoch  31 Iteration  54 Loss:  0.48563873767852783\n",
            "Epoch  31 Iteration  55 Loss:  0.3448466658592224\n",
            "Epoch  31 Iteration  56 Loss:  0.41496214270591736\n",
            "Epoch  31 Iteration  57 Loss:  0.11774718761444092\n",
            "Epoch  31 Iteration  58 Loss:  0.3111741244792938\n",
            "Epoch  31 Iteration  59 Loss:  0.23444142937660217\n",
            "Epoch  31 Iteration  60 Loss:  0.4002397656440735\n",
            "Epoch  31 Iteration  61 Loss:  0.3251771926879883\n",
            "Epoch  31 Iteration  62 Loss:  0.3205646574497223\n",
            "Epoch  31 Iteration  63 Loss:  0.3338153064250946\n",
            "Epoch  31 Iteration  64 Loss:  0.2845490872859955\n",
            "Epoch  31 Iteration  65 Loss:  0.36506378650665283\n",
            "*********************************************************************\n",
            "EPOCH  31  ==> Loss:  0.34341716721202387  Time taken ==>  228.03661012649536\n",
            "*********************************************************************\n",
            "Epoch  32 Iteration  0 Loss:  0.6050882339477539\n",
            "Epoch  32 Iteration  1 Loss:  0.30211392045021057\n",
            "Epoch  32 Iteration  2 Loss:  0.3823305368423462\n",
            "Epoch  32 Iteration  3 Loss:  0.27727654576301575\n",
            "Epoch  32 Iteration  4 Loss:  0.3900946080684662\n",
            "Epoch  32 Iteration  5 Loss:  0.14734068512916565\n",
            "Epoch  32 Iteration  6 Loss:  0.31430280208587646\n",
            "Epoch  32 Iteration  7 Loss:  0.13118696212768555\n",
            "Epoch  32 Iteration  8 Loss:  0.3629806637763977\n",
            "Epoch  32 Iteration  9 Loss:  0.44772395491600037\n",
            "Epoch  32 Iteration  10 Loss:  0.28479114174842834\n",
            "Epoch  32 Iteration  11 Loss:  0.24733857810497284\n",
            "Epoch  32 Iteration  12 Loss:  0.2767256498336792\n",
            "Epoch  32 Iteration  13 Loss:  0.324659526348114\n",
            "Epoch  32 Iteration  14 Loss:  0.2158936858177185\n",
            "Epoch  32 Iteration  15 Loss:  0.19734381139278412\n",
            "Epoch  32 Iteration  16 Loss:  0.42091143131256104\n",
            "Epoch  32 Iteration  17 Loss:  0.24787163734436035\n",
            "Epoch  32 Iteration  18 Loss:  0.28952276706695557\n",
            "Epoch  32 Iteration  19 Loss:  0.3344968557357788\n",
            "Epoch  32 Iteration  20 Loss:  0.4428328573703766\n",
            "Epoch  32 Iteration  21 Loss:  0.2921769618988037\n",
            "Epoch  32 Iteration  22 Loss:  0.2363147735595703\n",
            "Epoch  32 Iteration  23 Loss:  0.19988836348056793\n",
            "Epoch  32 Iteration  24 Loss:  0.1070924699306488\n",
            "Epoch  32 Iteration  25 Loss:  0.473336786031723\n",
            "Epoch  32 Iteration  26 Loss:  0.2661135792732239\n",
            "Epoch  32 Iteration  27 Loss:  0.3902911841869354\n",
            "Epoch  32 Iteration  28 Loss:  0.20132198929786682\n",
            "Epoch  32 Iteration  29 Loss:  0.5394328832626343\n",
            "Epoch  32 Iteration  30 Loss:  0.27415862679481506\n",
            "Epoch  32 Iteration  31 Loss:  0.28578320145606995\n",
            "Epoch  32 Iteration  32 Loss:  0.2711949646472931\n",
            "Epoch  32 Iteration  33 Loss:  0.18652230501174927\n",
            "Epoch  32 Iteration  34 Loss:  0.5071499347686768\n",
            "Epoch  32 Iteration  35 Loss:  0.34683677554130554\n",
            "Epoch  32 Iteration  36 Loss:  0.4989100992679596\n",
            "Epoch  32 Iteration  37 Loss:  0.21975383162498474\n",
            "Epoch  32 Iteration  38 Loss:  0.21696829795837402\n",
            "Epoch  32 Iteration  39 Loss:  0.4919029474258423\n",
            "Epoch  32 Iteration  40 Loss:  0.4030897915363312\n",
            "Epoch  32 Iteration  41 Loss:  0.2321823686361313\n",
            "Epoch  32 Iteration  42 Loss:  0.5281452536582947\n",
            "Epoch  32 Iteration  43 Loss:  0.3211671710014343\n",
            "Epoch  32 Iteration  44 Loss:  0.4257127642631531\n",
            "Epoch  32 Iteration  45 Loss:  0.1777602732181549\n",
            "Epoch  32 Iteration  46 Loss:  0.282320499420166\n",
            "Epoch  32 Iteration  47 Loss:  0.31026095151901245\n",
            "Epoch  32 Iteration  48 Loss:  0.2420007586479187\n",
            "Epoch  32 Iteration  49 Loss:  0.20582906901836395\n",
            "Epoch  32 Iteration  50 Loss:  0.5097976326942444\n",
            "Epoch  32 Iteration  51 Loss:  0.28486892580986023\n",
            "Epoch  32 Iteration  52 Loss:  0.3535834550857544\n",
            "Epoch  32 Iteration  53 Loss:  0.17618893086910248\n",
            "Epoch  32 Iteration  54 Loss:  0.20495252311229706\n",
            "Epoch  32 Iteration  55 Loss:  0.415330708026886\n",
            "Epoch  32 Iteration  56 Loss:  0.5298131108283997\n",
            "Epoch  32 Iteration  57 Loss:  0.48336341977119446\n",
            "Epoch  32 Iteration  58 Loss:  0.456856369972229\n",
            "Epoch  32 Iteration  59 Loss:  0.42397215962409973\n",
            "Epoch  32 Iteration  60 Loss:  0.4317571222782135\n",
            "Epoch  32 Iteration  61 Loss:  0.30085140466690063\n",
            "Epoch  32 Iteration  62 Loss:  0.2528631389141083\n",
            "Epoch  32 Iteration  63 Loss:  0.3118271231651306\n",
            "Epoch  32 Iteration  64 Loss:  0.4975663125514984\n",
            "Epoch  32 Iteration  65 Loss:  0.6791858673095703\n",
            "*********************************************************************\n",
            "EPOCH  32  ==> Loss:  0.3346852111545476  Time taken ==>  226.91449737548828\n",
            "*********************************************************************\n",
            "Epoch  33 Iteration  0 Loss:  0.22279351949691772\n",
            "Epoch  33 Iteration  1 Loss:  0.7072404623031616\n",
            "Epoch  33 Iteration  2 Loss:  0.20353694260120392\n",
            "Epoch  33 Iteration  3 Loss:  0.3082256019115448\n",
            "Epoch  33 Iteration  4 Loss:  0.32671457529067993\n",
            "Epoch  33 Iteration  5 Loss:  0.27444881200790405\n",
            "Epoch  33 Iteration  6 Loss:  0.29812726378440857\n",
            "Epoch  33 Iteration  7 Loss:  0.6131722927093506\n",
            "Epoch  33 Iteration  8 Loss:  0.34729722142219543\n",
            "Epoch  33 Iteration  9 Loss:  0.2840524911880493\n",
            "Epoch  33 Iteration  10 Loss:  0.29108619689941406\n",
            "Epoch  33 Iteration  11 Loss:  0.38306963443756104\n",
            "Epoch  33 Iteration  12 Loss:  0.3353976607322693\n",
            "Epoch  33 Iteration  13 Loss:  0.47543373703956604\n",
            "Epoch  33 Iteration  14 Loss:  0.20666858553886414\n",
            "Epoch  33 Iteration  15 Loss:  0.2934124171733856\n",
            "Epoch  33 Iteration  16 Loss:  0.41988256573677063\n",
            "Epoch  33 Iteration  17 Loss:  0.22206619381904602\n",
            "Epoch  33 Iteration  18 Loss:  0.16355235874652863\n",
            "Epoch  33 Iteration  19 Loss:  0.3061295747756958\n",
            "Epoch  33 Iteration  20 Loss:  0.3017124831676483\n",
            "Epoch  33 Iteration  21 Loss:  0.3994847238063812\n",
            "Epoch  33 Iteration  22 Loss:  0.36929795145988464\n",
            "Epoch  33 Iteration  23 Loss:  0.35383838415145874\n",
            "Epoch  33 Iteration  24 Loss:  0.3423921465873718\n",
            "Epoch  33 Iteration  25 Loss:  0.2357674390077591\n",
            "Epoch  33 Iteration  26 Loss:  0.49200379848480225\n",
            "Epoch  33 Iteration  27 Loss:  0.3759637176990509\n",
            "Epoch  33 Iteration  28 Loss:  0.48534733057022095\n",
            "Epoch  33 Iteration  29 Loss:  0.329794704914093\n",
            "Epoch  33 Iteration  30 Loss:  0.4287523925304413\n",
            "Epoch  33 Iteration  31 Loss:  0.239739328622818\n",
            "Epoch  33 Iteration  32 Loss:  0.49634936451911926\n",
            "Epoch  33 Iteration  33 Loss:  0.3751073479652405\n",
            "Epoch  33 Iteration  34 Loss:  0.3408181369304657\n",
            "Epoch  33 Iteration  35 Loss:  0.5123666524887085\n",
            "Epoch  33 Iteration  36 Loss:  0.5602792501449585\n",
            "Epoch  33 Iteration  37 Loss:  0.20613151788711548\n",
            "Epoch  33 Iteration  38 Loss:  0.30953705310821533\n",
            "Epoch  33 Iteration  39 Loss:  0.17983663082122803\n",
            "Epoch  33 Iteration  40 Loss:  0.2705417275428772\n",
            "Epoch  33 Iteration  41 Loss:  0.8120415210723877\n",
            "Epoch  33 Iteration  42 Loss:  0.5215536952018738\n",
            "Epoch  33 Iteration  43 Loss:  0.4132322669029236\n",
            "Epoch  33 Iteration  44 Loss:  0.3104148805141449\n",
            "Epoch  33 Iteration  45 Loss:  0.15255394577980042\n",
            "Epoch  33 Iteration  46 Loss:  0.29964420199394226\n",
            "Epoch  33 Iteration  47 Loss:  0.2495362013578415\n",
            "Epoch  33 Iteration  48 Loss:  0.7473750114440918\n",
            "Epoch  33 Iteration  49 Loss:  0.2621835172176361\n",
            "Epoch  33 Iteration  50 Loss:  0.27712273597717285\n",
            "Epoch  33 Iteration  51 Loss:  0.371151328086853\n",
            "Epoch  33 Iteration  52 Loss:  0.2411602884531021\n",
            "Epoch  33 Iteration  53 Loss:  0.6052908897399902\n",
            "Epoch  33 Iteration  54 Loss:  0.27387213706970215\n",
            "Epoch  33 Iteration  55 Loss:  0.5962108969688416\n",
            "Epoch  33 Iteration  56 Loss:  0.3223380744457245\n",
            "Epoch  33 Iteration  57 Loss:  0.27477478981018066\n",
            "Epoch  33 Iteration  58 Loss:  0.39849022030830383\n",
            "Epoch  33 Iteration  59 Loss:  0.15392975509166718\n",
            "Epoch  33 Iteration  60 Loss:  0.5874677896499634\n",
            "Epoch  33 Iteration  61 Loss:  0.39482998847961426\n",
            "Epoch  33 Iteration  62 Loss:  0.38111767172813416\n",
            "Epoch  33 Iteration  63 Loss:  0.24069274961948395\n",
            "Epoch  33 Iteration  64 Loss:  0.3228912651538849\n",
            "Epoch  33 Iteration  65 Loss:  0.21646595001220703\n",
            "*********************************************************************\n",
            "EPOCH  33  ==> Loss:  0.35929866609248245  Time taken ==>  226.398672580719\n",
            "*********************************************************************\n",
            "Epoch  34 Iteration  0 Loss:  0.25036221742630005\n",
            "Epoch  34 Iteration  1 Loss:  0.4229505956172943\n",
            "Epoch  34 Iteration  2 Loss:  0.45399385690689087\n",
            "Epoch  34 Iteration  3 Loss:  0.19849400222301483\n",
            "Epoch  34 Iteration  4 Loss:  0.2414759397506714\n",
            "Epoch  34 Iteration  5 Loss:  0.31570374965667725\n",
            "Epoch  34 Iteration  6 Loss:  0.3795314133167267\n",
            "Epoch  34 Iteration  7 Loss:  0.2631693482398987\n",
            "Epoch  34 Iteration  8 Loss:  0.3560227155685425\n",
            "Epoch  34 Iteration  9 Loss:  0.23324164748191833\n",
            "Epoch  34 Iteration  10 Loss:  0.34558022022247314\n",
            "Epoch  34 Iteration  11 Loss:  0.44107362627983093\n",
            "Epoch  34 Iteration  12 Loss:  0.471609890460968\n",
            "Epoch  34 Iteration  13 Loss:  0.5086058974266052\n",
            "Epoch  34 Iteration  14 Loss:  0.39911049604415894\n",
            "Epoch  34 Iteration  15 Loss:  0.34770524501800537\n",
            "Epoch  34 Iteration  16 Loss:  0.24882327020168304\n",
            "Epoch  34 Iteration  17 Loss:  0.2531753182411194\n",
            "Epoch  34 Iteration  18 Loss:  0.28980204463005066\n",
            "Epoch  34 Iteration  19 Loss:  0.19143632054328918\n",
            "Epoch  34 Iteration  20 Loss:  0.26721179485321045\n",
            "Epoch  34 Iteration  21 Loss:  0.2299908548593521\n",
            "Epoch  34 Iteration  22 Loss:  0.3997691571712494\n",
            "Epoch  34 Iteration  23 Loss:  0.36191093921661377\n",
            "Epoch  34 Iteration  24 Loss:  0.46096929907798767\n",
            "Epoch  34 Iteration  25 Loss:  0.5888411998748779\n",
            "Epoch  34 Iteration  26 Loss:  0.45524337887763977\n",
            "Epoch  34 Iteration  27 Loss:  0.6049333810806274\n",
            "Epoch  34 Iteration  28 Loss:  0.12419726699590683\n",
            "Epoch  34 Iteration  29 Loss:  0.38100576400756836\n",
            "Epoch  34 Iteration  30 Loss:  0.22814171016216278\n",
            "Epoch  34 Iteration  31 Loss:  0.3972308337688446\n",
            "Epoch  34 Iteration  32 Loss:  0.5540066957473755\n",
            "Epoch  34 Iteration  33 Loss:  0.18757078051567078\n",
            "Epoch  34 Iteration  34 Loss:  0.36517006158828735\n",
            "Epoch  34 Iteration  35 Loss:  0.43973106145858765\n",
            "Epoch  34 Iteration  36 Loss:  0.3777022957801819\n",
            "Epoch  34 Iteration  37 Loss:  0.27901721000671387\n",
            "Epoch  34 Iteration  38 Loss:  0.30321693420410156\n",
            "Epoch  34 Iteration  39 Loss:  0.1722358763217926\n",
            "Epoch  34 Iteration  40 Loss:  0.259325236082077\n",
            "Epoch  34 Iteration  41 Loss:  0.4319334030151367\n",
            "Epoch  34 Iteration  42 Loss:  0.35133206844329834\n",
            "Epoch  34 Iteration  43 Loss:  0.25753188133239746\n",
            "Epoch  34 Iteration  44 Loss:  0.44490286707878113\n",
            "Epoch  34 Iteration  45 Loss:  0.3665977418422699\n",
            "Epoch  34 Iteration  46 Loss:  0.2562849521636963\n",
            "Epoch  34 Iteration  47 Loss:  0.2116425335407257\n",
            "Epoch  34 Iteration  48 Loss:  0.2626960873603821\n",
            "Epoch  34 Iteration  49 Loss:  0.36276453733444214\n",
            "Epoch  34 Iteration  50 Loss:  0.3612310588359833\n",
            "Epoch  34 Iteration  51 Loss:  0.7039620876312256\n",
            "Epoch  34 Iteration  52 Loss:  0.5284000635147095\n",
            "Epoch  34 Iteration  53 Loss:  0.25019770860671997\n",
            "Epoch  34 Iteration  54 Loss:  0.25823700428009033\n",
            "Epoch  34 Iteration  55 Loss:  0.33797961473464966\n",
            "Epoch  34 Iteration  56 Loss:  0.5065542459487915\n",
            "Epoch  34 Iteration  57 Loss:  0.5252155065536499\n",
            "Epoch  34 Iteration  58 Loss:  0.44512709975242615\n",
            "Epoch  34 Iteration  59 Loss:  0.3052559196949005\n",
            "Epoch  34 Iteration  60 Loss:  0.2715933620929718\n",
            "Epoch  34 Iteration  61 Loss:  0.15025335550308228\n",
            "Epoch  34 Iteration  62 Loss:  0.6192408204078674\n",
            "Epoch  34 Iteration  63 Loss:  0.29786550998687744\n",
            "Epoch  34 Iteration  64 Loss:  0.24794067442417145\n",
            "Epoch  34 Iteration  65 Loss:  0.399933785200119\n",
            "*********************************************************************\n",
            "EPOCH  34  ==> Loss:  0.3515752035785805  Time taken ==>  227.4827117919922\n",
            "*********************************************************************\n",
            "Epoch  35 Iteration  0 Loss:  0.34313419461250305\n",
            "Epoch  35 Iteration  1 Loss:  0.366158664226532\n",
            "Epoch  35 Iteration  2 Loss:  0.3789655268192291\n",
            "Epoch  35 Iteration  3 Loss:  0.4065365195274353\n",
            "Epoch  35 Iteration  4 Loss:  0.23983365297317505\n",
            "Epoch  35 Iteration  5 Loss:  0.08679276704788208\n",
            "Epoch  35 Iteration  6 Loss:  0.2824462652206421\n",
            "Epoch  35 Iteration  7 Loss:  0.25286227464675903\n",
            "Epoch  35 Iteration  8 Loss:  0.4340243935585022\n",
            "Epoch  35 Iteration  9 Loss:  0.4458845853805542\n",
            "Epoch  35 Iteration  10 Loss:  0.17700731754302979\n",
            "Epoch  35 Iteration  11 Loss:  0.1751476377248764\n",
            "Epoch  35 Iteration  12 Loss:  0.44620364904403687\n",
            "Epoch  35 Iteration  13 Loss:  0.2391568124294281\n",
            "Epoch  35 Iteration  14 Loss:  0.3729479908943176\n",
            "Epoch  35 Iteration  15 Loss:  0.21956099569797516\n",
            "Epoch  35 Iteration  16 Loss:  0.22585968673229218\n",
            "Epoch  35 Iteration  17 Loss:  0.19446109235286713\n",
            "Epoch  35 Iteration  18 Loss:  0.27202847599983215\n",
            "Epoch  35 Iteration  19 Loss:  0.20457246899604797\n",
            "Epoch  35 Iteration  20 Loss:  0.49768123030662537\n",
            "Epoch  35 Iteration  21 Loss:  0.3156959116458893\n",
            "Epoch  35 Iteration  22 Loss:  0.4513283371925354\n",
            "Epoch  35 Iteration  23 Loss:  0.2734794318675995\n",
            "Epoch  35 Iteration  24 Loss:  0.44314485788345337\n",
            "Epoch  35 Iteration  25 Loss:  0.23952464759349823\n",
            "Epoch  35 Iteration  26 Loss:  0.3912941813468933\n",
            "Epoch  35 Iteration  27 Loss:  0.36398595571517944\n",
            "Epoch  35 Iteration  28 Loss:  0.10418516397476196\n",
            "Epoch  35 Iteration  29 Loss:  0.1798786073923111\n",
            "Epoch  35 Iteration  30 Loss:  0.27183079719543457\n",
            "Epoch  35 Iteration  31 Loss:  0.3912702798843384\n",
            "Epoch  35 Iteration  32 Loss:  0.20451809465885162\n",
            "Epoch  35 Iteration  33 Loss:  0.2535907030105591\n",
            "Epoch  35 Iteration  34 Loss:  0.1406194567680359\n",
            "Epoch  35 Iteration  35 Loss:  0.23755699396133423\n",
            "Epoch  35 Iteration  36 Loss:  0.26701733469963074\n",
            "Epoch  35 Iteration  37 Loss:  0.09891469776630402\n",
            "Epoch  35 Iteration  38 Loss:  0.26600298285484314\n",
            "Epoch  35 Iteration  39 Loss:  0.24562709033489227\n",
            "Epoch  35 Iteration  40 Loss:  0.4523531496524811\n",
            "Epoch  35 Iteration  41 Loss:  0.5299147367477417\n",
            "Epoch  35 Iteration  42 Loss:  0.48487621545791626\n",
            "Epoch  35 Iteration  43 Loss:  0.1129055991768837\n",
            "Epoch  35 Iteration  44 Loss:  0.5752724409103394\n",
            "Epoch  35 Iteration  45 Loss:  0.22514718770980835\n",
            "Epoch  35 Iteration  46 Loss:  0.2761074900627136\n",
            "Epoch  35 Iteration  47 Loss:  0.37794530391693115\n",
            "Epoch  35 Iteration  48 Loss:  0.403311550617218\n",
            "Epoch  35 Iteration  49 Loss:  0.32582274079322815\n",
            "Epoch  35 Iteration  50 Loss:  0.3788198232650757\n",
            "Epoch  35 Iteration  51 Loss:  0.3748984932899475\n",
            "Epoch  35 Iteration  52 Loss:  0.5861270427703857\n",
            "Epoch  35 Iteration  53 Loss:  0.34489578008651733\n",
            "Epoch  35 Iteration  54 Loss:  0.24078422784805298\n",
            "Epoch  35 Iteration  55 Loss:  0.34573420882225037\n",
            "Epoch  35 Iteration  56 Loss:  0.1915724128484726\n",
            "Epoch  35 Iteration  57 Loss:  0.3793575167655945\n",
            "Epoch  35 Iteration  58 Loss:  0.2675444185733795\n",
            "Epoch  35 Iteration  59 Loss:  0.26698821783065796\n",
            "Epoch  35 Iteration  60 Loss:  0.3963479995727539\n",
            "Epoch  35 Iteration  61 Loss:  0.40751564502716064\n",
            "Epoch  35 Iteration  62 Loss:  0.22833098471164703\n",
            "Epoch  35 Iteration  63 Loss:  0.34814372658729553\n",
            "Epoch  35 Iteration  64 Loss:  0.27405238151550293\n",
            "Epoch  35 Iteration  65 Loss:  0.12086207419633865\n",
            "*********************************************************************\n",
            "EPOCH  35  ==> Loss:  0.3078236832763209  Time taken ==>  228.40175580978394\n",
            "*********************************************************************\n",
            "Epoch  36 Iteration  0 Loss:  0.2579043507575989\n",
            "Epoch  36 Iteration  1 Loss:  0.2177727073431015\n",
            "Epoch  36 Iteration  2 Loss:  0.5756165385246277\n",
            "Epoch  36 Iteration  3 Loss:  0.6253388524055481\n",
            "Epoch  36 Iteration  4 Loss:  0.17026127874851227\n",
            "Epoch  36 Iteration  5 Loss:  0.21190322935581207\n",
            "Epoch  36 Iteration  6 Loss:  0.5749796032905579\n",
            "Epoch  36 Iteration  7 Loss:  0.4237210750579834\n",
            "Epoch  36 Iteration  8 Loss:  0.39226412773132324\n",
            "Epoch  36 Iteration  9 Loss:  0.29639771580696106\n",
            "Epoch  36 Iteration  10 Loss:  0.3269387483596802\n",
            "Epoch  36 Iteration  11 Loss:  0.37230396270751953\n",
            "Epoch  36 Iteration  12 Loss:  0.2752087712287903\n",
            "Epoch  36 Iteration  13 Loss:  0.24699975550174713\n",
            "Epoch  36 Iteration  14 Loss:  0.35598114132881165\n",
            "Epoch  36 Iteration  15 Loss:  0.17971061170101166\n",
            "Epoch  36 Iteration  16 Loss:  0.4426541030406952\n",
            "Epoch  36 Iteration  17 Loss:  0.21681302785873413\n",
            "Epoch  36 Iteration  18 Loss:  0.3440796136856079\n",
            "Epoch  36 Iteration  19 Loss:  0.5673103928565979\n",
            "Epoch  36 Iteration  20 Loss:  0.17312294244766235\n",
            "Epoch  36 Iteration  21 Loss:  0.301511287689209\n",
            "Epoch  36 Iteration  22 Loss:  0.5139603614807129\n",
            "Epoch  36 Iteration  23 Loss:  0.23908986151218414\n",
            "Epoch  36 Iteration  24 Loss:  0.28863516449928284\n",
            "Epoch  36 Iteration  25 Loss:  0.38988161087036133\n",
            "Epoch  36 Iteration  26 Loss:  0.6102691292762756\n",
            "Epoch  36 Iteration  27 Loss:  0.22731558978557587\n",
            "Epoch  36 Iteration  28 Loss:  0.663150429725647\n",
            "Epoch  36 Iteration  29 Loss:  0.22562813758850098\n",
            "Epoch  36 Iteration  30 Loss:  0.27355560660362244\n",
            "Epoch  36 Iteration  31 Loss:  0.3831543028354645\n",
            "Epoch  36 Iteration  32 Loss:  0.45524388551712036\n",
            "Epoch  36 Iteration  33 Loss:  0.2398301661014557\n",
            "Epoch  36 Iteration  34 Loss:  0.27402669191360474\n",
            "Epoch  36 Iteration  35 Loss:  0.17563527822494507\n",
            "Epoch  36 Iteration  36 Loss:  0.3873006999492645\n",
            "Epoch  36 Iteration  37 Loss:  0.12266919016838074\n",
            "Epoch  36 Iteration  38 Loss:  0.14585590362548828\n",
            "Epoch  36 Iteration  39 Loss:  0.38811194896698\n",
            "Epoch  36 Iteration  40 Loss:  0.20373521745204926\n",
            "Epoch  36 Iteration  41 Loss:  0.3922794461250305\n",
            "Epoch  36 Iteration  42 Loss:  0.2795029580593109\n",
            "Epoch  36 Iteration  43 Loss:  0.13841305673122406\n",
            "Epoch  36 Iteration  44 Loss:  0.21851278841495514\n",
            "Epoch  36 Iteration  45 Loss:  0.28405746817588806\n",
            "Epoch  36 Iteration  46 Loss:  0.5504915714263916\n",
            "Epoch  36 Iteration  47 Loss:  0.2600836753845215\n",
            "Epoch  36 Iteration  48 Loss:  0.5652748346328735\n",
            "Epoch  36 Iteration  49 Loss:  0.36794978380203247\n",
            "Epoch  36 Iteration  50 Loss:  0.1327260136604309\n",
            "Epoch  36 Iteration  51 Loss:  0.12417047470808029\n",
            "Epoch  36 Iteration  52 Loss:  0.30008813738822937\n",
            "Epoch  36 Iteration  53 Loss:  0.47988128662109375\n",
            "Epoch  36 Iteration  54 Loss:  0.25140833854675293\n",
            "Epoch  36 Iteration  55 Loss:  0.16702978312969208\n",
            "Epoch  36 Iteration  56 Loss:  0.37184521555900574\n",
            "Epoch  36 Iteration  57 Loss:  0.2552809715270996\n",
            "Epoch  36 Iteration  58 Loss:  0.15431027114391327\n",
            "Epoch  36 Iteration  59 Loss:  0.16206860542297363\n",
            "Epoch  36 Iteration  60 Loss:  0.3563382029533386\n",
            "Epoch  36 Iteration  61 Loss:  0.29842332005500793\n",
            "Epoch  36 Iteration  62 Loss:  0.348347544670105\n",
            "Epoch  36 Iteration  63 Loss:  0.47372329235076904\n",
            "Epoch  36 Iteration  64 Loss:  0.46096083521842957\n",
            "Epoch  36 Iteration  65 Loss:  0.0942535325884819\n",
            "*********************************************************************\n",
            "EPOCH  36  ==> Loss:  0.3218676427548582  Time taken ==>  228.32627391815186\n",
            "*********************************************************************\n",
            "Epoch  37 Iteration  0 Loss:  0.5118446350097656\n",
            "Epoch  37 Iteration  1 Loss:  0.3367047905921936\n",
            "Epoch  37 Iteration  2 Loss:  0.44621604681015015\n",
            "Epoch  37 Iteration  3 Loss:  0.3113459646701813\n",
            "Epoch  37 Iteration  4 Loss:  0.6026076674461365\n",
            "Epoch  37 Iteration  5 Loss:  0.22740021347999573\n",
            "Epoch  37 Iteration  6 Loss:  0.29395613074302673\n",
            "Epoch  37 Iteration  7 Loss:  0.47727763652801514\n",
            "Epoch  37 Iteration  8 Loss:  0.4579729437828064\n",
            "Epoch  37 Iteration  9 Loss:  0.5069706439971924\n",
            "Epoch  37 Iteration  10 Loss:  0.11021208018064499\n",
            "Epoch  37 Iteration  11 Loss:  0.16818468272686005\n",
            "Epoch  37 Iteration  12 Loss:  0.28624534606933594\n",
            "Epoch  37 Iteration  13 Loss:  0.199631467461586\n",
            "Epoch  37 Iteration  14 Loss:  0.18185913562774658\n",
            "Epoch  37 Iteration  15 Loss:  0.36115244030952454\n",
            "Epoch  37 Iteration  16 Loss:  0.3185690939426422\n",
            "Epoch  37 Iteration  17 Loss:  0.3925495743751526\n",
            "Epoch  37 Iteration  18 Loss:  0.36837220191955566\n",
            "Epoch  37 Iteration  19 Loss:  0.10262458771467209\n",
            "Epoch  37 Iteration  20 Loss:  0.2332880049943924\n",
            "Epoch  37 Iteration  21 Loss:  0.2654356360435486\n",
            "Epoch  37 Iteration  22 Loss:  0.2017872929573059\n",
            "Epoch  37 Iteration  23 Loss:  0.39834803342819214\n",
            "Epoch  37 Iteration  24 Loss:  0.1967713087797165\n",
            "Epoch  37 Iteration  25 Loss:  0.4922027587890625\n",
            "Epoch  37 Iteration  26 Loss:  0.7146718502044678\n",
            "Epoch  37 Iteration  27 Loss:  0.35980311036109924\n",
            "Epoch  37 Iteration  28 Loss:  0.2605128586292267\n",
            "Epoch  37 Iteration  29 Loss:  0.4020666778087616\n",
            "Epoch  37 Iteration  30 Loss:  0.27958381175994873\n",
            "Epoch  37 Iteration  31 Loss:  0.10500527918338776\n",
            "Epoch  37 Iteration  32 Loss:  0.1562296748161316\n",
            "Epoch  37 Iteration  33 Loss:  0.22273264825344086\n",
            "Epoch  37 Iteration  34 Loss:  0.5129765868186951\n",
            "Epoch  37 Iteration  35 Loss:  0.35702580213546753\n",
            "Epoch  37 Iteration  36 Loss:  0.32457590103149414\n",
            "Epoch  37 Iteration  37 Loss:  0.22678008675575256\n",
            "Epoch  37 Iteration  38 Loss:  0.2006535828113556\n",
            "Epoch  37 Iteration  39 Loss:  0.36863404512405396\n",
            "Epoch  37 Iteration  40 Loss:  0.2855895757675171\n",
            "Epoch  37 Iteration  41 Loss:  0.4332911968231201\n",
            "Epoch  37 Iteration  42 Loss:  0.24937213957309723\n",
            "Epoch  37 Iteration  43 Loss:  0.3910243511199951\n",
            "Epoch  37 Iteration  44 Loss:  0.6002313494682312\n",
            "Epoch  37 Iteration  45 Loss:  0.20851469039916992\n",
            "Epoch  37 Iteration  46 Loss:  0.18602636456489563\n",
            "Epoch  37 Iteration  47 Loss:  0.38572824001312256\n",
            "Epoch  37 Iteration  48 Loss:  0.13391870260238647\n",
            "Epoch  37 Iteration  49 Loss:  0.43662285804748535\n",
            "Epoch  37 Iteration  50 Loss:  0.23104555904865265\n",
            "Epoch  37 Iteration  51 Loss:  0.13602378964424133\n",
            "Epoch  37 Iteration  52 Loss:  0.34020525217056274\n",
            "Epoch  37 Iteration  53 Loss:  0.3627215623855591\n",
            "Epoch  37 Iteration  54 Loss:  0.48156875371932983\n",
            "Epoch  37 Iteration  55 Loss:  0.11722388863563538\n",
            "Epoch  37 Iteration  56 Loss:  0.3335309624671936\n",
            "Epoch  37 Iteration  57 Loss:  0.21302135288715363\n",
            "Epoch  37 Iteration  58 Loss:  0.3735934793949127\n",
            "Epoch  37 Iteration  59 Loss:  0.26628342270851135\n",
            "Epoch  37 Iteration  60 Loss:  0.49864819645881653\n",
            "Epoch  37 Iteration  61 Loss:  0.15809237957000732\n",
            "Epoch  37 Iteration  62 Loss:  0.30445191264152527\n",
            "Epoch  37 Iteration  63 Loss:  0.21043868362903595\n",
            "Epoch  37 Iteration  64 Loss:  0.38377079367637634\n",
            "Epoch  37 Iteration  65 Loss:  0.20489074289798737\n",
            "*********************************************************************\n",
            "EPOCH  37  ==> Loss:  0.31616079446041223  Time taken ==>  227.7053084373474\n",
            "*********************************************************************\n",
            "Epoch  38 Iteration  0 Loss:  0.39686936140060425\n",
            "Epoch  38 Iteration  1 Loss:  0.4129662811756134\n",
            "Epoch  38 Iteration  2 Loss:  0.3349685072898865\n",
            "Epoch  38 Iteration  3 Loss:  0.13338033854961395\n",
            "Epoch  38 Iteration  4 Loss:  0.2719307839870453\n",
            "Epoch  38 Iteration  5 Loss:  0.4100337028503418\n",
            "Epoch  38 Iteration  6 Loss:  0.18745267391204834\n",
            "Epoch  38 Iteration  7 Loss:  0.18229448795318604\n",
            "Epoch  38 Iteration  8 Loss:  0.1696944236755371\n",
            "Epoch  38 Iteration  9 Loss:  0.2639451324939728\n",
            "Epoch  38 Iteration  10 Loss:  0.16567756235599518\n",
            "Epoch  38 Iteration  11 Loss:  0.46374261379241943\n",
            "Epoch  38 Iteration  12 Loss:  0.20005729794502258\n",
            "Epoch  38 Iteration  13 Loss:  0.1472330242395401\n",
            "Epoch  38 Iteration  14 Loss:  0.25031355023384094\n",
            "Epoch  38 Iteration  15 Loss:  0.4204693138599396\n",
            "Epoch  38 Iteration  16 Loss:  0.33737078309059143\n",
            "Epoch  38 Iteration  17 Loss:  0.6525553464889526\n",
            "Epoch  38 Iteration  18 Loss:  0.4381376802921295\n",
            "Epoch  38 Iteration  19 Loss:  0.19438761472702026\n",
            "Epoch  38 Iteration  20 Loss:  0.2029164731502533\n",
            "Epoch  38 Iteration  21 Loss:  0.2449500858783722\n",
            "Epoch  38 Iteration  22 Loss:  0.31074053049087524\n",
            "Epoch  38 Iteration  23 Loss:  0.3204197287559509\n",
            "Epoch  38 Iteration  24 Loss:  0.22709763050079346\n",
            "Epoch  38 Iteration  25 Loss:  0.05988012254238129\n",
            "Epoch  38 Iteration  26 Loss:  0.7590039968490601\n",
            "Epoch  38 Iteration  27 Loss:  0.1946241706609726\n",
            "Epoch  38 Iteration  28 Loss:  0.44473937153816223\n",
            "Epoch  38 Iteration  29 Loss:  0.35741618275642395\n",
            "Epoch  38 Iteration  30 Loss:  0.3575643301010132\n",
            "Epoch  38 Iteration  31 Loss:  0.1795295923948288\n",
            "Epoch  38 Iteration  32 Loss:  0.2530898451805115\n",
            "Epoch  38 Iteration  33 Loss:  0.4585326015949249\n",
            "Epoch  38 Iteration  34 Loss:  0.20682524144649506\n",
            "Epoch  38 Iteration  35 Loss:  0.17511345446109772\n",
            "Epoch  38 Iteration  36 Loss:  0.14040294289588928\n",
            "Epoch  38 Iteration  37 Loss:  0.26216474175453186\n",
            "Epoch  38 Iteration  38 Loss:  0.3558039367198944\n",
            "Epoch  38 Iteration  39 Loss:  0.17052677273750305\n",
            "Epoch  38 Iteration  40 Loss:  0.40315377712249756\n",
            "Epoch  38 Iteration  41 Loss:  0.12221485376358032\n",
            "Epoch  38 Iteration  42 Loss:  0.233771413564682\n",
            "Epoch  38 Iteration  43 Loss:  0.4241020381450653\n",
            "Epoch  38 Iteration  44 Loss:  0.4752671420574188\n",
            "Epoch  38 Iteration  45 Loss:  0.627082109451294\n",
            "Epoch  38 Iteration  46 Loss:  0.178879976272583\n",
            "Epoch  38 Iteration  47 Loss:  0.1443791687488556\n",
            "Epoch  38 Iteration  48 Loss:  0.5293091535568237\n",
            "Epoch  38 Iteration  49 Loss:  0.38619890809059143\n",
            "Epoch  38 Iteration  50 Loss:  0.43529653549194336\n",
            "Epoch  38 Iteration  51 Loss:  0.3428291976451874\n",
            "Epoch  38 Iteration  52 Loss:  0.5083907842636108\n",
            "Epoch  38 Iteration  53 Loss:  0.3905683159828186\n",
            "Epoch  38 Iteration  54 Loss:  0.4432557225227356\n",
            "Epoch  38 Iteration  55 Loss:  0.6561851501464844\n",
            "Epoch  38 Iteration  56 Loss:  0.4001891613006592\n",
            "Epoch  38 Iteration  57 Loss:  0.11822570115327835\n",
            "Epoch  38 Iteration  58 Loss:  0.256106436252594\n",
            "Epoch  38 Iteration  59 Loss:  0.3365083336830139\n",
            "Epoch  38 Iteration  60 Loss:  0.21112191677093506\n",
            "Epoch  38 Iteration  61 Loss:  0.3879162669181824\n",
            "Epoch  38 Iteration  62 Loss:  0.3995024859905243\n",
            "Epoch  38 Iteration  63 Loss:  0.3850071132183075\n",
            "Epoch  38 Iteration  64 Loss:  0.18137021362781525\n",
            "Epoch  38 Iteration  65 Loss:  0.37782272696495056\n",
            "*********************************************************************\n",
            "EPOCH  38  ==> Loss:  0.3192344975065101  Time taken ==>  227.43247199058533\n",
            "*********************************************************************\n",
            "Epoch  39 Iteration  0 Loss:  0.43630772829055786\n",
            "Epoch  39 Iteration  1 Loss:  0.20017382502555847\n",
            "Epoch  39 Iteration  2 Loss:  0.2855917811393738\n",
            "Epoch  39 Iteration  3 Loss:  0.32449477910995483\n",
            "Epoch  39 Iteration  4 Loss:  0.30289211869239807\n",
            "Epoch  39 Iteration  5 Loss:  0.23102959990501404\n",
            "Epoch  39 Iteration  6 Loss:  0.25069427490234375\n",
            "Epoch  39 Iteration  7 Loss:  0.35929661989212036\n",
            "Epoch  39 Iteration  8 Loss:  0.3151581585407257\n",
            "Epoch  39 Iteration  9 Loss:  0.2078874111175537\n",
            "Epoch  39 Iteration  10 Loss:  0.6893795132637024\n",
            "Epoch  39 Iteration  11 Loss:  0.3716999590396881\n",
            "Epoch  39 Iteration  12 Loss:  0.24470119178295135\n",
            "Epoch  39 Iteration  13 Loss:  0.22971965372562408\n",
            "Epoch  39 Iteration  14 Loss:  0.21429532766342163\n",
            "Epoch  39 Iteration  15 Loss:  0.3668026328086853\n",
            "Epoch  39 Iteration  16 Loss:  0.3560541868209839\n",
            "Epoch  39 Iteration  17 Loss:  0.4047490358352661\n",
            "Epoch  39 Iteration  18 Loss:  0.5705812573432922\n",
            "Epoch  39 Iteration  19 Loss:  0.3451792299747467\n",
            "Epoch  39 Iteration  20 Loss:  0.5619364976882935\n",
            "Epoch  39 Iteration  21 Loss:  0.40452712774276733\n",
            "Epoch  39 Iteration  22 Loss:  0.38521987199783325\n",
            "Epoch  39 Iteration  23 Loss:  0.3254619240760803\n",
            "Epoch  39 Iteration  24 Loss:  0.325517863035202\n",
            "Epoch  39 Iteration  25 Loss:  0.3877708315849304\n",
            "Epoch  39 Iteration  26 Loss:  0.7933694124221802\n",
            "Epoch  39 Iteration  27 Loss:  0.5622178316116333\n",
            "Epoch  39 Iteration  28 Loss:  0.248808354139328\n",
            "Epoch  39 Iteration  29 Loss:  0.13412699103355408\n",
            "Epoch  39 Iteration  30 Loss:  0.6980180740356445\n",
            "Epoch  39 Iteration  31 Loss:  0.4071806073188782\n",
            "Epoch  39 Iteration  32 Loss:  0.17675337195396423\n",
            "Epoch  39 Iteration  33 Loss:  0.14605648815631866\n",
            "Epoch  39 Iteration  34 Loss:  0.48531967401504517\n",
            "Epoch  39 Iteration  35 Loss:  0.2783719301223755\n",
            "Epoch  39 Iteration  36 Loss:  0.33384019136428833\n",
            "Epoch  39 Iteration  37 Loss:  0.38900285959243774\n",
            "Epoch  39 Iteration  38 Loss:  0.4063114821910858\n",
            "Epoch  39 Iteration  39 Loss:  0.6683801412582397\n",
            "Epoch  39 Iteration  40 Loss:  0.2676263153553009\n",
            "Epoch  39 Iteration  41 Loss:  0.32775604724884033\n",
            "Epoch  39 Iteration  42 Loss:  0.4498339295387268\n",
            "Epoch  39 Iteration  43 Loss:  0.4274333119392395\n",
            "Epoch  39 Iteration  44 Loss:  0.23326244950294495\n",
            "Epoch  39 Iteration  45 Loss:  0.13589386641979218\n",
            "Epoch  39 Iteration  46 Loss:  0.26875659823417664\n",
            "Epoch  39 Iteration  47 Loss:  0.516512393951416\n",
            "Epoch  39 Iteration  48 Loss:  0.17910538613796234\n",
            "Epoch  39 Iteration  49 Loss:  0.42280763387680054\n",
            "Epoch  39 Iteration  50 Loss:  0.22554819285869598\n",
            "Epoch  39 Iteration  51 Loss:  0.5224895477294922\n",
            "Epoch  39 Iteration  52 Loss:  0.1329181045293808\n",
            "Epoch  39 Iteration  53 Loss:  0.2956054210662842\n",
            "Epoch  39 Iteration  54 Loss:  0.34525230526924133\n",
            "Epoch  39 Iteration  55 Loss:  0.3644940257072449\n",
            "Epoch  39 Iteration  56 Loss:  0.14164194464683533\n",
            "Epoch  39 Iteration  57 Loss:  0.09515883773565292\n",
            "Epoch  39 Iteration  58 Loss:  0.09224390983581543\n",
            "Epoch  39 Iteration  59 Loss:  0.2794268727302551\n",
            "Epoch  39 Iteration  60 Loss:  0.1413009762763977\n",
            "Epoch  39 Iteration  61 Loss:  0.4318642020225525\n",
            "Epoch  39 Iteration  62 Loss:  0.476489782333374\n",
            "Epoch  39 Iteration  63 Loss:  0.507343053817749\n",
            "Epoch  39 Iteration  64 Loss:  0.36313939094543457\n",
            "Epoch  39 Iteration  65 Loss:  0.4281211197376251\n",
            "*********************************************************************\n",
            "EPOCH  39  ==> Loss:  0.34692280954032234  Time taken ==>  228.46794080734253\n",
            "*********************************************************************\n",
            "Epoch  40 Iteration  0 Loss:  0.32566314935684204\n",
            "Epoch  40 Iteration  1 Loss:  0.4037443995475769\n",
            "Epoch  40 Iteration  2 Loss:  0.44284504652023315\n",
            "Epoch  40 Iteration  3 Loss:  0.29129666090011597\n",
            "Epoch  40 Iteration  4 Loss:  0.3933318853378296\n",
            "Epoch  40 Iteration  5 Loss:  0.3549375534057617\n",
            "Epoch  40 Iteration  6 Loss:  0.40931129455566406\n",
            "Epoch  40 Iteration  7 Loss:  0.20814339816570282\n",
            "Epoch  40 Iteration  8 Loss:  0.46728673577308655\n",
            "Epoch  40 Iteration  9 Loss:  0.38765403628349304\n",
            "Epoch  40 Iteration  10 Loss:  0.30798134207725525\n",
            "Epoch  40 Iteration  11 Loss:  0.2347477823495865\n",
            "Epoch  40 Iteration  12 Loss:  0.16473126411437988\n",
            "Epoch  40 Iteration  13 Loss:  0.3997740149497986\n",
            "Epoch  40 Iteration  14 Loss:  0.30314120650291443\n",
            "Epoch  40 Iteration  15 Loss:  0.553627073764801\n",
            "Epoch  40 Iteration  16 Loss:  0.1704781949520111\n",
            "Epoch  40 Iteration  17 Loss:  0.29936978220939636\n",
            "Epoch  40 Iteration  18 Loss:  0.3505457043647766\n",
            "Epoch  40 Iteration  19 Loss:  0.5791370868682861\n",
            "Epoch  40 Iteration  20 Loss:  0.22667253017425537\n",
            "Epoch  40 Iteration  21 Loss:  0.35368409752845764\n",
            "Epoch  40 Iteration  22 Loss:  0.3035067021846771\n",
            "Epoch  40 Iteration  23 Loss:  0.33330750465393066\n",
            "Epoch  40 Iteration  24 Loss:  0.38744187355041504\n",
            "Epoch  40 Iteration  25 Loss:  0.2951088845729828\n",
            "Epoch  40 Iteration  26 Loss:  0.39483779668807983\n",
            "Epoch  40 Iteration  27 Loss:  0.3060581088066101\n",
            "Epoch  40 Iteration  28 Loss:  0.3969368636608124\n",
            "Epoch  40 Iteration  29 Loss:  0.2898484468460083\n",
            "Epoch  40 Iteration  30 Loss:  0.422137975692749\n",
            "Epoch  40 Iteration  31 Loss:  0.3565816283226013\n",
            "Epoch  40 Iteration  32 Loss:  0.5306769609451294\n",
            "Epoch  40 Iteration  33 Loss:  0.2643152177333832\n",
            "Epoch  40 Iteration  34 Loss:  0.06668582558631897\n",
            "Epoch  40 Iteration  35 Loss:  0.43288010358810425\n",
            "Epoch  40 Iteration  36 Loss:  0.26145458221435547\n",
            "Epoch  40 Iteration  37 Loss:  0.4951023757457733\n",
            "Epoch  40 Iteration  38 Loss:  0.349294513463974\n",
            "Epoch  40 Iteration  39 Loss:  0.29977768659591675\n",
            "Epoch  40 Iteration  40 Loss:  0.331543892621994\n",
            "Epoch  40 Iteration  41 Loss:  0.3572578430175781\n",
            "Epoch  40 Iteration  42 Loss:  0.4294772446155548\n",
            "Epoch  40 Iteration  43 Loss:  0.2662968933582306\n",
            "Epoch  40 Iteration  44 Loss:  0.19779500365257263\n",
            "Epoch  40 Iteration  45 Loss:  0.26806530356407166\n",
            "Epoch  40 Iteration  46 Loss:  0.24962173402309418\n",
            "Epoch  40 Iteration  47 Loss:  0.2185158133506775\n",
            "Epoch  40 Iteration  48 Loss:  0.2147601991891861\n",
            "Epoch  40 Iteration  49 Loss:  0.15062792599201202\n",
            "Epoch  40 Iteration  50 Loss:  0.18686942756175995\n",
            "Epoch  40 Iteration  51 Loss:  0.3161090016365051\n",
            "Epoch  40 Iteration  52 Loss:  0.20414233207702637\n",
            "Epoch  40 Iteration  53 Loss:  0.3455314040184021\n",
            "Epoch  40 Iteration  54 Loss:  0.424560010433197\n",
            "Epoch  40 Iteration  55 Loss:  0.2774236500263214\n",
            "Epoch  40 Iteration  56 Loss:  0.29246991872787476\n",
            "Epoch  40 Iteration  57 Loss:  0.10221799463033676\n",
            "Epoch  40 Iteration  58 Loss:  0.08422421663999557\n",
            "Epoch  40 Iteration  59 Loss:  0.28236645460128784\n",
            "Epoch  40 Iteration  60 Loss:  0.07707244902849197\n",
            "Epoch  40 Iteration  61 Loss:  0.3783855438232422\n",
            "Epoch  40 Iteration  62 Loss:  0.3978242576122284\n",
            "Epoch  40 Iteration  63 Loss:  0.27443864941596985\n",
            "Epoch  40 Iteration  64 Loss:  0.48632702231407166\n",
            "Epoch  40 Iteration  65 Loss:  0.162966787815094\n",
            "*********************************************************************\n",
            "EPOCH  40  ==> Loss:  0.31501436764092156  Time taken ==>  228.85582184791565\n",
            "*********************************************************************\n",
            "Epoch  41 Iteration  0 Loss:  0.23280982673168182\n",
            "Epoch  41 Iteration  1 Loss:  0.18326501548290253\n",
            "Epoch  41 Iteration  2 Loss:  0.25576046109199524\n",
            "Epoch  41 Iteration  3 Loss:  0.18605691194534302\n",
            "Epoch  41 Iteration  4 Loss:  0.2269400656223297\n",
            "Epoch  41 Iteration  5 Loss:  0.39215606451034546\n",
            "Epoch  41 Iteration  6 Loss:  0.33035174012184143\n",
            "Epoch  41 Iteration  7 Loss:  0.3755878508090973\n",
            "Epoch  41 Iteration  8 Loss:  0.25743788480758667\n",
            "Epoch  41 Iteration  9 Loss:  0.18116366863250732\n",
            "Epoch  41 Iteration  10 Loss:  0.39099669456481934\n",
            "Epoch  41 Iteration  11 Loss:  0.17072997987270355\n",
            "Epoch  41 Iteration  12 Loss:  0.28911954164505005\n",
            "Epoch  41 Iteration  13 Loss:  0.21385154128074646\n",
            "Epoch  41 Iteration  14 Loss:  0.2257000207901001\n",
            "Epoch  41 Iteration  15 Loss:  0.43603718280792236\n",
            "Epoch  41 Iteration  16 Loss:  0.32237744331359863\n",
            "Epoch  41 Iteration  17 Loss:  0.19354385137557983\n",
            "Epoch  41 Iteration  18 Loss:  0.2041788399219513\n",
            "Epoch  41 Iteration  19 Loss:  0.577667772769928\n",
            "Epoch  41 Iteration  20 Loss:  0.20813220739364624\n",
            "Epoch  41 Iteration  21 Loss:  0.22559352219104767\n",
            "Epoch  41 Iteration  22 Loss:  0.17554453015327454\n",
            "Epoch  41 Iteration  23 Loss:  0.3683067560195923\n",
            "Epoch  41 Iteration  24 Loss:  0.2983602285385132\n",
            "Epoch  41 Iteration  25 Loss:  0.4381340742111206\n",
            "Epoch  41 Iteration  26 Loss:  0.2064308524131775\n",
            "Epoch  41 Iteration  27 Loss:  0.3334488272666931\n",
            "Epoch  41 Iteration  28 Loss:  0.32494834065437317\n",
            "Epoch  41 Iteration  29 Loss:  0.33593207597732544\n",
            "Epoch  41 Iteration  30 Loss:  0.2524152398109436\n",
            "Epoch  41 Iteration  31 Loss:  0.7773984670639038\n",
            "Epoch  41 Iteration  32 Loss:  0.2281467318534851\n",
            "Epoch  41 Iteration  33 Loss:  0.273720920085907\n",
            "Epoch  41 Iteration  34 Loss:  0.2402508556842804\n",
            "Epoch  41 Iteration  35 Loss:  0.3057067096233368\n",
            "Epoch  41 Iteration  36 Loss:  0.19384510815143585\n",
            "Epoch  41 Iteration  37 Loss:  0.5954712629318237\n",
            "Epoch  41 Iteration  38 Loss:  0.39353394508361816\n",
            "Epoch  41 Iteration  39 Loss:  0.4300330877304077\n",
            "Epoch  41 Iteration  40 Loss:  0.33789217472076416\n",
            "Epoch  41 Iteration  41 Loss:  0.19550226628780365\n",
            "Epoch  41 Iteration  42 Loss:  0.25261935591697693\n",
            "Epoch  41 Iteration  43 Loss:  0.3329828977584839\n",
            "Epoch  41 Iteration  44 Loss:  0.2848900854587555\n",
            "Epoch  41 Iteration  45 Loss:  0.23592112958431244\n",
            "Epoch  41 Iteration  46 Loss:  0.37155383825302124\n",
            "Epoch  41 Iteration  47 Loss:  0.32833656668663025\n",
            "Epoch  41 Iteration  48 Loss:  0.4240242838859558\n",
            "Epoch  41 Iteration  49 Loss:  0.33339792490005493\n",
            "Epoch  41 Iteration  50 Loss:  0.3394799530506134\n",
            "Epoch  41 Iteration  51 Loss:  0.2856953740119934\n",
            "Epoch  41 Iteration  52 Loss:  0.18116439878940582\n",
            "Epoch  41 Iteration  53 Loss:  0.22088053822517395\n",
            "Epoch  41 Iteration  54 Loss:  0.28953468799591064\n",
            "Epoch  41 Iteration  55 Loss:  0.5575772523880005\n",
            "Epoch  41 Iteration  56 Loss:  0.35381585359573364\n",
            "Epoch  41 Iteration  57 Loss:  0.25091543793678284\n",
            "Epoch  41 Iteration  58 Loss:  0.33563607931137085\n",
            "Epoch  41 Iteration  59 Loss:  0.4540662169456482\n",
            "Epoch  41 Iteration  60 Loss:  0.21206004917621613\n",
            "Epoch  41 Iteration  61 Loss:  0.31533995270729065\n",
            "Epoch  41 Iteration  62 Loss:  0.2877897024154663\n",
            "Epoch  41 Iteration  63 Loss:  0.24170246720314026\n",
            "Epoch  41 Iteration  64 Loss:  0.48446887731552124\n",
            "Epoch  41 Iteration  65 Loss:  0.40772271156311035\n",
            "*********************************************************************\n",
            "EPOCH  41  ==> Loss:  0.3116069117730314  Time taken ==>  230.95883560180664\n",
            "*********************************************************************\n",
            "Epoch  42 Iteration  0 Loss:  0.33025026321411133\n",
            "Epoch  42 Iteration  1 Loss:  0.22462865710258484\n",
            "Epoch  42 Iteration  2 Loss:  0.22892650961875916\n",
            "Epoch  42 Iteration  3 Loss:  0.35757753252983093\n",
            "Epoch  42 Iteration  4 Loss:  0.1571217179298401\n",
            "Epoch  42 Iteration  5 Loss:  0.3594074845314026\n",
            "Epoch  42 Iteration  6 Loss:  0.1757500320672989\n",
            "Epoch  42 Iteration  7 Loss:  0.300956666469574\n",
            "Epoch  42 Iteration  8 Loss:  0.25344085693359375\n",
            "Epoch  42 Iteration  9 Loss:  0.41454675793647766\n",
            "Epoch  42 Iteration  10 Loss:  0.5640756487846375\n",
            "Epoch  42 Iteration  11 Loss:  0.5129993557929993\n",
            "Epoch  42 Iteration  12 Loss:  0.35043978691101074\n",
            "Epoch  42 Iteration  13 Loss:  0.2863556444644928\n",
            "Epoch  42 Iteration  14 Loss:  0.2306547313928604\n",
            "Epoch  42 Iteration  15 Loss:  0.2596788704395294\n",
            "Epoch  42 Iteration  16 Loss:  0.12219461798667908\n",
            "Epoch  42 Iteration  17 Loss:  0.39162784814834595\n",
            "Epoch  42 Iteration  18 Loss:  0.41644179821014404\n",
            "Epoch  42 Iteration  19 Loss:  0.5071529746055603\n",
            "Epoch  42 Iteration  20 Loss:  0.3536190092563629\n",
            "Epoch  42 Iteration  21 Loss:  0.22023937106132507\n",
            "Epoch  42 Iteration  22 Loss:  0.2073155641555786\n",
            "Epoch  42 Iteration  23 Loss:  0.19743087887763977\n",
            "Epoch  42 Iteration  24 Loss:  0.38667264580726624\n",
            "Epoch  42 Iteration  25 Loss:  0.31261223554611206\n",
            "Epoch  42 Iteration  26 Loss:  0.13355262577533722\n",
            "Epoch  42 Iteration  27 Loss:  0.3491826057434082\n",
            "Epoch  42 Iteration  28 Loss:  0.417551726102829\n",
            "Epoch  42 Iteration  29 Loss:  0.2184728980064392\n",
            "Epoch  42 Iteration  30 Loss:  0.11367738246917725\n",
            "Epoch  42 Iteration  31 Loss:  0.3578924238681793\n",
            "Epoch  42 Iteration  32 Loss:  0.27969664335250854\n",
            "Epoch  42 Iteration  33 Loss:  0.3089483976364136\n",
            "Epoch  42 Iteration  34 Loss:  0.2966097593307495\n",
            "Epoch  42 Iteration  35 Loss:  0.3054332137107849\n",
            "Epoch  42 Iteration  36 Loss:  0.43811219930648804\n",
            "Epoch  42 Iteration  37 Loss:  0.1525050550699234\n",
            "Epoch  42 Iteration  38 Loss:  0.2877442240715027\n",
            "Epoch  42 Iteration  39 Loss:  0.23011182248592377\n",
            "Epoch  42 Iteration  40 Loss:  0.40884390473365784\n",
            "Epoch  42 Iteration  41 Loss:  0.13547541201114655\n",
            "Epoch  42 Iteration  42 Loss:  0.20429345965385437\n",
            "Epoch  42 Iteration  43 Loss:  0.3235231041908264\n",
            "Epoch  42 Iteration  44 Loss:  0.33100977540016174\n",
            "Epoch  42 Iteration  45 Loss:  0.3474622964859009\n",
            "Epoch  42 Iteration  46 Loss:  0.3263801336288452\n",
            "Epoch  42 Iteration  47 Loss:  0.15751980245113373\n",
            "Epoch  42 Iteration  48 Loss:  0.4060743451118469\n",
            "Epoch  42 Iteration  49 Loss:  0.6398370265960693\n",
            "Epoch  42 Iteration  50 Loss:  0.31367647647857666\n",
            "Epoch  42 Iteration  51 Loss:  0.46980828046798706\n",
            "Epoch  42 Iteration  52 Loss:  0.5526902079582214\n",
            "Epoch  42 Iteration  53 Loss:  0.3170599937438965\n",
            "Epoch  42 Iteration  54 Loss:  0.2498243898153305\n",
            "Epoch  42 Iteration  55 Loss:  0.3095460832118988\n",
            "Epoch  42 Iteration  56 Loss:  0.31737300753593445\n",
            "Epoch  42 Iteration  57 Loss:  0.5562824010848999\n",
            "Epoch  42 Iteration  58 Loss:  0.39008671045303345\n",
            "Epoch  42 Iteration  59 Loss:  0.11029093712568283\n",
            "Epoch  42 Iteration  60 Loss:  0.19057391583919525\n",
            "Epoch  42 Iteration  61 Loss:  0.462186723947525\n",
            "Epoch  42 Iteration  62 Loss:  0.2989708185195923\n",
            "Epoch  42 Iteration  63 Loss:  0.3634806275367737\n",
            "Epoch  42 Iteration  64 Loss:  0.09128692001104355\n",
            "Epoch  42 Iteration  65 Loss:  0.3592716455459595\n",
            "*********************************************************************\n",
            "EPOCH  42  ==> Loss:  0.31279449751882843  Time taken ==>  228.50409364700317\n",
            "*********************************************************************\n",
            "Epoch  43 Iteration  0 Loss:  0.3472127914428711\n",
            "Epoch  43 Iteration  1 Loss:  0.25097331404685974\n",
            "Epoch  43 Iteration  2 Loss:  0.5104368329048157\n",
            "Epoch  43 Iteration  3 Loss:  0.24688923358917236\n",
            "Epoch  43 Iteration  4 Loss:  0.3718547821044922\n",
            "Epoch  43 Iteration  5 Loss:  0.23769009113311768\n",
            "Epoch  43 Iteration  6 Loss:  0.48884084820747375\n",
            "Epoch  43 Iteration  7 Loss:  0.45950067043304443\n",
            "Epoch  43 Iteration  8 Loss:  0.40172985196113586\n",
            "Epoch  43 Iteration  9 Loss:  0.3593287467956543\n",
            "Epoch  43 Iteration  10 Loss:  0.284182608127594\n",
            "Epoch  43 Iteration  11 Loss:  0.29053550958633423\n",
            "Epoch  43 Iteration  12 Loss:  0.2442634552717209\n",
            "Epoch  43 Iteration  13 Loss:  0.14497606456279755\n",
            "Epoch  43 Iteration  14 Loss:  0.2820742428302765\n",
            "Epoch  43 Iteration  15 Loss:  0.1691979020833969\n",
            "Epoch  43 Iteration  16 Loss:  0.45692190527915955\n",
            "Epoch  43 Iteration  17 Loss:  0.2375514656305313\n",
            "Epoch  43 Iteration  18 Loss:  0.15654172003269196\n",
            "Epoch  43 Iteration  19 Loss:  0.39266425371170044\n",
            "Epoch  43 Iteration  20 Loss:  0.07346506416797638\n",
            "Epoch  43 Iteration  21 Loss:  0.17356765270233154\n",
            "Epoch  43 Iteration  22 Loss:  0.4257689416408539\n",
            "Epoch  43 Iteration  23 Loss:  0.22166679799556732\n",
            "Epoch  43 Iteration  24 Loss:  0.5077990889549255\n",
            "Epoch  43 Iteration  25 Loss:  0.24974916875362396\n",
            "Epoch  43 Iteration  26 Loss:  0.2923588156700134\n",
            "Epoch  43 Iteration  27 Loss:  0.2488618940114975\n",
            "Epoch  43 Iteration  28 Loss:  0.20189623534679413\n",
            "Epoch  43 Iteration  29 Loss:  0.250544011592865\n",
            "Epoch  43 Iteration  30 Loss:  0.325310081243515\n",
            "Epoch  43 Iteration  31 Loss:  0.4428575038909912\n",
            "Epoch  43 Iteration  32 Loss:  0.27626511454582214\n",
            "Epoch  43 Iteration  33 Loss:  0.37249428033828735\n",
            "Epoch  43 Iteration  34 Loss:  0.23040425777435303\n",
            "Epoch  43 Iteration  35 Loss:  0.28208738565444946\n",
            "Epoch  43 Iteration  36 Loss:  0.2500993609428406\n",
            "Epoch  43 Iteration  37 Loss:  0.28670376539230347\n",
            "Epoch  43 Iteration  38 Loss:  0.20137576758861542\n",
            "Epoch  43 Iteration  39 Loss:  0.14974345266819\n",
            "Epoch  43 Iteration  40 Loss:  0.25780826807022095\n",
            "Epoch  43 Iteration  41 Loss:  0.436496764421463\n",
            "Epoch  43 Iteration  42 Loss:  0.2937782406806946\n",
            "Epoch  43 Iteration  43 Loss:  0.24910935759544373\n",
            "Epoch  43 Iteration  44 Loss:  0.4838287830352783\n",
            "Epoch  43 Iteration  45 Loss:  0.19367218017578125\n",
            "Epoch  43 Iteration  46 Loss:  0.27658993005752563\n",
            "Epoch  43 Iteration  47 Loss:  0.35792773962020874\n",
            "Epoch  43 Iteration  48 Loss:  0.24947670102119446\n",
            "Epoch  43 Iteration  49 Loss:  0.14009524881839752\n",
            "Epoch  43 Iteration  50 Loss:  0.10012349486351013\n",
            "Epoch  43 Iteration  51 Loss:  0.16032356023788452\n",
            "Epoch  43 Iteration  52 Loss:  0.6294904947280884\n",
            "Epoch  43 Iteration  53 Loss:  0.44595563411712646\n",
            "Epoch  43 Iteration  54 Loss:  0.21530362963676453\n",
            "Epoch  43 Iteration  55 Loss:  0.2397526204586029\n",
            "Epoch  43 Iteration  56 Loss:  0.5573061108589172\n",
            "Epoch  43 Iteration  57 Loss:  0.45594602823257446\n",
            "Epoch  43 Iteration  58 Loss:  0.26657602190971375\n",
            "Epoch  43 Iteration  59 Loss:  0.24742268025875092\n",
            "Epoch  43 Iteration  60 Loss:  0.34068459272384644\n",
            "Epoch  43 Iteration  61 Loss:  0.24826142191886902\n",
            "Epoch  43 Iteration  62 Loss:  0.478177547454834\n",
            "Epoch  43 Iteration  63 Loss:  0.48815521597862244\n",
            "Epoch  43 Iteration  64 Loss:  0.33104610443115234\n",
            "Epoch  43 Iteration  65 Loss:  0.37693557143211365\n",
            "*********************************************************************\n",
            "EPOCH  43  ==> Loss:  0.307827710202246  Time taken ==>  230.0322105884552\n",
            "*********************************************************************\n",
            "Epoch  44 Iteration  0 Loss:  0.26114070415496826\n",
            "Epoch  44 Iteration  1 Loss:  0.31229910254478455\n",
            "Epoch  44 Iteration  2 Loss:  0.5099920034408569\n",
            "Epoch  44 Iteration  3 Loss:  0.17412951588630676\n",
            "Epoch  44 Iteration  4 Loss:  0.37313711643218994\n",
            "Epoch  44 Iteration  5 Loss:  0.34169381856918335\n",
            "Epoch  44 Iteration  6 Loss:  0.19904595613479614\n",
            "Epoch  44 Iteration  7 Loss:  0.4020896553993225\n",
            "Epoch  44 Iteration  8 Loss:  0.2951364517211914\n",
            "Epoch  44 Iteration  9 Loss:  0.15136800706386566\n",
            "Epoch  44 Iteration  10 Loss:  0.28042522072792053\n",
            "Epoch  44 Iteration  11 Loss:  0.40889906883239746\n",
            "Epoch  44 Iteration  12 Loss:  0.848090648651123\n",
            "Epoch  44 Iteration  13 Loss:  0.26355358958244324\n",
            "Epoch  44 Iteration  14 Loss:  0.21351918578147888\n",
            "Epoch  44 Iteration  15 Loss:  0.2723829746246338\n",
            "Epoch  44 Iteration  16 Loss:  0.28771716356277466\n",
            "Epoch  44 Iteration  17 Loss:  0.3116299510002136\n",
            "Epoch  44 Iteration  18 Loss:  0.16599015891551971\n",
            "Epoch  44 Iteration  19 Loss:  0.38448673486709595\n",
            "Epoch  44 Iteration  20 Loss:  0.19669218361377716\n",
            "Epoch  44 Iteration  21 Loss:  0.08986740559339523\n",
            "Epoch  44 Iteration  22 Loss:  0.5382543206214905\n",
            "Epoch  44 Iteration  23 Loss:  0.2489611804485321\n",
            "Epoch  44 Iteration  24 Loss:  0.3650661110877991\n",
            "Epoch  44 Iteration  25 Loss:  0.36885273456573486\n",
            "Epoch  44 Iteration  26 Loss:  0.3881187438964844\n",
            "Epoch  44 Iteration  27 Loss:  0.21806345880031586\n",
            "Epoch  44 Iteration  28 Loss:  0.32495376467704773\n",
            "Epoch  44 Iteration  29 Loss:  0.2533745765686035\n",
            "Epoch  44 Iteration  30 Loss:  0.2885003387928009\n",
            "Epoch  44 Iteration  31 Loss:  0.19995704293251038\n",
            "Epoch  44 Iteration  32 Loss:  0.22041989862918854\n",
            "Epoch  44 Iteration  33 Loss:  0.21102602779865265\n",
            "Epoch  44 Iteration  34 Loss:  0.254323810338974\n",
            "Epoch  44 Iteration  35 Loss:  0.3357049524784088\n",
            "Epoch  44 Iteration  36 Loss:  0.1956089735031128\n",
            "Epoch  44 Iteration  37 Loss:  0.3201102018356323\n",
            "Epoch  44 Iteration  38 Loss:  0.11101797223091125\n",
            "Epoch  44 Iteration  39 Loss:  0.3814186155796051\n",
            "Epoch  44 Iteration  40 Loss:  0.09173320978879929\n",
            "Epoch  44 Iteration  41 Loss:  0.3319266736507416\n",
            "Epoch  44 Iteration  42 Loss:  0.5587784647941589\n",
            "Epoch  44 Iteration  43 Loss:  0.14231950044631958\n",
            "Epoch  44 Iteration  44 Loss:  0.2127484381198883\n",
            "Epoch  44 Iteration  45 Loss:  0.3476638197898865\n",
            "Epoch  44 Iteration  46 Loss:  0.12003754824399948\n",
            "Epoch  44 Iteration  47 Loss:  0.31853118538856506\n",
            "Epoch  44 Iteration  48 Loss:  0.12116052210330963\n",
            "Epoch  44 Iteration  49 Loss:  0.30484604835510254\n",
            "Epoch  44 Iteration  50 Loss:  0.1291818767786026\n",
            "Epoch  44 Iteration  51 Loss:  0.17281949520111084\n",
            "Epoch  44 Iteration  52 Loss:  0.23652705550193787\n",
            "Epoch  44 Iteration  53 Loss:  0.22489410638809204\n",
            "Epoch  44 Iteration  54 Loss:  0.10763770341873169\n",
            "Epoch  44 Iteration  55 Loss:  0.47939857840538025\n",
            "Epoch  44 Iteration  56 Loss:  0.2727336883544922\n",
            "Epoch  44 Iteration  57 Loss:  0.2853618264198303\n",
            "Epoch  44 Iteration  58 Loss:  0.18968814611434937\n",
            "Epoch  44 Iteration  59 Loss:  0.25039058923721313\n",
            "Epoch  44 Iteration  60 Loss:  0.20725831389427185\n",
            "Epoch  44 Iteration  61 Loss:  0.7829769849777222\n",
            "Epoch  44 Iteration  62 Loss:  0.5279257297515869\n",
            "Epoch  44 Iteration  63 Loss:  0.36163848638534546\n",
            "Epoch  44 Iteration  64 Loss:  0.47021469473838806\n",
            "Epoch  44 Iteration  65 Loss:  0.30907922983169556\n",
            "*********************************************************************\n",
            "EPOCH  44  ==> Loss:  0.2958256251206904  Time taken ==>  229.00180292129517\n",
            "*********************************************************************\n",
            "Epoch  45 Iteration  0 Loss:  0.22275464236736298\n",
            "Epoch  45 Iteration  1 Loss:  0.4847959578037262\n",
            "Epoch  45 Iteration  2 Loss:  0.3811248540878296\n",
            "Epoch  45 Iteration  3 Loss:  0.19404509663581848\n",
            "Epoch  45 Iteration  4 Loss:  0.13199619948863983\n",
            "Epoch  45 Iteration  5 Loss:  0.3106400668621063\n",
            "Epoch  45 Iteration  6 Loss:  0.5383835434913635\n",
            "Epoch  45 Iteration  7 Loss:  0.2326499968767166\n",
            "Epoch  45 Iteration  8 Loss:  0.2045515775680542\n",
            "Epoch  45 Iteration  9 Loss:  0.4261457920074463\n",
            "Epoch  45 Iteration  10 Loss:  0.26439154148101807\n",
            "Epoch  45 Iteration  11 Loss:  0.3605455458164215\n",
            "Epoch  45 Iteration  12 Loss:  0.3265630006790161\n",
            "Epoch  45 Iteration  13 Loss:  0.255355566740036\n",
            "Epoch  45 Iteration  14 Loss:  0.2271525263786316\n",
            "Epoch  45 Iteration  15 Loss:  0.27193641662597656\n",
            "Epoch  45 Iteration  16 Loss:  0.31536272168159485\n",
            "Epoch  45 Iteration  17 Loss:  0.3153620958328247\n",
            "Epoch  45 Iteration  18 Loss:  0.3025757670402527\n",
            "Epoch  45 Iteration  19 Loss:  0.5427381992340088\n",
            "Epoch  45 Iteration  20 Loss:  0.11684848368167877\n",
            "Epoch  45 Iteration  21 Loss:  0.24324284493923187\n",
            "Epoch  45 Iteration  22 Loss:  0.2743224501609802\n",
            "Epoch  45 Iteration  23 Loss:  0.37155091762542725\n",
            "Epoch  45 Iteration  24 Loss:  0.18510542809963226\n",
            "Epoch  45 Iteration  25 Loss:  0.5967555642127991\n",
            "Epoch  45 Iteration  26 Loss:  0.37413620948791504\n",
            "Epoch  45 Iteration  27 Loss:  0.2732766270637512\n",
            "Epoch  45 Iteration  28 Loss:  0.3194829821586609\n",
            "Epoch  45 Iteration  29 Loss:  0.5653344392776489\n",
            "Epoch  45 Iteration  30 Loss:  0.455105721950531\n",
            "Epoch  45 Iteration  31 Loss:  0.20675437152385712\n",
            "Epoch  45 Iteration  32 Loss:  0.3241947293281555\n",
            "Epoch  45 Iteration  33 Loss:  0.39160165190696716\n",
            "Epoch  45 Iteration  34 Loss:  0.11208970099687576\n",
            "Epoch  45 Iteration  35 Loss:  0.25249776244163513\n",
            "Epoch  45 Iteration  36 Loss:  0.33294349908828735\n",
            "Epoch  45 Iteration  37 Loss:  0.1746223121881485\n",
            "Epoch  45 Iteration  38 Loss:  0.609581708908081\n",
            "Epoch  45 Iteration  39 Loss:  0.5651362538337708\n",
            "Epoch  45 Iteration  40 Loss:  0.6217213869094849\n",
            "Epoch  45 Iteration  41 Loss:  0.3915233016014099\n",
            "Epoch  45 Iteration  42 Loss:  0.1243632584810257\n",
            "Epoch  45 Iteration  43 Loss:  0.41077351570129395\n",
            "Epoch  45 Iteration  44 Loss:  0.21240942180156708\n",
            "Epoch  45 Iteration  45 Loss:  0.3333163261413574\n",
            "Epoch  45 Iteration  46 Loss:  0.2804514169692993\n",
            "Epoch  45 Iteration  47 Loss:  0.20461630821228027\n",
            "Epoch  45 Iteration  48 Loss:  0.232513427734375\n",
            "Epoch  45 Iteration  49 Loss:  0.29265525937080383\n",
            "Epoch  45 Iteration  50 Loss:  0.21312548220157623\n",
            "Epoch  45 Iteration  51 Loss:  0.2933332324028015\n",
            "Epoch  45 Iteration  52 Loss:  0.39599013328552246\n",
            "Epoch  45 Iteration  53 Loss:  0.2270011454820633\n",
            "Epoch  45 Iteration  54 Loss:  0.348149836063385\n",
            "Epoch  45 Iteration  55 Loss:  0.24810485541820526\n",
            "Epoch  45 Iteration  56 Loss:  0.16138514876365662\n",
            "Epoch  45 Iteration  57 Loss:  0.3069726228713989\n",
            "Epoch  45 Iteration  58 Loss:  0.4375905394554138\n",
            "Epoch  45 Iteration  59 Loss:  0.12063726037740707\n",
            "Epoch  45 Iteration  60 Loss:  0.21368922293186188\n",
            "Epoch  45 Iteration  61 Loss:  0.3848823606967926\n",
            "Epoch  45 Iteration  62 Loss:  0.21183574199676514\n",
            "Epoch  45 Iteration  63 Loss:  0.47294968366622925\n",
            "Epoch  45 Iteration  64 Loss:  0.20121772587299347\n",
            "Epoch  45 Iteration  65 Loss:  0.22790944576263428\n",
            "*********************************************************************\n",
            "EPOCH  45  ==> Loss:  0.31246625496582553  Time taken ==>  227.43875765800476\n",
            "*********************************************************************\n",
            "Epoch  46 Iteration  0 Loss:  0.4230801463127136\n",
            "Epoch  46 Iteration  1 Loss:  0.3348759114742279\n",
            "Epoch  46 Iteration  2 Loss:  0.22773566842079163\n",
            "Epoch  46 Iteration  3 Loss:  0.23386012017726898\n",
            "Epoch  46 Iteration  4 Loss:  0.14413854479789734\n",
            "Epoch  46 Iteration  5 Loss:  0.2529183328151703\n",
            "Epoch  46 Iteration  6 Loss:  0.2497268170118332\n",
            "Epoch  46 Iteration  7 Loss:  0.2744068503379822\n",
            "Epoch  46 Iteration  8 Loss:  0.1717730462551117\n",
            "Epoch  46 Iteration  9 Loss:  0.42601168155670166\n",
            "Epoch  46 Iteration  10 Loss:  0.4373805522918701\n",
            "Epoch  46 Iteration  11 Loss:  0.3226124048233032\n",
            "Epoch  46 Iteration  12 Loss:  0.16623488068580627\n",
            "Epoch  46 Iteration  13 Loss:  0.4885549247264862\n",
            "Epoch  46 Iteration  14 Loss:  0.40300965309143066\n",
            "Epoch  46 Iteration  15 Loss:  0.2800905704498291\n",
            "Epoch  46 Iteration  16 Loss:  0.5744398832321167\n",
            "Epoch  46 Iteration  17 Loss:  0.24175187945365906\n",
            "Epoch  46 Iteration  18 Loss:  0.11018827557563782\n",
            "Epoch  46 Iteration  19 Loss:  0.24111047387123108\n",
            "Epoch  46 Iteration  20 Loss:  0.31099435687065125\n",
            "Epoch  46 Iteration  21 Loss:  0.2738184630870819\n",
            "Epoch  46 Iteration  22 Loss:  0.5941195487976074\n",
            "Epoch  46 Iteration  23 Loss:  0.3909554183483124\n",
            "Epoch  46 Iteration  24 Loss:  0.15272866189479828\n",
            "Epoch  46 Iteration  25 Loss:  0.3671477437019348\n",
            "Epoch  46 Iteration  26 Loss:  0.33273983001708984\n",
            "Epoch  46 Iteration  27 Loss:  0.20939376950263977\n",
            "Epoch  46 Iteration  28 Loss:  0.38706550002098083\n",
            "Epoch  46 Iteration  29 Loss:  0.07809756696224213\n",
            "Epoch  46 Iteration  30 Loss:  0.21209734678268433\n",
            "Epoch  46 Iteration  31 Loss:  0.24671699106693268\n",
            "Epoch  46 Iteration  32 Loss:  0.3765870928764343\n",
            "Epoch  46 Iteration  33 Loss:  0.300717294216156\n",
            "Epoch  46 Iteration  34 Loss:  0.1365213394165039\n",
            "Epoch  46 Iteration  35 Loss:  0.24184231460094452\n",
            "Epoch  46 Iteration  36 Loss:  0.5703920722007751\n",
            "Epoch  46 Iteration  37 Loss:  0.11117497086524963\n",
            "Epoch  46 Iteration  38 Loss:  0.34841564297676086\n",
            "Epoch  46 Iteration  39 Loss:  0.2022297978401184\n",
            "Epoch  46 Iteration  40 Loss:  0.20669938623905182\n",
            "Epoch  46 Iteration  41 Loss:  0.30043351650238037\n",
            "Epoch  46 Iteration  42 Loss:  0.16359880566596985\n",
            "Epoch  46 Iteration  43 Loss:  0.42731404304504395\n",
            "Epoch  46 Iteration  44 Loss:  0.2058371752500534\n",
            "Epoch  46 Iteration  45 Loss:  0.24032050371170044\n",
            "Epoch  46 Iteration  46 Loss:  0.3096964657306671\n",
            "Epoch  46 Iteration  47 Loss:  0.3095346689224243\n",
            "Epoch  46 Iteration  48 Loss:  0.3648102283477783\n",
            "Epoch  46 Iteration  49 Loss:  0.2835308313369751\n",
            "Epoch  46 Iteration  50 Loss:  0.23417413234710693\n",
            "Epoch  46 Iteration  51 Loss:  0.22245755791664124\n",
            "Epoch  46 Iteration  52 Loss:  0.23699918389320374\n",
            "Epoch  46 Iteration  53 Loss:  0.1391814649105072\n",
            "Epoch  46 Iteration  54 Loss:  0.23559632897377014\n",
            "Epoch  46 Iteration  55 Loss:  0.2674984633922577\n",
            "Epoch  46 Iteration  56 Loss:  0.1579657644033432\n",
            "Epoch  46 Iteration  57 Loss:  0.3355017304420471\n",
            "Epoch  46 Iteration  58 Loss:  0.4420601725578308\n",
            "Epoch  46 Iteration  59 Loss:  0.16697387397289276\n",
            "Epoch  46 Iteration  60 Loss:  0.5135204792022705\n",
            "Epoch  46 Iteration  61 Loss:  0.30277320742607117\n",
            "Epoch  46 Iteration  62 Loss:  0.41541674733161926\n",
            "Epoch  46 Iteration  63 Loss:  0.17082977294921875\n",
            "Epoch  46 Iteration  64 Loss:  0.14685608446598053\n",
            "Epoch  46 Iteration  65 Loss:  0.780099093914032\n",
            "*********************************************************************\n",
            "EPOCH  46  ==> Loss:  0.2943535760948152  Time taken ==>  227.68711924552917\n",
            "*********************************************************************\n",
            "Epoch  47 Iteration  0 Loss:  0.33370381593704224\n",
            "Epoch  47 Iteration  1 Loss:  0.4514405429363251\n",
            "Epoch  47 Iteration  2 Loss:  0.21586057543754578\n",
            "Epoch  47 Iteration  3 Loss:  0.3667859435081482\n",
            "Epoch  47 Iteration  4 Loss:  0.4885978698730469\n",
            "Epoch  47 Iteration  5 Loss:  0.32718098163604736\n",
            "Epoch  47 Iteration  6 Loss:  0.1698158234357834\n",
            "Epoch  47 Iteration  7 Loss:  0.11794359982013702\n",
            "Epoch  47 Iteration  8 Loss:  0.20064254105091095\n",
            "Epoch  47 Iteration  9 Loss:  0.03112185001373291\n",
            "Epoch  47 Iteration  10 Loss:  0.27665093541145325\n",
            "Epoch  47 Iteration  11 Loss:  0.19983193278312683\n",
            "Epoch  47 Iteration  12 Loss:  0.26941367983818054\n",
            "Epoch  47 Iteration  13 Loss:  0.11372169107198715\n",
            "Epoch  47 Iteration  14 Loss:  0.25674086809158325\n",
            "Epoch  47 Iteration  15 Loss:  0.3838917911052704\n",
            "Epoch  47 Iteration  16 Loss:  0.12453257292509079\n",
            "Epoch  47 Iteration  17 Loss:  0.39234498143196106\n",
            "Epoch  47 Iteration  18 Loss:  0.4289371967315674\n",
            "Epoch  47 Iteration  19 Loss:  0.30754226446151733\n",
            "Epoch  47 Iteration  20 Loss:  0.2491944283246994\n",
            "Epoch  47 Iteration  21 Loss:  0.23558279871940613\n",
            "Epoch  47 Iteration  22 Loss:  0.31621187925338745\n",
            "Epoch  47 Iteration  23 Loss:  0.24561277031898499\n",
            "Epoch  47 Iteration  24 Loss:  0.33253031969070435\n",
            "Epoch  47 Iteration  25 Loss:  0.3270500600337982\n",
            "Epoch  47 Iteration  26 Loss:  0.26224201917648315\n",
            "Epoch  47 Iteration  27 Loss:  0.18339821696281433\n",
            "Epoch  47 Iteration  28 Loss:  0.06864099949598312\n",
            "Epoch  47 Iteration  29 Loss:  0.3712520897388458\n",
            "Epoch  47 Iteration  30 Loss:  0.34290915727615356\n",
            "Epoch  47 Iteration  31 Loss:  0.46241623163223267\n",
            "Epoch  47 Iteration  32 Loss:  0.2902674973011017\n",
            "Epoch  47 Iteration  33 Loss:  0.41824817657470703\n",
            "Epoch  47 Iteration  34 Loss:  0.271348774433136\n",
            "Epoch  47 Iteration  35 Loss:  0.2141873836517334\n",
            "Epoch  47 Iteration  36 Loss:  0.3583466410636902\n",
            "Epoch  47 Iteration  37 Loss:  0.2712780833244324\n",
            "Epoch  47 Iteration  38 Loss:  0.34930646419525146\n",
            "Epoch  47 Iteration  39 Loss:  0.1636311411857605\n",
            "Epoch  47 Iteration  40 Loss:  0.14903657138347626\n",
            "Epoch  47 Iteration  41 Loss:  0.1833045780658722\n",
            "Epoch  47 Iteration  42 Loss:  0.23995471000671387\n",
            "Epoch  47 Iteration  43 Loss:  0.5529770851135254\n",
            "Epoch  47 Iteration  44 Loss:  0.2948259115219116\n",
            "Epoch  47 Iteration  45 Loss:  0.3725399374961853\n",
            "Epoch  47 Iteration  46 Loss:  0.3842364549636841\n",
            "Epoch  47 Iteration  47 Loss:  0.27017784118652344\n",
            "Epoch  47 Iteration  48 Loss:  0.2629014253616333\n",
            "Epoch  47 Iteration  49 Loss:  0.4332241415977478\n",
            "Epoch  47 Iteration  50 Loss:  0.35531705617904663\n",
            "Epoch  47 Iteration  51 Loss:  0.31609219312667847\n",
            "Epoch  47 Iteration  52 Loss:  0.4488350749015808\n",
            "Epoch  47 Iteration  53 Loss:  0.22479064762592316\n",
            "Epoch  47 Iteration  54 Loss:  0.30761343240737915\n",
            "Epoch  47 Iteration  55 Loss:  0.3245587646961212\n",
            "Epoch  47 Iteration  56 Loss:  0.33332279324531555\n",
            "Epoch  47 Iteration  57 Loss:  0.24274735152721405\n",
            "Epoch  47 Iteration  58 Loss:  0.2435852289199829\n",
            "Epoch  47 Iteration  59 Loss:  0.4218192994594574\n",
            "Epoch  47 Iteration  60 Loss:  0.2635726034641266\n",
            "Epoch  47 Iteration  61 Loss:  0.2634015679359436\n",
            "Epoch  47 Iteration  62 Loss:  0.4045572578907013\n",
            "Epoch  47 Iteration  63 Loss:  0.35033687949180603\n",
            "Epoch  47 Iteration  64 Loss:  0.5555840134620667\n",
            "Epoch  47 Iteration  65 Loss:  0.1381215751171112\n",
            "*********************************************************************\n",
            "EPOCH  47  ==> Loss:  0.2958756210000226  Time taken ==>  226.63657593727112\n",
            "*********************************************************************\n",
            "Epoch  48 Iteration  0 Loss:  0.5041302442550659\n",
            "Epoch  48 Iteration  1 Loss:  0.241949200630188\n",
            "Epoch  48 Iteration  2 Loss:  0.2400812804698944\n",
            "Epoch  48 Iteration  3 Loss:  0.18569932878017426\n",
            "Epoch  48 Iteration  4 Loss:  0.16282476484775543\n",
            "Epoch  48 Iteration  5 Loss:  0.24900095164775848\n",
            "Epoch  48 Iteration  6 Loss:  0.12423364818096161\n",
            "Epoch  48 Iteration  7 Loss:  0.2629670798778534\n",
            "Epoch  48 Iteration  8 Loss:  0.16116146743297577\n",
            "Epoch  48 Iteration  9 Loss:  0.21114492416381836\n",
            "Epoch  48 Iteration  10 Loss:  0.19341759383678436\n",
            "Epoch  48 Iteration  11 Loss:  0.24585282802581787\n",
            "Epoch  48 Iteration  12 Loss:  0.23367735743522644\n",
            "Epoch  48 Iteration  13 Loss:  0.3480595052242279\n",
            "Epoch  48 Iteration  14 Loss:  0.5229681134223938\n",
            "Epoch  48 Iteration  15 Loss:  0.35347598791122437\n",
            "Epoch  48 Iteration  16 Loss:  0.13067397475242615\n",
            "Epoch  48 Iteration  17 Loss:  0.10077794641256332\n",
            "Epoch  48 Iteration  18 Loss:  0.3429364264011383\n",
            "Epoch  48 Iteration  19 Loss:  0.2570991814136505\n",
            "Epoch  48 Iteration  20 Loss:  0.20050624012947083\n",
            "Epoch  48 Iteration  21 Loss:  0.6975066661834717\n",
            "Epoch  48 Iteration  22 Loss:  0.22777552902698517\n",
            "Epoch  48 Iteration  23 Loss:  0.282031387090683\n",
            "Epoch  48 Iteration  24 Loss:  0.26096752285957336\n",
            "Epoch  48 Iteration  25 Loss:  0.4298473596572876\n",
            "Epoch  48 Iteration  26 Loss:  0.3809969425201416\n",
            "Epoch  48 Iteration  27 Loss:  0.18670959770679474\n",
            "Epoch  48 Iteration  28 Loss:  0.1774376630783081\n",
            "Epoch  48 Iteration  29 Loss:  0.25370508432388306\n",
            "Epoch  48 Iteration  30 Loss:  0.27311962842941284\n",
            "Epoch  48 Iteration  31 Loss:  0.39070671796798706\n",
            "Epoch  48 Iteration  32 Loss:  0.2925882935523987\n",
            "Epoch  48 Iteration  33 Loss:  0.19417744874954224\n",
            "Epoch  48 Iteration  34 Loss:  0.29145684838294983\n",
            "Epoch  48 Iteration  35 Loss:  0.2521292567253113\n",
            "Epoch  48 Iteration  36 Loss:  0.3833930492401123\n",
            "Epoch  48 Iteration  37 Loss:  0.38359999656677246\n",
            "Epoch  48 Iteration  38 Loss:  0.7057404518127441\n",
            "Epoch  48 Iteration  39 Loss:  0.1873880922794342\n",
            "Epoch  48 Iteration  40 Loss:  0.0949588418006897\n",
            "Epoch  48 Iteration  41 Loss:  0.48442310094833374\n",
            "Epoch  48 Iteration  42 Loss:  0.23914434015750885\n",
            "Epoch  48 Iteration  43 Loss:  0.24748903512954712\n",
            "Epoch  48 Iteration  44 Loss:  0.3361213803291321\n",
            "Epoch  48 Iteration  45 Loss:  0.439199298620224\n",
            "Epoch  48 Iteration  46 Loss:  0.5107373595237732\n",
            "Epoch  48 Iteration  47 Loss:  0.11541897803544998\n",
            "Epoch  48 Iteration  48 Loss:  0.1995023787021637\n",
            "Epoch  48 Iteration  49 Loss:  0.2786330580711365\n",
            "Epoch  48 Iteration  50 Loss:  0.45571285486221313\n",
            "Epoch  48 Iteration  51 Loss:  0.6020355224609375\n",
            "Epoch  48 Iteration  52 Loss:  0.37704846262931824\n",
            "Epoch  48 Iteration  53 Loss:  0.35065925121307373\n",
            "Epoch  48 Iteration  54 Loss:  0.19697239995002747\n",
            "Epoch  48 Iteration  55 Loss:  0.2507054805755615\n",
            "Epoch  48 Iteration  56 Loss:  0.08416551351547241\n",
            "Epoch  48 Iteration  57 Loss:  0.32726678252220154\n",
            "Epoch  48 Iteration  58 Loss:  0.23937906324863434\n",
            "Epoch  48 Iteration  59 Loss:  0.48704659938812256\n",
            "Epoch  48 Iteration  60 Loss:  0.2685002386569977\n",
            "Epoch  48 Iteration  61 Loss:  0.199727863073349\n",
            "Epoch  48 Iteration  62 Loss:  0.2251390665769577\n",
            "Epoch  48 Iteration  63 Loss:  0.21753497421741486\n",
            "Epoch  48 Iteration  64 Loss:  0.47435200214385986\n",
            "Epoch  48 Iteration  65 Loss:  0.2916061282157898\n",
            "*********************************************************************\n",
            "EPOCH  48  ==> Loss:  0.2957181144844402  Time taken ==>  227.88080763816833\n",
            "*********************************************************************\n",
            "Epoch  49 Iteration  0 Loss:  0.2717708945274353\n",
            "Epoch  49 Iteration  1 Loss:  0.4459570348262787\n",
            "Epoch  49 Iteration  2 Loss:  0.25409364700317383\n",
            "Epoch  49 Iteration  3 Loss:  0.4526483416557312\n",
            "Epoch  49 Iteration  4 Loss:  0.19689665734767914\n",
            "Epoch  49 Iteration  5 Loss:  0.31752073764801025\n",
            "Epoch  49 Iteration  6 Loss:  0.21254625916481018\n",
            "Epoch  49 Iteration  7 Loss:  0.41640782356262207\n",
            "Epoch  49 Iteration  8 Loss:  0.16914497315883636\n",
            "Epoch  49 Iteration  9 Loss:  0.34568336606025696\n",
            "Epoch  49 Iteration  10 Loss:  0.4242752194404602\n",
            "Epoch  49 Iteration  11 Loss:  0.37578630447387695\n",
            "Epoch  49 Iteration  12 Loss:  0.41394177079200745\n",
            "Epoch  49 Iteration  13 Loss:  0.2432115375995636\n",
            "Epoch  49 Iteration  14 Loss:  0.21856166422367096\n",
            "Epoch  49 Iteration  15 Loss:  0.4917450547218323\n",
            "Epoch  49 Iteration  16 Loss:  0.36219659447669983\n",
            "Epoch  49 Iteration  17 Loss:  0.3378850221633911\n",
            "Epoch  49 Iteration  18 Loss:  0.26154273748397827\n",
            "Epoch  49 Iteration  19 Loss:  0.29588180780410767\n",
            "Epoch  49 Iteration  20 Loss:  0.19232840836048126\n",
            "Epoch  49 Iteration  21 Loss:  0.317796915769577\n",
            "Epoch  49 Iteration  22 Loss:  0.14474549889564514\n",
            "Epoch  49 Iteration  23 Loss:  0.3767451345920563\n",
            "Epoch  49 Iteration  24 Loss:  0.39288219809532166\n",
            "Epoch  49 Iteration  25 Loss:  0.12667520344257355\n",
            "Epoch  49 Iteration  26 Loss:  0.10762276500463486\n",
            "Epoch  49 Iteration  27 Loss:  0.43750885128974915\n",
            "Epoch  49 Iteration  28 Loss:  0.2680690884590149\n",
            "Epoch  49 Iteration  29 Loss:  0.29317840933799744\n",
            "Epoch  49 Iteration  30 Loss:  0.4236259460449219\n",
            "Epoch  49 Iteration  31 Loss:  0.143539696931839\n",
            "Epoch  49 Iteration  32 Loss:  0.5660699009895325\n",
            "Epoch  49 Iteration  33 Loss:  0.20114508271217346\n",
            "Epoch  49 Iteration  34 Loss:  0.2249368578195572\n",
            "Epoch  49 Iteration  35 Loss:  0.34703871607780457\n",
            "Epoch  49 Iteration  36 Loss:  0.16429753601551056\n",
            "Epoch  49 Iteration  37 Loss:  0.40093404054641724\n",
            "Epoch  49 Iteration  38 Loss:  0.4077339172363281\n",
            "Epoch  49 Iteration  39 Loss:  0.394355833530426\n",
            "Epoch  49 Iteration  40 Loss:  0.21886219084262848\n",
            "Epoch  49 Iteration  41 Loss:  0.2668490707874298\n",
            "Epoch  49 Iteration  42 Loss:  0.22719690203666687\n",
            "Epoch  49 Iteration  43 Loss:  0.2520349621772766\n",
            "Epoch  49 Iteration  44 Loss:  0.19805334508419037\n",
            "Epoch  49 Iteration  45 Loss:  0.32170119881629944\n",
            "Epoch  49 Iteration  46 Loss:  0.3684875965118408\n",
            "Epoch  49 Iteration  47 Loss:  0.28818249702453613\n",
            "Epoch  49 Iteration  48 Loss:  0.2878338396549225\n",
            "Epoch  49 Iteration  49 Loss:  0.19302977621555328\n",
            "Epoch  49 Iteration  50 Loss:  0.18602889776229858\n",
            "Epoch  49 Iteration  51 Loss:  0.23381593823432922\n",
            "Epoch  49 Iteration  52 Loss:  0.32873645424842834\n",
            "Epoch  49 Iteration  53 Loss:  0.4622712731361389\n",
            "Epoch  49 Iteration  54 Loss:  0.2290390431880951\n",
            "Epoch  49 Iteration  55 Loss:  0.24077840149402618\n",
            "Epoch  49 Iteration  56 Loss:  0.34633082151412964\n",
            "Epoch  49 Iteration  57 Loss:  0.3301992118358612\n",
            "Epoch  49 Iteration  58 Loss:  0.3084987998008728\n",
            "Epoch  49 Iteration  59 Loss:  0.2644502818584442\n",
            "Epoch  49 Iteration  60 Loss:  0.11943449825048447\n",
            "Epoch  49 Iteration  61 Loss:  0.24420787394046783\n",
            "Epoch  49 Iteration  62 Loss:  0.0888039693236351\n",
            "Epoch  49 Iteration  63 Loss:  0.6033684015274048\n",
            "Epoch  49 Iteration  64 Loss:  0.21523398160934448\n",
            "Epoch  49 Iteration  65 Loss:  0.28876176476478577\n",
            "*********************************************************************\n",
            "EPOCH  49  ==> Loss:  0.2962290672867587  Time taken ==>  227.51322889328003\n",
            "*********************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRlKIvj3Vc74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def get_classification(test_data_batches, trained_model, ce_loss):\n",
        "  all_predictions = []\n",
        "  all_true = []\n",
        "  trained_model.eval()\n",
        "  for epoch in range(1):\n",
        "    epoch_loss = 0\n",
        "    start = time.time()\n",
        "    for i, sample in enumerate(test_data_batches):\n",
        "      batchInput = sample['data']\n",
        "      batchLabel = sample['label']\n",
        "      batchLap = torch.Tensor(get_laplacian_for_batch(sample['data']))\n",
        "\n",
        "      batchInput = batchInput.cuda()\n",
        "      batchLabel = batchLabel.cuda()\n",
        "      batchLap = batchLap.cuda()\n",
        "      \n",
        "      \n",
        "\n",
        "      with torch.no_grad():\n",
        "        batchOutput = trained_model(batchInput, batchLap)\n",
        "      \n",
        "      predictedLabel = torch.argmax(batchOutput)\n",
        "      all_predictions.append(predictedLabel.item())\n",
        "      all_true.append(batchLabel.item())\n",
        "      print(\"Predicted Class : \", predictedLabel.item(), \" True Label : \", batchLabel.item())\n",
        "\n",
        "      loss = ce_loss(batchOutput, batchLabel.long())\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      print(\"Epoch \", epoch, \"Iteration \", i, \"Loss: \", loss.item())\n",
        "    end = time.time()\n",
        "    time_per_epoch = end - start\n",
        "    epoch_loss /= len(test_data_batches)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_true = np.array(all_true)\n",
        "    accuracy = 1 - (np.count_nonzero(all_predictions - all_true) / len(all_predictions))\n",
        "    \n",
        "    # accuracy = (all_predictions == all_true).float().sum() / len(all_predictions)\n",
        "    print(\"*********************************************************************\")\n",
        "    print(\"Test Loss ==>  \", epoch_loss, \" Accuracy ==> \", accuracy,  \" Time taken ==> \", time_per_epoch)\n",
        "    print(\"*********************************************************************\")\n",
        "  return all_predictions          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnIh9cOyZ6Yo",
        "colab_type": "code",
        "outputId": "4a659165-837a-407f-f10f-f96cd7b9a350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ce_loss = nn.CrossEntropyLoss()\n",
        "all_preds = get_classification(test_data_batches, gcn_model, ce_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  0 Loss:  0.032591819763183594\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  1 Loss:  0.005115032196044922\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  2 Loss:  0.2283463478088379\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  3 Loss:  0.014149665832519531\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  4 Loss:  0.005074977874755859\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  5 Loss:  0.00011920928955078125\n",
            "Predicted Class :  2  True Label :  1.0\n",
            "Epoch  0 Iteration  6 Loss:  2.3232994079589844\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  7 Loss:  0.1092233657836914\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  8 Loss:  0.0040721893310546875\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  9 Loss:  0.006213665008544922\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  10 Loss:  0.15484952926635742\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  11 Loss:  0.005667686462402344\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  12 Loss:  0.4054551124572754\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  13 Loss:  0.146759033203125\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  14 Loss:  0.003973484039306641\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  15 Loss:  0.009441375732421875\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  16 Loss:  0.3594064712524414\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  17 Loss:  0.0002613067626953125\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  18 Loss:  0.011683464050292969\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  19 Loss:  0.44539690017700195\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  20 Loss:  0.011884689331054688\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  21 Loss:  0.07928609848022461\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  22 Loss:  0.30814480781555176\n",
            "Predicted Class :  9  True Label :  9.0\n",
            "Epoch  0 Iteration  23 Loss:  0.05771756172180176\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  24 Loss:  0.1743483543395996\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  25 Loss:  0.0043735504150390625\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  26 Loss:  0.08382987976074219\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  27 Loss:  0.0041656494140625\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  28 Loss:  0.010053157806396484\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  29 Loss:  0.10426998138427734\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  30 Loss:  0.06627321243286133\n",
            "Predicted Class :  2  True Label :  9.0\n",
            "Epoch  0 Iteration  31 Loss:  3.0622644424438477\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  32 Loss:  0.020128250122070312\n",
            "Predicted Class :  3  True Label :  3.0\n",
            "Epoch  0 Iteration  33 Loss:  0.46181297302246094\n",
            "Predicted Class :  2  True Label :  7.0\n",
            "Epoch  0 Iteration  34 Loss:  1.424947738647461\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  35 Loss:  0.023883819580078125\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  36 Loss:  0.018435001373291016\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  37 Loss:  0.033388614654541016\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  38 Loss:  0.006371498107910156\n",
            "Predicted Class :  6  True Label :  6.0\n",
            "Epoch  0 Iteration  39 Loss:  0.894819974899292\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  40 Loss:  0.11296987533569336\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  41 Loss:  0.06489419937133789\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  42 Loss:  0.06969070434570312\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  43 Loss:  0.08343505859375\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  44 Loss:  0.021706581115722656\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  45 Loss:  0.26096057891845703\n",
            "Predicted Class :  7  True Label :  1.0\n",
            "Epoch  0 Iteration  46 Loss:  4.712689399719238\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  47 Loss:  0.4003901481628418\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  48 Loss:  0.0640406608581543\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  49 Loss:  4.76837158203125e-05\n",
            "Predicted Class :  9  True Label :  9.0\n",
            "Epoch  0 Iteration  50 Loss:  0.022478580474853516\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  51 Loss:  0.00019550323486328125\n",
            "Predicted Class :  2  True Label :  7.0\n",
            "Epoch  0 Iteration  52 Loss:  0.8077921867370605\n",
            "Predicted Class :  2  True Label :  1.0\n",
            "Epoch  0 Iteration  53 Loss:  2.9589836597442627\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  54 Loss:  0.02234649658203125\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  55 Loss:  0.03313732147216797\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  56 Loss:  0.000743865966796875\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  57 Loss:  0.1245412826538086\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  58 Loss:  0.011483192443847656\n",
            "Predicted Class :  2  True Label :  1.0\n",
            "Epoch  0 Iteration  59 Loss:  1.9663587808609009\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  60 Loss:  0.0032210350036621094\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  61 Loss:  0.0802302360534668\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  62 Loss:  0.034815311431884766\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  63 Loss:  0.11659908294677734\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  64 Loss:  0.6107974052429199\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  65 Loss:  0.35862207412719727\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  66 Loss:  0.00315093994140625\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  67 Loss:  0.016145706176757812\n",
            "Predicted Class :  2  True Label :  1.0\n",
            "Epoch  0 Iteration  68 Loss:  3.2315731048583984\n",
            "Predicted Class :  3  True Label :  3.0\n",
            "Epoch  0 Iteration  69 Loss:  0.8615608215332031\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  70 Loss:  0.007061481475830078\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  71 Loss:  0.006527900695800781\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  72 Loss:  0.018011093139648438\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  73 Loss:  0.016982078552246094\n",
            "Predicted Class :  2  True Label :  1.0\n",
            "Epoch  0 Iteration  74 Loss:  1.902207612991333\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  75 Loss:  0.08178901672363281\n",
            "Predicted Class :  2  True Label :  9.0\n",
            "Epoch  0 Iteration  76 Loss:  1.6145093441009521\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  77 Loss:  0.031552791595458984\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  78 Loss:  0.05405378341674805\n",
            "Predicted Class :  2  True Label :  1.0\n",
            "Epoch  0 Iteration  79 Loss:  3.3229050636291504\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  80 Loss:  0.005360603332519531\n",
            "Predicted Class :  4  True Label :  4.0\n",
            "Epoch  0 Iteration  81 Loss:  0.004261970520019531\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  82 Loss:  0.19200611114501953\n",
            "Predicted Class :  2  True Label :  2.0\n",
            "Epoch  0 Iteration  83 Loss:  0.007094860076904297\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  84 Loss:  0.004208087921142578\n",
            "Predicted Class :  1  True Label :  6.0\n",
            "Epoch  0 Iteration  85 Loss:  1.3780162334442139\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  86 Loss:  0.006744384765625\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  87 Loss:  0.061002254486083984\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  88 Loss:  0.04526329040527344\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  89 Loss:  0.0028429031372070312\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  90 Loss:  0.005171298980712891\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  91 Loss:  0.36571645736694336\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  92 Loss:  0.17615699768066406\n",
            "Predicted Class :  6  True Label :  6.0\n",
            "Epoch  0 Iteration  93 Loss:  0.8955202102661133\n",
            "Predicted Class :  0  True Label :  0.0\n",
            "Epoch  0 Iteration  94 Loss:  0.013019561767578125\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  95 Loss:  0.058092594146728516\n",
            "Predicted Class :  8  True Label :  8.0\n",
            "Epoch  0 Iteration  96 Loss:  0.39116859436035156\n",
            "Predicted Class :  5  True Label :  5.0\n",
            "Epoch  0 Iteration  97 Loss:  0.019974708557128906\n",
            "Predicted Class :  6  True Label :  6.0\n",
            "Epoch  0 Iteration  98 Loss:  0.6781854629516602\n",
            "Predicted Class :  7  True Label :  7.0\n",
            "Epoch  0 Iteration  99 Loss:  0.14696168899536133\n",
            "*********************************************************************\n",
            "Test Loss ==>   0.3968946921825409  Accuracy ==>  0.88  Time taken ==>  9.33225154876709\n",
            "*********************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcVQe5eip_Xc",
        "colab_type": "code",
        "outputId": "1f337d53-95dc-4673-ab15-c80455a8a711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "torch.save(trained_gcn_model, '/content/trained_gcn.pth')\n",
        "torch.save(trained_gcn_model.state_dict(), '/content/trained_gcn_state_dict.pth')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type pointGCN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type gcnLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type globalPooling. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4S08WJGTxiY",
        "colab_type": "code",
        "outputId": "2b0dbc98-1305-4536-f602-b90f2ef5bfed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "gcn_model = pointGCN(para)\n",
        "# gcn_model.load_state_dict(torch.load('/content/gdrive/My Drive/CIS_565_2019/trained_gcn_state_dict.pth'))\n",
        "gcn_model = torch.load('/content/gdrive/My Drive/CIS_565_2019/trained_gcn.pth')\n",
        "gcn_model = gcn_model.to(device)\n",
        "gcn_model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pointGCN(\n",
              "  (gcn1_layer): gcnLayer(\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (gcn1_dropout_layer): Dropout(p=0.09999999999999998, inplace=False)\n",
              "  (gcn1_global_pooling_layer): globalPooling()\n",
              "  (gcn2_layer): gcnLayer(\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (gcn2_dropout_layer): Dropout(p=0.09999999999999998, inplace=False)\n",
              "  (gcn2_global_pooling_layer): globalPooling()\n",
              "  (global_features_dropout_layer): Dropout(p=0.44999999999999996, inplace=False)\n",
              "  (fc1): Linear(in_features=4000, out_features=600, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc1_droput): Dropout(p=0.44999999999999996, inplace=False)\n",
              "  (fc2): Linear(in_features=600, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK_CCpCMqrbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_global_pooling(ip):\n",
        "  var = torch.var(ip, dim=1) ** 2\n",
        "  var = var.repeat(ip.shape[1], 1)\n",
        "  op = torch.cat((var, ip.squeeze()), dim = 1)\n",
        "  return op\n",
        "\n",
        "def point_wise_forward(gcn_model, inputPC, scaledLaplacian):\n",
        "  gcn_1         = gcn_model.gcn1_layer(inputPC, scaledLaplacian)\n",
        "  gcn_1_output  = gcn_model.gcn1_dropout_layer(gcn_1)\n",
        "  \n",
        "  gcn_1_pooling = point_wise_global_pooling(gcn_1_output)\n",
        "  \n",
        "  gcn_2         = gcn_model.gcn2_layer(gcn_1_output, scaledLaplacian)\n",
        "  gcn_2_output  = gcn_model.gcn2_dropout_layer(gcn_2)\n",
        "  gcn_2_pooling = point_wise_global_pooling(gcn_2_output)\n",
        "\n",
        "  globalFeatures = torch.cat((gcn_1_pooling, gcn_2_pooling), dim=1)\n",
        "  globalFeatures = gcn_model.global_features_dropout_layer(globalFeatures)\n",
        "\n",
        "  # fully connected layer 1\n",
        "  fc_layer_1 = gcn_model.fc1(globalFeatures)\n",
        "  fc_layer_1 = gcn_model.relu(fc_layer_1)\n",
        "  fc_layer_1 = gcn_model.fc1_droput(fc_layer_1)\n",
        "\n",
        "  # fully connected layer 2\n",
        "  fc_layer_2 = gcn_model.fc2(fc_layer_1)\n",
        "\n",
        "  return fc_layer_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rHDDxctc2G",
        "colab_type": "code",
        "outputId": "1c77c303-f57f-4c32-b4b3-cf8e5cccf7a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def point_wise_classification(inputTrainTensor, gcn_model, indices):\n",
        "  all_class = []\n",
        "  for idx in range(len(indices)):\n",
        "    x = inputTrainTensor[indices[idx]].unsqueeze(0)\n",
        "    L = torch.Tensor(get_laplacian_for_batch(x))\n",
        "    x = x.cuda()\n",
        "    L = L.cuda()\n",
        "    y = point_wise_forward(gcn_model, x, L)\n",
        "    y = y.unsqueeze(0)\n",
        "    y = torch.argmax(y, dim=2)\n",
        "    all_class.append(y)\n",
        "  all_class = torch.cat(all_class, dim=0)\n",
        "  return all_class\n",
        "\n",
        "indices = [i for i in range(500)]\n",
        "all_class = point_wise_classification(inputTrainTensor, gcn_model, indices)\n",
        "print(all_class.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([500, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTMHgLPFxqTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"./all_points.npy\", inputTrainTensor.numpy())\n",
        "np.save(\"./all_predictions.npy\", all_class.cpu().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBcLJyIjsQBB",
        "colab_type": "code",
        "outputId": "4287c276-95ae-4c09-ccdc-19b3b1031577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.rand(1, 1024, 1000)\n",
        "y = point_wise_global_pooling(x)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1024, 2000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXAf41SUWLf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_point_wise_classification(sample, sampleLabel, trained_model, ce_loss):\n",
        "  all_predictions = []\n",
        "  all_true = []\n",
        "  trained_model.eval()\n",
        "  \n",
        "  for epoch in range(1):\n",
        "    epoch_loss = 0\n",
        "    start = time.time()\n",
        "    for i, sample in enumerate(test_data_batches):\n",
        "      batchInput = sample['data']\n",
        "      batchLabel = sample['label']\n",
        "      batchLap = torch.Tensor(get_laplacian_for_batch(sample['data']))\n",
        "\n",
        "      batchInput = batchInput.cuda()\n",
        "      batchLabel = batchLabel.cuda()\n",
        "      batchLap = batchLap.cuda()\n",
        "      \n",
        "      \n",
        "\n",
        "      with torch.no_grad():\n",
        "        batchOutput = trained_model(batchInput, batchLap)\n",
        "      \n",
        "      predictedLabel = torch.argmax(batchOutput)\n",
        "      all_predictions.append(predictedLabel.item())\n",
        "      all_true.append(batchLabel.item())\n",
        "      print(\"Predicted Class : \", predictedLabel.item(), \" True Label : \", batchLabel.item())\n",
        "\n",
        "      loss = ce_loss(batchOutput, batchLabel.long())\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      print(\"Epoch \", epoch, \"Iteration \", i, \"Loss: \", loss.item())\n",
        "    end = time.time()\n",
        "    time_per_epoch = end - start\n",
        "    epoch_loss /= len(test_data_batches)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_true = np.array(all_true)\n",
        "    accuracy = 1 - (np.count_nonzero(all_predictions - all_true) / len(all_predictions))\n",
        "    \n",
        "    # accuracy = (all_predictions == all_true).float().sum() / len(all_predictions)\n",
        "    print(\"*********************************************************************\")\n",
        "    print(\"Test Loss ==>  \", epoch_loss, \" Accuracy ==> \", accuracy,  \" Time taken ==> \", time_per_epoch)\n",
        "    print(\"*********************************************************************\")\n",
        "  return all_predictions "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}